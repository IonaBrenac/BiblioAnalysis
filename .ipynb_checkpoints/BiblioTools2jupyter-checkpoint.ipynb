{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "users = \"amal\"\n",
    " \n",
    "dic_users ={\"amal\":2,'francois':3,'iona':1}\n",
    "\n",
    "user = {}\n",
    "user = {\n",
    "        1 : {\n",
    "        'mac_packages' : '',\n",
    "        'path1' : '',\n",
    "        'path2' : 'BiblioAnalysis Data/'\n",
    "        },\n",
    "        2 : {\n",
    "        'mac_packages' : '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages',\n",
    "        'path1' : 'My_Jupyter/',\n",
    "        'path2' : 'BiblioAnalysis Data/'\n",
    "        },\n",
    "        3 : {\n",
    "        'mac_packages' : '',\n",
    "        'path1' : '',\n",
    "        'path2' : 'BiblioAnalysis Data/'\n",
    "        }\n",
    "        }\n",
    "     \n",
    "i_user = dic_users[users]\n",
    "\n",
    "# Add path of 'site-packages' where useful packages are stored on MAC-OS\n",
    "sys.path.append(user[i_user]['mac_packages'])\n",
    "\n",
    "myprojectname = 'Test_BT/'\n",
    "\n",
    "\n",
    "root = Path.home() \n",
    "\n",
    "# folder containing the wos or scopus file to process\n",
    "in_dir_parsing = root / Path(user[i_user]['path2'] + myprojectname +'rawdata')\n",
    "\n",
    "# folder containing the wos or scopus parsed and filered file\n",
    "in_dir_corpus = root / Path(user[i_user]['path2'] + myprojectname)\n",
    "\n",
    "# folder containing the wos or scopus parsing and filering files                  \n",
    "in_dir_describe_corpus = root / Path(user[i_user]['path2'] + myprojectname)\n",
    "\n",
    "\n",
    "out_dir = root / Path(user[i_user]['path2'] + myprojectname)\n",
    "database = 'scopus' # 'wos'\n",
    "expert = False\n",
    "rep_utils = root / Path(user[i_user]['path1'] + 'BiblioAnalysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data parsing\n",
    "- articles.dat is the central file, listing all the publications within the corpus. It contains informations such as the document type (article, letter, review, conf proceeding, etc), title, year of publication, publication source, doi, number of citations (given by WOS or Scopus at the time of the extraction) AND a unique identifier used in all the other files to identify a precise publication.\n",
    "- database.dat keeps track of the origin of the data, some part of the analysis being specific to WOS or Scopus data.\n",
    "- authors.dat lists all authors names associated to all publications ID.\n",
    "- addresses.dat lists all adresses associated to all publications ID, along with a specific ID for each adresse line. These adresses are reported as they appear in the raw data, without any further processing.\n",
    "- countries.dat lists all countries associated to all publications ID and adresses lines ID. The countries are extracted from the adresses fields of the raw data, with some cleaning (changing mentions of US states and UK countries to respectively the USA and UK).\n",
    "- institutions.dat lists all the comma-separated entities appearing in the adresses field associated to all publications ID and adresses lines ID, except those refering to a physical adresses. These entities correspond to various name variants of universities, organisms, hospitals, labs, services, departments, etc as they appear in the raw data. No treatment is made to e.g. filtering out the entities corresponding a given hierarchy level.\n",
    "- keywords.dat lists various types of keywords associated to all publications ID. \"AK\" keywords correspond to Author's keywords. \"IK\" keywords correspond to either WOS or Scopus keywords, which are built based on the authors' keywords, the title and abstract. \"TK\" correspond to title words (from which we simply remove common words and stop words - no stemming is performed). TK are especially useful when studying pre-90's publications, when the use of keywords was not yet standard.\n",
    "- references.dat lists all the references associated to all publications ID. The rawdata is parsed to store the first author name, title, source, volume and page of each reference of the raw \"references\" field.\n",
    "- subjects.dat lists all subject categories associated to all publications ID (a journal may be associated to many subject category). WOS classifies the sources it indexes into ∼ 250 categories, that are reported in the extracted data. Scopus classifies its sources into 27 major categories and ∼ 300 sub-categories, none of which are reported in the extracted data. We use Elsevier Source Title List (october 2017 version) to retrieve that information. The \"subject.dat\" contains the info relative to the major categories.\n",
    "- subjects2.dat lists Scopus's sub-categories, if the use database is Scopus.\n",
    "- AA_log.txt keeps track of the date/time the script was executed and of all the messages displayed on the terminal (number of publications extracted, % of references rejected, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/python -v\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\" \n",
    "   Author : Sebastian Grauwin (http://www.sebastian-grauwin.com/)\n",
    "   Copyright (C) 2017\n",
    "   extracted from biblio_parser.py\n",
    "\"\"\"\n",
    "\n",
    "# usage: parser.py -i DIR [-o DIR] [-e]\n",
    "# \n",
    "\n",
    "import os\n",
    "import numpy\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import Utils.Utils as Utils #Utils.Utils as Utils <-----------------------------------------------------\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "## ##################################################\n",
    "# a dictionnary of common_words used when extracting title words\n",
    "common_words = ['a', 'able', 'about', 'across', 'after', 'all', 'almost', 'also', 'am', 'among', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'between', 'but', 'by', 'can', 'cannot', 'could', 'dear', 'did', 'do', 'does', 'during','either', 'else', 'ever', 'every', 'for', 'from', 'get', 'got', 'had', 'has', 'have', 'he', 'her', 'hers', 'him', 'his', 'how', 'however', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'least', 'let', 'like', 'likely', 'may', 'me', 'might', 'most', 'must', 'my', 'neither', 'no', 'nor', 'not', 'of', 'off', 'often', 'on', 'only', 'or', 'other', 'our', 'own', 'rather', 'said', 'say', 'says', 'she', 'should', 'since', 'so', 'some', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'these', 'they', 'this', 'tis', 'to', 'too', 'twas', 'us', 'wants', 'was', 'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'would', 'yet', 'you', 'your']\n",
    "french_words = ['un','une','au','aux','entre','a','le','la','les','du','de','des','mais','ou','et','dans','avec','en','sur','sous','avant','apres','vers','par','pendant','depuis','pour','chez','est','ont']\n",
    "common_words = common_words + french_words;\n",
    "punctuation = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', ' - ']\n",
    "# I kept the '-' out of this list\n",
    "## ##################################################\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "def biblio_parser(in_dir,out_dir,database,expert):\n",
    "\n",
    "  ## INITIALIZATION\n",
    "  t1=time.time()\n",
    "\n",
    "  #.. detecting raw files\n",
    "  if database == 'wos': pattern = \"*.txt\"\n",
    "  if database == 'scopus': pattern = \"*.csv\"\n",
    "  print (\"..Analysing files %s/%s\" % (in_dir,pattern) )\n",
    "  srclst=[]\n",
    "  for path, subdirs, files in os.walk(in_dir):\n",
    "    for name in files:\n",
    "        if fnmatch(name, pattern):\n",
    "            srclst.append( os.path.join(path, name))\n",
    "  print (\"....%d '%s' files detected\" % (len(srclst),pattern))\n",
    "\n",
    "\n",
    "  #.. prep empty parsed files\n",
    "  outfilenames={\"A\":\"articles\",\"AU\":\"authors\",\"K\":\"keywords\",\"S\":\"subjects\",\"R\":\"references\",\"CU\":\"countries\",\"AD\":\"addresses\",\"I\":\"institutions\"}\n",
    "  if expert:\n",
    "    #output all info that might be useful in advanced projects \n",
    "    outfilenames.update({ \"AB\":\"abstracts\" ,\"CI\":\"cities\"})\n",
    "    if database==\"wos\": outfilenames.update({\"FT\":\"fundingtext\", \"RP\":\"AUaddresses\"})\n",
    "  if database==\"scopus\": outfilenames.update({\"S2\":\"subjects2\"})\n",
    "  outf=dict()\n",
    "  print (out_dir)\n",
    "  for elt in outfilenames: \n",
    "    print(elt,os.path.join(out_dir, outfilenames[elt]+\".dat\"))\n",
    "    outf[elt] = open(os.path.join(out_dir, outfilenames[elt]+\".dat\"),'w', encoding='utf-8')\n",
    "  \n",
    "  #.. keep origin of data\n",
    "  dst0  = os.path.join(out_dir, \"database.dat\") \n",
    "  with open(dst0,'w') as ff:\n",
    "    if database == 'wos': ff.write('Web Of Science')\n",
    "    if database == 'scopus': ff.write('Scopus')\n",
    "\n",
    "  # SCOPUS JOURNAL CATEGORIES\n",
    "  #... scopus export data do not contain any subject category. Here we upload an official scopus list from february 2015 listing the SUBJCAT each journal correspond to (multiple category per journal possible)\n",
    "  if (database =='scopus'):\n",
    "    print (\"..upload Scopus publication sources' categories from auxiliary file\")\n",
    "    journalCATS=dict()\n",
    "    issnCATS=dict()\n",
    "    code_cat=dict()\n",
    "    appearsXtimes=[]\n",
    "    \n",
    "    try:                      # Add to work with Jupyter  11/02/2021 \n",
    "        script_dir = os.path.dirname(__file__)\n",
    "    except:\n",
    "        script_dir = rep_utils\n",
    "    \n",
    "    #.. read cat_catcode file\n",
    "    filename = os.path.join(script_dir, 'Utils/scopus_cat_codes.txt')\n",
    "    with open(filename,'r', encoding='utf8') as fd:\n",
    "      for line in fd.readlines():\n",
    "        line = line.strip('\\n')\n",
    "        foo=line.split('\\t')\n",
    "        code_cat[foo[1]]=foo[0]\n",
    "    #.. read journal_catcode file\n",
    "    filename = os.path.join(script_dir, 'Utils/scopus_journals_issn_cat.txt')\n",
    "    with open(filename,'r', encoding='utf8') as fd:\n",
    "      for line in fd.readlines():\n",
    "        line = line.strip('\\n')\n",
    "        foo=line.split('\\t')\n",
    "        jrn=foo[0]\n",
    "        issn=foo[1]\n",
    "        bar=foo[2].replace(' ','').split(';')\n",
    "        if (jrn not in journalCATS and len(foo[2])>1): journalCATS[jrn]=[[],[]]\n",
    "        if (issn not in issnCATS and len(issn)>1 and len(foo[2])>1): issnCATS[issn]=[[],[]]\n",
    "        else: appearsXtimes.append(jrn)\n",
    "        for i in range(len(bar)):\n",
    "          if len(bar[i])>0: \n",
    "            cat=code_cat[bar[i]]\n",
    "            journalCATS[jrn][0].append(cat) \n",
    "            if len(issn)>1: issnCATS[issn][0].append(cat)  \n",
    "            cat=code_cat[bar[i][0:2]+'00'].replace('(all)','').replace('General ','')\n",
    "            journalCATS[jrn][1].append(cat) \n",
    "            if len(issn)>1: issnCATS[issn][1].append(cat) \n",
    "    for jrn in journalCATS: journalCATS[jrn]=[list(set(journalCATS[jrn][0])),list(set(journalCATS[jrn][1]))]\n",
    "    for issn in issnCATS: issnCATS[issn]=[list(set(issnCATS[issn][0])),list(set(issnCATS[issn][1]))]\n",
    "    del code_cat;     \n",
    "    # create dict of journals / sources not in that category file        \n",
    "    journalNOT=dict()\n",
    "\n",
    "    #print(\"..%d sources appear more than once in the scopus's source/category file\" % len(list(set(appearsXtimes))))\n",
    "\n",
    "\n",
    "\n",
    "  ## #############################################################\n",
    "  ## TREAT DATA\n",
    "  #.. some parameters to count errors / filtered publis\n",
    "  kompt_publis = 0\n",
    "  kompt_refs = 0\n",
    "  kompt_corrupt_refs = 0\n",
    "  kompt_refs_with_DOI = 0\n",
    "  #.. \"id\" will be the new id for articles in the dataset, \"unique_ids\" tracks the WOS or SCOPUS unique id that we use to remove duplicates\n",
    "  id = int(-1)\n",
    "  UNIQUE_IDS = dict()\n",
    "\n",
    "  # parse and clean metadata about each article\n",
    "  for src in srclst:\n",
    "      pl = Utils.ArticleList()\n",
    "      pl.read_file(src,database)\n",
    "      print (\"..processing %d publications in file %s\" % (len(pl.articles), src)) \n",
    "      kompt_publis+=len(pl.articles)\n",
    "      if (len(pl.articles) > 0):\n",
    "          for article in pl.articles:\n",
    "            if article.UT not in UNIQUE_IDS:\n",
    "              UNIQUE_IDS[article.UT] = ''\n",
    "              id = id + 1\n",
    "              #article \n",
    "              foo = article.AU.split('; ')\n",
    "              firstAU = foo[0].replace(',','')\n",
    "              if (article.J9=='' and article.SO==''): article.J9='[]';\n",
    "              if (article.J9==''): article.J9=article.SO;\n",
    "              if (article.DT==''): article.DT='[]';\n",
    "              if (database=='scopus'): article.J9=article.SO;\n",
    "              outf[\"A\"].write(\"%d\\t%s\\t%d\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" % (id,firstAU,article.PY,article.J9,article.VL,article.BP,article.DI,article.PT,article.DT,article.LA,article.TC,article.TI,article.PD,article.UT))\n",
    "              #journals\n",
    "              if \"J\" in outf: outf[\"J\"].write(\"%d\\t%s\\t%s\\n\" % (id, article.SO,article.SN))\n",
    "              #authors\n",
    "              if(article.AU != \"\"): \n",
    "                  foo = article.AU.split('; ')\n",
    "                  for i in range(len(foo)):\n",
    "                      #... check the upper or lower format of letter to have the author name in format \"Grauwin S\"\n",
    "                      foobar=normauthor(foo[i])\n",
    "                      outf[\"AU\"].write(\"%d\\t%d\\t%s\\n\" % (id,i,foobar))\n",
    "              #keywords\n",
    "              if(article.DE != \"\"):\n",
    "                  #.. author keywords\n",
    "                  foo = article.DE.split('; ')\n",
    "                  for f in foo:\n",
    "                      outf[\"K\"].write(\"%d\\tAK\\t%s\\n\" % (id,f.upper()))\n",
    "              if(article.ID != \"\"):\n",
    "                  #.. WOS or SCOPUS keywords \n",
    "                  foo = article.ID.split('; ')\n",
    "                  for f in foo:\n",
    "                      outf[\"K\"].write(\"%d\\tIK\\t%s\\n\" % (id,f.upper()))\n",
    "              if(article.TI != \"\"):\n",
    "                  #.. title words (excluding the common words listed on the top of this file)\n",
    "                  foo = article.TI\n",
    "                  #... remove ponctuations !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\n",
    "                  for p in punctuation: foo = foo.replace(p,'')\n",
    "                  foo = foo.split(' ')\n",
    "                  for f in foo:\n",
    "                    bar = f.lower()\n",
    "                    if bar not in common_words and len(bar)>0:\n",
    "                      outf[\"K\"].write(\"%d\\tTK\\t%s\\n\" % (id, bar.upper()))\n",
    "              #subjects\n",
    "              if database=='wos':\n",
    "                if(article.WC != \"\"):\n",
    "                  foo = article.WC.split('; ')\n",
    "                  for cat in foo: outf[\"S\"].write(\"%d\\t%s\\n\" % (id,cat))\n",
    "              if database=='scopus':\n",
    "                if article.SO in journalCATS:\n",
    "                  foo=journalCATS[article.SO][1]\n",
    "                  for cat in foo: outf[\"S\"].write(\"%d\\t%s\\n\" % (id,cat))\n",
    "                  foo=journalCATS[article.SO][0]\n",
    "                  for cat in foo: outf[\"S2\"].write(\"%d\\t%s\\n\" % (id,cat))\n",
    "                elif article.SN in issnCATS:\n",
    "                  foo=issnCATS[article.SN][1]\n",
    "                  for cat in foo: outf[\"S\"].write(\"%d\\t%s\\n\" % (id,cat))\n",
    "                  foo=issnCATS[article.SN][0]\n",
    "                  for cat in foo: outf[\"S2\"].write(\"%d\\t%s\\n\" % (id,cat))\n",
    "                else:\n",
    "                  if (article.SO, article.SN) not in journalNOT: journalNOT[(article.SO, article.SN)]=0\n",
    "                  journalNOT[(article.SO, article.SN)]+=1;\n",
    "                if ( (article.SO in journalCATS and 'Education' in journalCATS[article.SO][0]) or (article.SN in issnCATS and 'Education' in issnCATS[article.SN][0]) or 'education' in article.SO.lower() or 'learning' in article.SO.lower() ): outf[\"S\"].write(\"%d\\tEducation*\\n\" % (id))\n",
    "              # #references\n",
    "              if(article.CR != \"\"):\n",
    "                foo = article.CR.split('; ')\n",
    "                for i in range(len(foo)):\n",
    "                  if len(foo[i])>3:\n",
    "                    ref=Utils.Ref()\n",
    "                    ref.parse_ref(foo[i],database)\n",
    "                    kompt_refs += 1 \n",
    "                    if (ref.year > 0): \n",
    "                      if (ref.DOI !=\"\"): kompt_refs_with_DOI+=1\n",
    "                      outf[\"R\"].write(\"%d\\t%s\\t%d\\t%s\\t%s\\t%s\\t%s\\n\" % (id,ref.firstAU,ref.year,ref.journal,ref.volume,ref.page,ref.DOI))\n",
    "                    if (ref.year == 0):\n",
    "                      #print (foo[i])\n",
    "                      kompt_corrupt_refs += 1  \n",
    "              #countries / cities / institutions\n",
    "              if(article.C1 != \"\"):\n",
    "                  adresse = article.C1\n",
    "                  aux1 = adresse.find('[')\n",
    "                  aux2 = adresse.find(']')\n",
    "                  while (aux1 < aux2) and aux1 > -1:\n",
    "                      aux = adresse[aux1:min(aux2+2,len(adresse))]\n",
    "                      adresse = adresse.replace(aux,'')\n",
    "                      aux1 = adresse.find('[')\n",
    "                      aux2 = adresse.find(']')\n",
    "                  foo = adresse.split('; ')\n",
    "                  for i in range(len(foo)):\n",
    "                      #... \"institutions\" will keep everything listed between commas in the adresses, except from cities and countries\n",
    "                      foo[i] = foo[i].replace(', ',',')\n",
    "                      bar = foo[i].split(',') \n",
    "                      ll = len(bar)\n",
    "                      for j in range(ll - 2):\n",
    "                        outf[\"I\"].write(\"%d\\t%d\\t%s\\n\" % (id,i,bar[j]))\n",
    "                      #... country\n",
    "                      country = bar[ll-1].replace('.','').replace(';','').upper()\n",
    "                      lll=len(bar[ll-1])\n",
    "                      #....  if pb, indicate country X\n",
    "                      if lll<2: country='X'\n",
    "                      #.... put all USA states together under the label 'USA'\n",
    "                      usa_states=['AL','AK','AZ','AR','CA','NC','SC','CO','CT','ND','SD','DE','FL','GA','HI','ID','IL','IN','IA','KS','KY','LA','ME','MD','MA','MI','MN','MS','MO','MT','NE','NV','NH','NJ','NM','NY','OH','OK','PA','RI','TN','TX','UT','VT','VA','WV','WA','WI','WY','DC'];\n",
    "                      usa_states2=[f+' ' for f in usa_states];\n",
    "                      if (country[lll-3:lll] == 'USA' or country in usa_states or country[0:3] in usa_states2): country = 'USA'\n",
    "                      #.... put England, Wales, North Ireland, Scotland in UK\n",
    "                      if country in ['ENGLAND','WALES','NORTH IRELAND','SCOTLAND','UNITED KINGDOM']: country='UK'\n",
    "                      if country not in ['USA','UK']: \n",
    "                        country=\" \".join(w.capitalize() for w in country.lower().split())\n",
    "                      if (database =='scopus' and country == 'USA'):country='United States'\n",
    "                      if (database =='scopus' and country == 'UK'):country='United Kingdom'\n",
    "                      outf[\"CU\"].write(\"%d\\t%d\\t%s\\n\" % (id,i,country))\n",
    "                      #... cities\n",
    "                      if \"CI\" in outf:\n",
    "                        if (country==\"Australia\"):city=bar[ll-3]\n",
    "                        else: city=bar[ll-2]\n",
    "                        for nb in range(10): city=city.replace(str(nb),'');\n",
    "                        city=\" \".join(w.capitalize() for w in city.split(' ') if w!='F-')\n",
    "                        city=\" \".join(w for w in city.split(' ') if '-' not in w)\n",
    "                        if len(city) >0:\n",
    "                          outf[\"CI\"].write(\"%d\\t%d\\t%s\\n\" % (id,i,city))\n",
    "                      #... addresses\n",
    "                      add=\"\"\n",
    "                      for j in range(ll - 1): \n",
    "                        add+=\"%s, \" % (bar[j])\n",
    "                      add +=country\n",
    "                      outf[\"AD\"].write(\"%d\\t%d\\t%s\\n\" % (id,i,add))\n",
    "\n",
    "              # abstract (the 2 conditions refer to wos and scopus way to indicate the absence of abstract)\n",
    "              if \"AB\" in outf:\n",
    "                if(article.AB != \"\") and (article.AB != \"[No abstract available]\"):\n",
    "                  outf[\"AB\"].write(\"%d\\t%s\\n\" % (id,article.AB))\n",
    "\n",
    "              # funding text\n",
    "              if \"FT\" in outf:\n",
    "                if(article.FX != \"\"):\n",
    "                  outf[\"FT\"].write(\"%d\\t%s\\n\" % (id,article.FX))\n",
    "\n",
    "              # RP\n",
    "              if \"RP\" in outf:\n",
    "                RPfoo=article.C1.replace('; [',';\\t[').split('\\t')\n",
    "                for elt in RPfoo:\n",
    "                  aux1 = elt.find('[')\n",
    "                  aux2 = elt.find(']')\n",
    "                  if (aux1 < aux2 and aux1>-1):\n",
    "                    auxA = elt[aux1:min(aux2+2,len(elt))]\n",
    "                    AD=elt.replace(auxA,'').replace(';','')\n",
    "                    auxA=auxA.replace('[','').replace(']','').split('; ')\n",
    "                    for a in auxA:\n",
    "                      outf[\"RP\"].write(\"%d\\t%s\\t%s\\n\"% (id, normauthorB(a), AD))  \n",
    "\n",
    "      #.. delete the stuff dealing from the raw file just treated from memory        \n",
    "      del pl\n",
    "\n",
    "  ## END\n",
    "  #... how many papers?\n",
    "  print (\".......\")\n",
    "  print (\"..%d parsed publications in total (filtered out of %d publications)\" % (id + 1, kompt_publis) )\n",
    "  print (\"..%d refs in total, %d with a DOI\" % (kompt_refs, kompt_refs_with_DOI))\n",
    "  #... error in refs?\n",
    "  if kompt_refs > 0: print (\"..%d inadequate refs out of %d (%f%%) have been rejected by this parsing process (no publication year: unpublished, in press, arxiv ...) \" % (kompt_corrupt_refs, kompt_refs, (100.0 * kompt_corrupt_refs) / kompt_refs))\n",
    "  else: print ('..no references found in your dataset. Check whether you extracted your data properly!')\n",
    "  #... journal with unknown cat?\n",
    "  if database =='scopus':\n",
    "    print (\"..%d publis from %d distinct sources with unknown subject category\" % (sum([journalNOT[jr] for jr in journalNOT]), len(journalNOT)))\n",
    "    if len(journalNOT)>0:\n",
    "      foo=len([jr for jr in journalNOT if journalNOT[jr]>=5])\n",
    "      print (\"..%d of these sources with more than 5 papers in the corpus\" % (foo))\n",
    "      filename = 'scopus_sources_without_categories.dat'\n",
    "      print (\"...all these publication sources are listed in %s\" % filename)\n",
    "      with open(filename,'w') as out:\n",
    "        out.write('Publication source\\tISSN\\tNpapers\\n')\n",
    "        fb=list(journalNOT.items())\n",
    "        fb.sort(key=lambda e:-e[1])\n",
    "        for elt in fb:\n",
    "          try: \n",
    "            out.write(\"%s\\t%s\\t%d\\n\" % (elt[0][0],elt[0][1],elt[1]))\n",
    "          except:\n",
    "            out.write(\"[encoding problem]\\t%d\\n\" % (elt[1]))\n",
    "  #... close output files\n",
    "  for elt in outfilenames: outf[elt].close()\n",
    "\n",
    "\n",
    "  #... end\n",
    "  t2=time.time()\n",
    "  print ('..time needed: %ds' % (t2-t1))\n",
    "  return\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "def normauthor(foo):\n",
    "    foo = foo.replace(',','')\n",
    "    aux1 = foo.rfind(' ')\n",
    "    aux2 = len(foo)\n",
    "    foobar = foo.lower().capitalize()\n",
    "    if aux1 > 0: \n",
    "        s1 = foobar[aux1:aux2]\n",
    "        s2 = s1.upper() \n",
    "        foobar = foobar.replace(s1,s2)\n",
    "    aux = foobar.find('-')\n",
    "    if aux > 0: \n",
    "        bar1 = foobar[aux:aux+2]\n",
    "        bar2 = '-' + foobar[aux+1].upper()\n",
    "        foobar = foobar.replace(bar1,bar2)\n",
    "    aux = foobar.find(' ')\n",
    "    if aux > 0 and (aux!=len(foobar)-1): \n",
    "        bar1 = foobar[aux:aux+2]\n",
    "        bar2 = ' ' + foobar[aux+1].upper()\n",
    "        foobar = foobar.replace(bar1,bar2)\n",
    "    return foobar\n",
    "\n",
    "def normauthorB(foo):\n",
    "  bar=foo.lower().split(', ')\n",
    "  foobar=' '.join([e.capitalize() for e in bar[0].split(' ') if len(e)>0])\n",
    "  foobar='-'.join([f[0].upper()+f[1:] for f in foobar.split('-') if len(f)>0])\n",
    "  if len(bar)>1: foobar += ' ' + ''.join([e[0].upper() for e in bar[1].replace('-',' ').split(' ') if len(e)>0])\n",
    "\n",
    "  return foobar\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "biblio_parser(in_dir_parsing,out_dir,database,expert)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data parsing main\n",
    "Do not run the following cell. It is kept for Terminal launch of work"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "def main():\n",
    "# usage: parser.py [-h] [--version] -i DIR -o DIR [-v]\n",
    "# \n",
    "# optional arguments:\n",
    "#   -h, --help            show this help message and exit\n",
    "#   --version             show program's version number and exit\n",
    "#   -o DIR, --output_dir DIR\n",
    "#                         output directory name\n",
    "#   -i DIR, --input_dir DIR\n",
    "#                         input directory name\n",
    "  # Parse line options.\n",
    "  # Try to have always the same input options\n",
    "  parser = argparse.ArgumentParser(description = 'parser')\n",
    "\n",
    "  parser.add_argument('--version', action='version', version='%(prog)s 1.1')\n",
    "  \n",
    "  parser.add_argument(\"-i\", \"--input_dir\", nargs=1, required=True,\n",
    "          action = \"store\", dest=\"in_dir\",\n",
    "          help=\"input directory name\",\n",
    "          metavar='DIR')\n",
    "          \n",
    "  parser.add_argument(\"-o\", \"--output_dir\", nargs=1, required=False,\n",
    "          action = \"store\", dest=\"out_dir\",\n",
    "          help=\"output directory name\",\n",
    "          default = \"Desktop/\",\n",
    "          metavar='DIR')\n",
    "\n",
    "  parser.add_argument(\"-d\", \"--database\",\n",
    "          action = \"store\", dest=\"database\",\n",
    "          default = 'wos',\n",
    "          help=\"database [default %(default)s]\",\n",
    "          metavar='string')  \n",
    "\n",
    "  parser.add_argument(\"-e\", \"--expert\",\n",
    "          action = \"store_true\", dest=\"expert\",\n",
    "          default = False,\n",
    "          help=\"expert mode [default %(default)s]\")\n",
    "\n",
    "  #Analysis of input parameters\n",
    "  args = parser.parse_args()\n",
    "  \n",
    "  if (not os.path.exists(args.in_dir[0])):\n",
    "    print (\"Error: Input directory does not exist: \", args.in_dir[0])\n",
    "    exit()\n",
    "\n",
    "\n",
    "  if args.out_dir == 'Desktop/':\n",
    "    args.out_dir = args.in_dir\n",
    "\n",
    "  if (not os.path.exists(args.out_dir[0])):\n",
    "    args.out_dir[0]=args.in_dir[0]\n",
    "    print (\"Error: Output directory does not exist: \", args.out_dir[0])\n",
    "    exit()\n",
    "\n",
    "  if args.database not in ['wos','scopus']:\n",
    "    print (\"Error: database must be either 'wos' or 'scopus'\")\n",
    "    exit()\n",
    "\n",
    "  ##      \n",
    "\n",
    "  biblio_parser(args.in_dir[0],args.out_dir[0],args.database,args.expert)\n",
    "\n",
    "  return\n",
    "\n",
    "\n",
    "    \n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Corpus description\n",
    "Before doing anything else, you should get a general idea of the content of your database. Execute the following command line:\n",
    "\n",
    "- python BiblioTools3.2/describe_corpus.py -i myprojectname/ -v <br>\n",
    "\n",
    "The options -i indicates the data input folder and the option -v puts the verbose mode on (detailed info about the script process will be displayed in the terminal). This script performs several basic tasks:\n",
    "\n",
    "- it performs a series of frequency analysis, computing the number of occurrences of each item (authors, keywords, references, etc) within the publications of the corpus. These frequencies are automatically stored into several \"freq_xxx.dat\" files within a newly created \"freq\" folder.\n",
    "- it performs a series of generic statistical analysis, storing the numbers of distinct items of each type (e.g. there are x distinct keyword in the corpus ), the distributions of number of occurrences of each item (e.g. there are x keywords appearing in at least y publications) and the distribution of number of items per publication (e.g.there are x% of publications with y keywords). All these statistics are stored in the \"DISTRIBS_itemuse.json\" file.\n",
    "- it also performs a co-occurrence analysis, computing the number of co-occurrence of pairs of items among the top 100 most frequent items of each type (e.g. computing how often the two most used keywords appear together in the same publications). The results of this analysis are stored in the \"coocnetworks.json\" file. More systematic co-occurrence analysis can also be performed with another script, cf the Co-occurrence Maps section below.\n",
    "All the generated files can be opened and read with a simple text editor. The freq_xxx.dat, listing items by order of frequency, can also be read in a spreadsheet software such as excel. All the files are however primarily made to be read in the BiblioMaps interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "\"\"\" \n",
    "   Author : Sebastian Grauwin (http://www.sebastian-grauwin.com/)\n",
    "   Copyright (C) 2017\n",
    "\"\"\"\n",
    "\n",
    "# usage: describe_corpus.py -i DIR [-v]\n",
    "# \n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy\n",
    "import argparse\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "def describe_corpus(in_dir, out_dir, verbose):\n",
    "\n",
    "  ## INITIALIZATION\n",
    "  t1=time.time()\n",
    "  if verbose: print (\"INITIALIZATION\")\n",
    "  # empty dicts for the \"top 100 coocurrence network that will be shown in BiblioMaps\"\n",
    "  myNODES=[]\n",
    "  myLINKS=[]\n",
    "  with open(os.path.join(in_dir, \"articles.dat\") ,\"r\", encoding='utf-8') as file:\n",
    "    # dat file have one trailing blank line at end of file\n",
    "    data_lines=file.read().split(\"\\n\")[:-1]  \n",
    "  # total nb_art  \n",
    "  nb_art=len(data_lines)\n",
    "  print (\"... %d publis\" % nb_art)\n",
    "  # keep track of the database\n",
    "  with open(os.path.join(in_dir, \"database.dat\"),'r') as ff: thedatabase=ff.read().strip('\\n')\n",
    "  # file that will record all proba distribs  \n",
    "  outfile_itemuse=os.path.join(out_dir,'DISTRIBS_itemuse.json')\n",
    "  with open(outfile_itemuse,'w') as out: out.write('{\\n\\t\"database\":\"%s\",\\n\\t\"N\":%d' % (thedatabase, nb_art))\n",
    "\n",
    "\n",
    "  ## ##################################################\n",
    "  ## AUX FUNCTIONS\n",
    "  def do_analysis(stuff,filename,nick):\n",
    "    # HOW MUCH ITEMS BY PUBLI?\n",
    "    stuff.sort(key=lambda e:e[0])\n",
    "    bah=[(my_id, len(set(list(item)))) for my_id, item in itertools.groupby(stuff,key=lambda e:e[0])]\n",
    "    bah.sort(key=lambda x:x[1])\n",
    "    itemdistrib=[(nb,len(list(v))) for nb,v in itertools.groupby(bah,key=lambda e:e[1])]\n",
    "    with open(outfile_itemuse,'a') as out: out.write(',\\n\\t\"q%s\":[%s, %s]'% ( nick,[elt[0] for elt in itemdistrib],[elt[1] for elt in itemdistrib]))\n",
    "\n",
    "    # HOW MUCH TIME AN ITEM APPEARS?\n",
    "    stuff.sort(key=lambda e:e[1])\n",
    "    # duplicates are removed: we count in how many distinct publis an item appears \n",
    "    foo=[(xx,len(set(list(art_id)))) for xx,art_id in itertools.groupby(stuff,key=lambda e:e[1])]\n",
    "    foo.sort(key=lambda x:-x[1])\n",
    "    # generate co-occurrence network\n",
    "    keep=[foo[i] for i in range(min(100,len(foo)))]\n",
    "    generate_cooc(stuff,keep,nick, in_dir);\n",
    "    #\n",
    "    nb_with=len(list(set([e[0] for e in stuff])))\n",
    "    if nb_with < nb_art:\n",
    "      foo+=[('none available', nb_art-nb_with)]\n",
    "      foo.sort(key=lambda x:-x[1])\n",
    "    #\n",
    "    outfile = os.path.join(out_dir, filename)\n",
    "    # output the top used items\n",
    "    mythr=len(foo);kompt=1;\n",
    "    while(mythr>10000):\n",
    "      mythr=len([elt for elt in foo if elt[1]>kompt])\n",
    "      kompt+=1\n",
    "\n",
    "    with open(outfile,'w',encoding='utf-8-sig') as out:\n",
    "      out.write(\"item,count,f\\n\")\n",
    "      for i in range(mythr):\n",
    "        (x,v)=foo[i]\n",
    "        out.write('\"%s\",%d,%.2f\\n' % (x,v,100.0*v/nb_art))\n",
    "    # output the total distrib\n",
    "    distrib=[(nb,len(list(v))) for nb,v in itertools.groupby(foo,key=lambda e:e[1])]\n",
    "    distrib.sort()\n",
    "    xx=[elt[0] for elt in distrib]\n",
    "    yy=[elt[1] for elt in distrib]\n",
    "    yy=list(sum(yy)-numpy.cumsum([0]+yy))\n",
    "    yy=yy[0:-1]\n",
    "    with open(outfile_itemuse,'a') as out: out.write(',\\n\\t\"p%s\":[%s, %s]'% (nick,xx,yy))\n",
    "\n",
    "    # free memory\n",
    "    del stuff, xx, yy, distrib, keep, foo\n",
    "\n",
    "    return\n",
    "\n",
    "  ## ################\n",
    "\n",
    "  def generate_cooc(stuff,keep,nick,in_dir):\n",
    "    if nick in [\"AU\",\"CU\",\"S\",\"S2\",\"K\",\"R\",\"RJ\",\"I\",\"AK\",\"TK\"]:\n",
    "      nodeID={};aux={};comm={};\n",
    "      # nodes\n",
    "      for i in range(len(keep)):\n",
    "        nodeID[keep[i][0]]=i\n",
    "        myNODES.append('{\"type\":\"%s\",\"name\":%d,\"item\":\"%s\",\"size\":%d}' % (nick,i,keep[i][0],keep[i][1]))\n",
    "      # links\n",
    "      for elt in stuff:\n",
    "        if elt[1] in nodeID: \n",
    "          if elt[0] not in aux: aux[elt[0]]=[]\n",
    "          aux[elt[0]].append(elt[1])\n",
    "      for ee in aux:\n",
    "        for itema in aux[ee]:\n",
    "          for itemb in [ff for ff in aux[ee] if ff > itema]:\n",
    "             if (itema, itemb) not in comm: comm[(itema, itemb)]=0\n",
    "             comm[itema,itemb]+=1\n",
    "      for foo in comm:\n",
    "        myLINKS.append('{\"type\":\"%s\",\"source\":%d,\"target\":%d,\"Ncooc\":%d}' % (nick,nodeID[foo[0]],nodeID[foo[1]],comm[foo]))\n",
    "\n",
    "      # free memory\n",
    "      del aux, nodeID, comm\n",
    "    return\n",
    "\n",
    "  ## ################\n",
    "\n",
    "  def treat_item(item, nick, which):\n",
    "    if verbose: print (\"... dealing with %s\" % (item) )\n",
    "    with open(os.path.join(in_dir, item+\".dat\") ,\"r\",encoding='utf-8') as file:\n",
    "      # dat file have one trailing blank line at end of file\n",
    "      data_lines=file.read().split(\"\\n\")[:-1] \n",
    "    aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[which]) for l in data_lines]\n",
    "    do_analysis(aux,'freq_'+item+'.dat',nick)  \n",
    "    # free memory\n",
    "    del data_lines \n",
    "    # end\n",
    "    return\n",
    "\n",
    "  ## ##################################################\n",
    "  ## TREAT DATA \n",
    "  if verbose: print (\"... dealing with years + journals + doctypes + languages\" )\n",
    "  # data_lines was already filtered to keep only publi in the correct time-window\n",
    "  #.. Y\n",
    "  aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[2]) for l in data_lines]\n",
    "  do_analysis(aux,'freq_years.dat','Y')\n",
    "  #.. J\n",
    "  aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[3]) for l in data_lines]\n",
    "  do_analysis(aux,'freq_journals.dat','J')\n",
    "  #.. D\n",
    "  aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[8]) for l in data_lines]\n",
    "  do_analysis(aux,'freq_doctypes.dat','DT') \n",
    "  #.. L\n",
    "  aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[9]) for l in data_lines]\n",
    "  do_analysis(aux,'freq_languages.dat','LA')    \n",
    "  #.. free memory\n",
    "  del data_lines\n",
    "  \n",
    "  treat_item('authors', 'AU', 2)\n",
    "  treat_item('subjects', 'S', 1)\n",
    "  treat_item('institutions', 'I', 2)\n",
    "  treat_item('countries', 'CU', 2)\n",
    "  #treat_item('cities', 'CI', 2) \n",
    "  with open(os.path.join(in_dir, \"database.dat\"),'r') as ff:\n",
    "    if ff.read()=='Scopus': treat_item('subjects2', 'S2', 1)\n",
    "\n",
    "  if verbose: print (\"... dealing with keywords + title words\") \n",
    "  with open(os.path.join(in_dir, \"keywords.dat\") ,\"r\",encoding='utf-8') as file:\n",
    "    # dat file have one trailing blank line at end of file\n",
    "    data_lines=file.read().split(\"\\n\")[:-1]\n",
    "  #.. IK\n",
    "  aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[2]) for l in data_lines if l.split(\"\\t\")[1]=='IK' ]\n",
    "  do_analysis(aux,'freq_keywords.dat','K')\n",
    "  #.. AK\n",
    "  aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[2]) for l in data_lines if l.split(\"\\t\")[1]=='AK' ]\n",
    "  do_analysis(aux,'freq_authorskeywords.dat','AK')  \n",
    "  #.. TK\n",
    "  aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[2]) for l in data_lines if l.split(\"\\t\")[1]=='TK' ]\n",
    "  do_analysis(aux,'freq_titlewords.dat','TK')  \n",
    "  #.. free memory\n",
    "  del data_lines \n",
    "\n",
    "\n",
    "  if verbose: print (\"... dealing with references + references journals\") \n",
    "  with open(os.path.join(in_dir, \"references.dat\") ,\"r\",encoding='utf-8', errors='ignore') as file:\n",
    "    # dat file have one trailing blank line at end of file\n",
    "    data_lines=file.read().split(\"\\n\")[:-1]\n",
    "  #.. R\n",
    "  aux = [(l.split(\"\\t\")[0],\", \".join(l.split(\"\\t\")[1:5]).replace(',0,0','').replace(', 0, 0','').replace(', 0','')) for l in data_lines]\n",
    "  do_analysis(aux,'freq_references.dat','R')  \n",
    "  #.. RJ\n",
    "  aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[3]) for l in data_lines]\n",
    "  do_analysis(aux,'freq_refjournals.dat','RJ')  \n",
    "  #.. free memory\n",
    "  del data_lines\n",
    "\n",
    "  ## ##################################################\n",
    "  ## END  \n",
    "  with open(os.path.join(out_dir,'coocnetworks.json'), 'w') as out:\n",
    "    out.write('{\\n\"nodes\":[\\n\\t%s\\n],\\n\"links\":[\\n\\t%s]\\n}' % (',\\n\\t'.join([elt for elt in myNODES]).replace('\\\\',''), ',\\n\\t'.join([elt for elt in myLINKS]) ))\n",
    "  with open(outfile_itemuse,'a') as out: out.write('\\n}')\n",
    "  if (verbose): print ('Time needed: %ds' % (time.time()-t1))\n",
    "  return\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "verbose = True\n",
    "describe_corpus(in_dir_corpus, out_dir, verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus description main\n",
    "Do not run the following cell. It is kept for Terminal launch of work"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def main():\n",
    "# usage: describe_corpus.py [-h] [--version] -i DIR [-v] [-H INT] [-O]\n",
    "#\n",
    "# optional arguments:\n",
    "#   -h, --help            show this help message and exit\n",
    "#   --version             show program's version number and exit\n",
    "#   -i DIR, --input_dir DIR input directory name \n",
    "  # Parse line options.\n",
    "  # Try to have always the same input options\n",
    "  parser = argparse.ArgumentParser(description = 'parser')\n",
    "\n",
    "  parser.add_argument('--version', action='version', version='%(prog)s 1.1')\n",
    "  \n",
    "  parser.add_argument(\"-i\", \"--input_dir\", nargs=1, required=True,\n",
    "          action = \"store\", dest=\"in_dir\",\n",
    "          help=\"input directory name\",\n",
    "          metavar='DIR')\n",
    "          \n",
    "  parser.add_argument(\"-v\", \"--verbose\",\n",
    "          action = \"store_true\", dest=\"verbose\",\n",
    "          default = False,\n",
    "          help=\"verbose mode [default %(default)s]\")\n",
    "\n",
    "  # Analysis of input parameters\n",
    "  args = parser.parse_args()\n",
    "  if (not os.path.exists(args.in_dir[0])):\n",
    "      print (\"Error, Input directory does not exist: \", args.in_dir[0])\n",
    "      exit()\n",
    "\n",
    "  # Prep out folder\n",
    "  out_dir=os.path.join(args.in_dir[0], 'freqs')\n",
    "  if not os.path.exists(out_dir): os.makedirs(out_dir)\n",
    "\n",
    "  # Proceed with main function\n",
    "  describe_corpus(args.in_dir[0],out_dir,args.verbose)\n",
    "\n",
    "  return\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the data\n",
    "If, upon exploring the nature of the data you realize that before going further you'd prefer to filter your corpus based on some characteristic (keeping only the publications from certain years, using some keywords or references, written by some authors from some countries, etc), you can filter the initial corpus thanks to the script:\n",
    "\n",
    "- python BiblioTools3.2/filter.py -i myprojectname/ -o myprojectname_filtered -v <br>\n",
    "\n",
    "Edit the 'filter.py' file to specify your filters. You'll also need to create a new \"myprojectname_filtered\" main folder before running the script.\n",
    "- create the files articles.dat, addresses.dat, authors.dat, countries.dat, institutions.dat, keywords.dat, references.dat, subjects.dat, subjects2.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "\"\"\" \n",
    "   Author : Sebastian Grauwin (http://www.sebastian-grauwin.com/)\n",
    "   Copyright (C) 2017\n",
    "\"\"\"\n",
    "\n",
    "# usage: filter.py -i DIR -o DIR [-v]\n",
    "# generic filter function to adapt according to your needs\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "def filter_corpus(in_dir,out_dir,verbose):\n",
    "\n",
    "  ## INITIALIZATION\n",
    "  t1=time.time()\n",
    "  # files to filter (will not be taken into account if they don't exist)\n",
    "  filenames=[\"articles\", \"addresses\", \"authors\", \"countries\", \"institutions\", \"journals\", \"keywords\", \"references\", \"subjects\", \"subjects2\", \"AUaddresses\", \"cities\", \"fundingtext\", \"abstracts\"] \n",
    "\n",
    "  ## ON WHICH ITEMS DO YOU WANT TO FILTER? \n",
    "  # 'AU':'author','CU':'country/territory','I':'institution','DT':'document type','LA':'language','Y':'publication year','S':'subject category','S2':'subject subcategory' (scopus only!!),'J':'publication source','K':'keyword','AK':'authors\\' keyword','TK':'title word','R':'reference','RJ':'reference source'\n",
    "  FILTER_ON=[\"Y\",\"J\"]\n",
    "\n",
    "  # combine all filters by \"union\" (one of the conditions needs to be checked)  or \"intersection\" (all conditions needs to be respected)\n",
    "  COMBINE=\"intersection\"\n",
    "\n",
    "  ## FILTER ACCORDING TO WHAT? - Beware, these are case sensitive!\n",
    "  ymin=2010;ymax=2020;                                 \n",
    "  keywords=[\"network\", \"bibliographic\"]\n",
    "  subjects=[\"Physics\", \"Computer Sciences\"]\n",
    "  subsubjects=[\"Education\"]\n",
    "  pubsources=[\"NATURE\", \"SCIENCE\", \"PHYS REV E\"]\n",
    "  authors=[\"Grauwin S\"]\n",
    "  countries=[\"France\", \"Germany\", \"UK\", \"Italy\", \"Spain\", \"Belgium\", \"Netherlands\"]\n",
    "  institutions=[\"CNRS\"]\n",
    "  references=[\"Grauwin S, 2009, P NATL ACAD SCI USA, 106\"]\n",
    "  refsources=[\"P NATL ACAD SCI USA\"]\n",
    "  languages=[\"English\",\"French\"]\n",
    "  doctypes=['Article', 'Review', 'Proceedings Paper', 'Article', 'Article in Press', 'Review', 'Conference Paper', 'Conference Review']\n",
    "\n",
    "\n",
    "\n",
    "  ## ##################################################\n",
    "  ## SELECT ID OF PUBLICATIONS TO KEEP\n",
    "  KEEPID=dict()\n",
    "\n",
    "  with open(os.path.join(in_dir, \"articles.dat\") ,\"r\") as file: data_lines=file.read().split(\"\\n\")[:-1]\n",
    "  nbpub=len(data_lines)  \n",
    "  if (\"Y\" in FILTER_ON): \n",
    "    print(\"...selecting publications with selected publication years\")\n",
    "    KEEPID[\"Y\"]=dict([(l.split(\"\\t\")[0],'') for l in data_lines if (int(l.split(\"\\t\")[2])<=ymax and int(l.split(\"\\t\")[2])>=ymin)])\n",
    "  if (\"J\" in FILTER_ON):\n",
    "    print(\"...selecting publications with selected sources\")\n",
    "    KEEPID[\"J\"]=dict([(l.split(\"\\t\")[0],'') for l in data_lines if (l.split(\"\\t\")[3] in pubsources)])\n",
    "  if (\"DT\" in FILTER_ON):\n",
    "    print(\"...selecting publications with selected doctypes\")\n",
    "    KEEPID[\"DT\"]=dict([(l.split(\"\\t\")[0],'') for l in data_lines if (l.split(\"\\t\")[8] in doctypes)])\n",
    "  if (\"LA\" in FILTER_ON):\n",
    "    print(\"...selecting publications with selected languages\")\n",
    "    KEEPID[\"LA\"]=dict([(l.split(\"\\t\")[0],'') for l in data_lines if (l.split(\"\\t\")[9] in languages)])\n",
    "  del data_lines\n",
    "\n",
    "  if (\"S\" in FILTER_ON):\n",
    "    print(\"...selecting publications with selected subjects\")\n",
    "    with open(os.path.join(in_dir, \"subjects.dat\") ,\"r\", encoding='utf8') as file: data_lines=file.read().split(\"\\n\")[:-1]  \n",
    "    KEEPID[\"S\"]=dict([(l.split(\"\\t\")[0],'') for l in data_lines if (l.split(\"\\t\")[1] in subjects)])\n",
    "    del data_lines\n",
    "\n",
    "  if (\"S2\" in FILTER_ON):\n",
    "    print(\"...selecting publications with selected subsubjects\")\n",
    "    with open(os.path.join(in_dir, \"subjects2.dat\") ,\"r\", encoding='utf8') as file: data_lines=file.read().split(\"\\n\")[:-1]  \n",
    "    KEEPID[\"S2\"]=dict([(l.split(\"\\t\")[0],'') for l in data_lines if (l.split(\"\\t\")[1] in subsubjects)])\n",
    "    del data_lines\n",
    "\n",
    "  if (\"AU\" in FILTER_ON):\n",
    "    print(\"...selecting publications with selected authors\")\n",
    "    with open(os.path.join(in_dir, \"authors.dat\") ,\"r\", encoding='utf8') as file: data_lines=file.read().split(\"\\n\")[:-1]  \n",
    "    KEEPID[\"AU\"]=dict([(l.split(\"\\t\")[0],'') for l in data_lines if (l.split(\"\\t\")[2] in authors)])\n",
    "    del data_lines\n",
    "\n",
    "  if (\"CU\" in FILTER_ON):\n",
    "    print(\"...selecting publications with countries\")\n",
    "    with open(os.path.join(in_dir, \"countries.dat\") ,\"r\", encoding='utf8') as file: data_lines=file.read().split(\"\\n\")[:-1]  \n",
    "    KEEPID[\"CU\"]=dict([(l.split(\"\\t\")[0],'') for l in data_lines if (l.split(\"\\t\")[2] in countries)])\n",
    "    del data_lines\n",
    "\n",
    "  if (\"I\" in FILTER_ON):\n",
    "    print(\"...selecting publications with selected institutions\")\n",
    "    with open(os.path.join(in_dir, \"institutions.dat\") ,\"r\", encoding='utf8') as file: data_lines=file.read().split(\"\\n\")[:-1]  \n",
    "    KEEPID[\"I\"]=dict([(l.split(\"\\t\")[0],'') for l in data_lines if (l.split(\"\\t\")[2] in institutions)])\n",
    "    del data_lines\n",
    "\n",
    "  if (\"K\" in FILTER_ON or \"TK\" in FILTER_ON or \"AK\" in FILTER_ON):\n",
    "    with open(os.path.join(in_dir, \"keywords.dat\") ,\"r\", encoding='utf8') as file: data_lines=file.read().split(\"\\n\")[:-1]  \n",
    "    if (\"K\" in FILTER_ON):\n",
    "      print(\"...selecting publications with selected keywords\")\n",
    "      KEEPID[\"K\"]=dict([(l.split(\"\\t\")[0],'') for l in data_lines if (l.split(\"\\t\")[1]==\"IK\" and l.split(\"\\t\")[2] in keywords)])\n",
    "    if (\"TK\" in FILTER_ON):\n",
    "      print(\"...selecting publications with selected title words\")\n",
    "      KEEPID[\"TK\"]=dict([(l.split(\"\\t\")[0],'') for l in data_lines if (l.split(\"\\t\")[1]==\"TK\" and l.split(\"\\t\")[2] in keywords)])\n",
    "    if (\"AK\" in FILTER_ON):\n",
    "      print(\"...selecting publications with selected authors' jeywords\")\n",
    "      KEEPID[\"AK\"]=dict([(l.split(\"\\t\")[0],'') for l in data_lines if (l.split(\"\\t\")[1]==\"AK\" and l.split(\"\\t\")[2] in keywords)])\n",
    "    del data_lines\n",
    "\n",
    "  if (\"R\" in FILTER_ON or \"RJ\" in FILTER_ON):\n",
    "    with open(os.path.join(in_dir, \"references.dat\") ,\"r\", encoding='utf8') as file: data_lines=file.read().split(\"\\n\")[:-1]  \n",
    "    if (\"R\" in FILTER_ON):\n",
    "      print(\"...selecting publications with selected references\")\n",
    "      KEEPID[\"R\"]=dict([(l.split(\"\\t\")[0],'') for l in data_lines if ( ', '.join([l.split(\"\\t\")[k] for k in range(1,6)]).replace(',0,0','').replace(', 0, 0','').replace(', 0','') in references)])\n",
    "    if (\"RJ\" in FILTER_ON):\n",
    "      print(\"...selecting publications with selected ref sources\")\n",
    "      KEEPID[\"RJ\"]=dict([(l.split(\"\\t\")[0],'') for l in data_lines if (l.split(\"\\t\")[3] in refsources)])\n",
    "    del data_lines\n",
    "\n",
    "  ## combine the filtering conditions by union / intersection\n",
    "  TOKEEP={}\n",
    "  if COMBINE==\"intersection\":\n",
    "    for pubid in KEEPID[FILTER_ON[0]]:\n",
    "      cond=True\n",
    "      for k in range(1,len(FILTER_ON)):\n",
    "        if pubid not in KEEPID[FILTER_ON[k]]: cond=False\n",
    "      if cond: TOKEEP[pubid]=''\n",
    "  if COMBINE==\"union\":\n",
    "    for x in FILTER_ON:\n",
    "      for pubid in KEEPID[x]:TOKEEP[pubid]=''\n",
    "\n",
    "  ## how much?\n",
    "  print (\".. %d publications selected out of %d (%.2f%%)\" % (len(TOKEEP), nbpub, len(TOKEEP)*100.0/nbpub) )\n",
    "\n",
    "  ## ##################################################\n",
    "  ## PROCEED\n",
    "  for fff in filenames:\n",
    "    if (os.path.exists(os.path.join(in_dir, fff+\".dat\"))):\n",
    "      print (\"filtering %s\" % fff)\n",
    "      with open(os.path.join(in_dir, fff+\".dat\") ,\"r\", encoding='utf8') as file: \n",
    "        data_lines=file.read().split(\"\\n\")[:-1] \n",
    "      with open(os.path.join(out_dir, fff+\".dat\") ,\"w\", encoding='utf8') as outfile: \n",
    "        for l in data_lines:\n",
    "          if l.split(\"\\t\")[0] in TOKEEP: outfile.write(\"%s\\n\" % l)\n",
    "        del data_lines\n",
    "  #\n",
    "  with open(os.path.join(in_dir, \"database.dat\") ,\"r\", encoding='utf8') as file: \n",
    "      foo=file.read()\n",
    "  with open(os.path.join(out_dir, \"database.dat\") ,\"w\", encoding='utf8') as outfile: \n",
    "      outfile.write(\"%s\\n\" % foo)\n",
    "  del foo\n",
    "\n",
    "  ## ##################################################\n",
    "  ## END  \n",
    "  print ('END - total time needed: %ds' % (time.time()-t1))\n",
    "  return\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "verbose = True\n",
    "filter_corpus(in_dir_corpus,out_dir,verbose)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the data main\n",
    "Do not run the following cell. It is kept for Terminal launch of work"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def main():\n",
    "# usage: describe_corpus.py [-h] [--version] -i DIR [-v]\n",
    "#\n",
    "# optional arguments:\n",
    "#   -h, --help            show this help message and exit\n",
    "#   --version             show program's version number and exit\n",
    "#   -i DIR, --input_dir DIR input directory name\n",
    "#   -r \n",
    "  # Parse line options.\n",
    "  # Try to have always the same input options\n",
    "  parser = argparse.ArgumentParser(description = 'parser')\n",
    "\n",
    "  parser.add_argument('--version', action='version', version='%(prog)s 1.1')\n",
    "  \n",
    "  parser.add_argument(\"-i\", \"--input_dir\", nargs=1, required=True,\n",
    "          action = \"store\", dest=\"in_dir\",\n",
    "          help=\"input directory name\",\n",
    "          metavar='DIR')\n",
    "\n",
    "  parser.add_argument(\"-o\", \"--output_dir\", nargs=1,\n",
    "          action = \"store\", dest=\"out_dir\",\n",
    "          help=\"output directory name\",\n",
    "          metavar='DIR')\n",
    "\n",
    "  parser.add_argument(\"-v\", \"--verbose\",\n",
    "          action = \"store_true\", dest=\"verbose\",\n",
    "          default = False,\n",
    "          help=\"verbose mode [default %(default)s]\")\n",
    "\n",
    "  #Analysis of input parameters\n",
    "  args = parser.parse_args()\n",
    "  \n",
    "  if (not os.path.exists(args.in_dir[0])):\n",
    "      print (\"Error: Input directory does not exist: \", args.in_dir[0])\n",
    "      exit()\n",
    "\n",
    "  if not os.path.exists(args.out_dir[0]): \n",
    "    os.makedirs(args.out_dir[0])\n",
    "    print (\"Output directory %s, which did not previously exist, was created\" % args.out_dir[0])\n",
    "\n",
    "\n",
    "  ##      \n",
    "\n",
    "  filter_corpus(args.in_dir[0],args.out_dir[0],args.verbose)\n",
    "\n",
    "  return\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliographic Coupling analysis\n",
    "You may execute the bibliographic coupling script with the command line:\n",
    "\n",
    "- python BiblioTools3.2/biblio_coupling.py -i myprojectname/ -v\n",
    "\n",
    "Example of BC clusters network visualisation created in Gephi and of one cluster's ID card, part of a lengthy PDF document listing the ID cards of all clusters.\n",
    "\n",
    "The options -i indicates the data input folder and the option -v puts the verbose mode on. This script execute a number of tasks:\n",
    "\n",
    "- It first creates the BC network, computing Kessler similarities between each pair of publications\n",
    "- It detects a first set of clusters (which we will refer to as \"TOP\") using Thomas Aynaud's python implementation of the louvain algorithm. A second set of clusters (\"SUBTOP\") is then computed by applying the same algorithm to the publications in each TOP cluster, hence providing a 2-level hierarchical partition.\n",
    "- The script will then asked whether you want to create output files for the cluster. By default, the script will output information only for clusters with more than 50 publications, but the script will asked you to confirm / change this threshold. Several files will then be created by the script:\n",
    "    - Output 1: two json files, storing information about the clusters, to be used in the BiblioMaps interface (cf below).\n",
    "    - Output 2a: two .tex files (one for each hierarchical level) you'll have to compile, displaying an \"ID Card\" for each cluster, ie the list of the most frequent keywords, subject, authors, references, etc... used by the publications within this cluster.\n",
    "    - Output 2b: one .gdf file storing information relative to the BC clusters at both the TOP and SUBTOP level. You may open this file with Gephi. You may create visualisations of either the TOP or SUBTOP level by filtering it out, resie the nodes with the \"size\" parameter, run a spatialisation layout algorithm (Force Atlas 2 usually yield satisfaying layouts). You may also choose a label within the few that are available (e.g. 'most_frequent_k' correspond to the most frequent keywords of each cluster). Refer to the Id cards created with latex to know more about the content of each cluster.\n",
    "- Finally, the script proposes you to output the BC network at the publication level, in both a gdf output format that can be opened with Gephi and a json format that can be opened in the BiblioMaps interface. You may either keep the whole network or select only the publications within a given cluster. Keep in mind that both interfaces can only handle a given number of nodes (no more than a few thousands for Gephi, a few hundreds for BiblioMaps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\" \n",
    "   Author : Sebastian Grauwin (http://www.sebastian-grauwin.com/)\n",
    "   Copyright (C) 2017\n",
    "\"\"\"\n",
    "\n",
    "# usage: biblio_coupling.py -i DIR [-o DIR] [-p] [-v]\n",
    "# if the output dir is not specified, the code use the input folder as an output folder \n",
    "# use option -p when the partitions have already been computed and you want to use them\n",
    "# use option -v for verbose informations\n",
    "\n",
    "import os\n",
    "import inspect\n",
    "import numpy\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import Utils.Utils as Utils\n",
    "import Utils.BCUtils as BCUtils\n",
    "import networkx as nx\n",
    "import Utils.community as community\n",
    "from subprocess import call, Popen, PIPE\n",
    "\n",
    "\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "def BC_network(in_dir,out_dir,ini_suffix,verbose,presaved,ask):\n",
    "  ## INI\n",
    "  t1=time.time()\n",
    "  with open(os.path.join(in_dir, \"database.dat\"),'r') as ff:\n",
    "    if ff.read()=='Scopus' and 'S' in stuff_tokeep: stuff_tokeep.append('S2')\n",
    "\n",
    "\n",
    "  part_suffix=ini_suffix+part_suffixBC\n",
    "  ############################################################\n",
    "  ############################################################\n",
    "  ## INPUT DATA\n",
    "  if verbose: print (\"..Initialize\") \n",
    "  with open(os.path.join(in_dir, \"database.dat\"),'r') as ff:\n",
    "    database=ff.read();\n",
    "\n",
    "  nR = dict() # store the number of refs of the articles\n",
    "  py_id = dict(); # store the publication year of the articles\n",
    "  py_ref = dict();  # store the sum of publication year of the articles' refs\n",
    "  tc_id = dict(); # store the Ncitations of the articles (according to records)\n",
    "\n",
    "  src1  = os.path.join(in_dir, \"articles.dat\") \n",
    "  pl = Utils.Article()\n",
    "  pl.read_file(src1)  \n",
    "  nb_art = len(pl.articles) # store the number of articles within database\n",
    "  for l in pl.articles:\n",
    "    #.. pub year\n",
    "    py_id[l.id]=l.year\n",
    "    py_ref[l.id]=0\n",
    "    #..times_cited\n",
    "    tc_id[l.id]=l.times_cited\n",
    "    #.. refs\n",
    "    nR[l.id] = 0\n",
    "  del pl\n",
    "\n",
    "  ############################################################\n",
    "  ############################################################\n",
    "  ## CREATE BC WEIGHT TABLE\n",
    "  if verbose: print (\"..Create the 'Bibliographic Coupling' weight table\")\n",
    " \n",
    "  ref_table = dict() # store the id of articles using a given ref\n",
    "  BC_table = dict() # store the number of common refs between pairs of articles\n",
    "\n",
    "  if verbose: print (\"....loading refs table\")\n",
    "  with open(os.path.join(in_dir, \"references.dat\") ,\"r\", encoding='utf8') as file: \n",
    "    data_lines=file.read().split(\"\\n\")[:-1] \n",
    "  for l in data_lines:\n",
    "    foo =', '.join([l.split(\"\\t\")[k] for k in range(1,6)]).replace(',0,0','').replace(', 0, 0','').replace(', 0','')\n",
    "    pubid=int(l.split(\"\\t\")[0])\n",
    "    pubyear=int(l.split(\"\\t\")[2])\n",
    "    if foo in ref_table: ref_table[foo].append( pubid )\n",
    "    else: ref_table[foo] = [pubid]\n",
    "    if pubyear < 2100:\n",
    "      nR[pubid] += 1\n",
    "      py_ref[pubid] += pubyear\n",
    "  del data_lines\n",
    "\n",
    "  if verbose: print (\"....detecting common references\")\n",
    "  for foo in ref_table:\n",
    "    if len(ref_table[foo]) >= RTUthr:\n",
    "      for i in ref_table[foo]:\n",
    "          for j in ref_table[foo]:\n",
    "              if (i<j):\n",
    "                  if i not in BC_table: BC_table[i] = dict()\n",
    "                  if j not in BC_table[i]: BC_table[i][j] = 0\n",
    "                  BC_table[i][j] += 1 \n",
    "  del ref_table\n",
    "\n",
    "  t2=time.time()\n",
    "  if verbose: print ('..time needed until now: %ds' % (t2-t1))\n",
    "\n",
    "  \n",
    "  ############################################################\n",
    "  ############################################################\n",
    "  ## PREP NETWORK \n",
    "  if verbose: print (\"....define graph in networkx format\")\n",
    "  G=nx.Graph()\n",
    "  TOTW=0\n",
    "  for i in BC_table:\n",
    "    for j in BC_table[i]:\n",
    "      w_ij = (1.0 * BC_table[i][j]) / math.sqrt(nR[i] * nR[j])\n",
    "      if ((BC_table[i][j] >= bcthr) and (nR[i]>=NRthr) and (nR[j]>=NRthr) and (abs(py_id[i]-py_id[j])<=DYthr) and (w_ij>=Wthr)) :\n",
    "        TOTW+=w_ij\n",
    "        G.add_edge(i, j, weight=w_ij, nc=BC_table[i][j])\n",
    "\n",
    "  del(BC_table)\n",
    "\n",
    "  #  compute stuff\n",
    "  if len(G.nodes())==0: \n",
    "    print (\"BC network is empty\")\n",
    "    return\n",
    "\n",
    "  # different ways of computing the degree depending on you having networkx 1 or 2\n",
    "  try:  h=dict(G.degree).values()\n",
    "  except: h=nx.degree(G).values()\n",
    "  avg_degree=sum(h)*1.0/len(h)\n",
    "  avg_weight=2*TOTW*1.0/(len(G.nodes())*(len(G.nodes())-1))\n",
    "\n",
    "\n",
    "  ############################################################\n",
    "  ############################################################\n",
    "  ## COMPUTE BC CLUSTERS\n",
    "  # check that option -p was not forgotten\n",
    "  fooname=\"partitions%s.txt\" % (part_suffix)\n",
    "  filename = os.path.join(in_dir, fooname)\n",
    "  if ask and not presaved  and os.path.isfile(filename):\n",
    "    confirm = input('..Do you want to compute a new partition? If you answer yes, the existing one (\"%s\") will be deleted. If you answer no, the script will continue by using the existing one. (y/n): ' % fooname)\n",
    "    if confirm == 'n': presaved = True\n",
    " \n",
    "  ############################################################\n",
    "  # aux functions c++\n",
    "  def runcpplouvain(XX,which):\n",
    "    call([louvainPATH+\"convert\", \"-i\", \"foo.txt\", \"-o\", \"foo.bin\", \"-w\", \"foo.weights\"])\n",
    "    max_mod=-1; best_topl=0;\n",
    "    for run in range(Nruns):\n",
    "      if Nruns > 1: print (\"......run (%d/%d)\" % (run+1, Nruns))\n",
    "      #.. run the louvain algo\n",
    "      with open(\"foo.tree\", \"w\") as f:\n",
    "        call([louvainPATH+\"louvain\", \"foo.bin\", \"-w\", \"foo.weights\", \"-l\", \"-1\"], stdout=f)  \n",
    "      #.. get the upper level partition and its modularity\n",
    "      output = str(Popen([louvainPATH+\"hierarchy\", \"foo.tree\"], stdout=PIPE).communicate()[0])\n",
    "      call([louvainPATH+\"hierarchy\", \"foo.tree\"])\n",
    "      topl=int(output[output.find('levels:')+8])-1\n",
    "      with open(\"part.txt\",\"w\") as f:\n",
    "        call([louvainPATH+\"hierarchy\", \"foo.tree\", \"-l\", str(topl)], stdout=f)\n",
    "      partfoo=dict((antimapping[x[0]], int(x[1])) for x in numpy.loadtxt('part.txt'))\n",
    "      mod = community.modularity(partfoo, XX)\n",
    "      #.. keep all info if this is the best one\n",
    "      if mod > max_mod:\n",
    "        max_mod = mod\n",
    "        best_topl = topl\n",
    "        if which == 'a':\n",
    "          out_part=dict()\n",
    "          for lev in range(topl):\n",
    "            with open(\"part.txt\",\"w\") as f:\n",
    "              call([louvainPATH+\"hierarchy\", \"foo.tree\", \"-l\", str(lev+1)], stdout=f)\n",
    "              out_part[lev]=dict((antimapping[x[0]], int(x[1])) for x in numpy.loadtxt('part.txt'))\n",
    "        elif which == 't':\n",
    "        \tout_part=partfoo.copy()\n",
    "        else: print ('error in runcpplouvain function')\n",
    "    return [out_part, max_mod, best_topl]\n",
    "\n",
    "  ############################################################\n",
    "  #... extract louvain partition with c++ code\n",
    "  if not presaved and (algo_method == 'c++'):\n",
    "    louvainPATH=os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))+'/Utils/louvain2017/'\n",
    "    if verbose: print (\"..Computing partition with c++ Louvain algo\")\n",
    "    #.. convert labels of graph to consecutive integers\n",
    "    mapping=dict(zip(G.nodes(),range(0,len(G.nodes())))) \n",
    "    antimapping=dict(zip(range(0,len(G.nodes())),G.nodes()))\n",
    "    H = nx.relabel_nodes(G, mapping, copy=True)\n",
    "    #.. output graph\n",
    "    nx.write_weighted_edgelist(H, 'foo.txt')\n",
    "    #.. compute partition\n",
    "    if verbose: print (\"......compute top partition\")\n",
    "    [louvain_partition, mod, topl]=runcpplouvain(G,'a')\n",
    "    part=louvain_partition[topl-1]\n",
    "    if verbose: print (\"..... splitting BC network in %d top-clusters, Q=%.4f\" % ( len(set(part.values())), mod))\n",
    "\n",
    "    #.. second louvain partition\n",
    "    if verbose: print (\"......compute level %d sub-clusters\" % (topl)) \n",
    "    part2=part.copy()\n",
    "    toupdate={}\n",
    "    for com in set(part.values()) :\n",
    "      list_nodes = [nodes for nodes in part.keys() if part[nodes] == com]\n",
    "      # split clusters of size > SIZECUT \n",
    "      if len(list_nodes) > SIZECUT:\n",
    "        if verbose: print (\"...==> splitting cluster %d (N=%d records)\" % (com, len(list_nodes))) \n",
    "        F = G.subgraph(list_nodes).copy()\n",
    "        mapping=dict(zip(F.nodes(),range(0,len(F.nodes())))) \n",
    "        antimapping=dict(zip(range(0,len(F.nodes())),F.nodes()))\n",
    "        H = nx.relabel_nodes(F, mapping, copy=True)\n",
    "        nx.write_weighted_edgelist(H, 'foo.txt')\n",
    "        #.. compute partition\n",
    "        [partfoo, mod, x]=runcpplouvain(F,'t')\n",
    "        # add prefix code\n",
    "        for aaa in partfoo.keys():\n",
    "          partfoo[aaa] = (com+1)*1000 + partfoo[aaa]\n",
    "        nb_comm = len(set(partfoo.values()))\n",
    "        if verbose: print (\"...==> cluster %d (N=%d records) was split in %d sub-clusters, Q=%.4f\" % (com, len(list_nodes), nb_comm, mod))\n",
    "        part2.update(partfoo)\n",
    "\n",
    "      else: # for communities of less than SIZECUT nodes, shift the com label as well\n",
    "        for n in list_nodes: toupdate[n]=''\n",
    "    for n in toupdate: part2[n]+=1\n",
    "    del toupdate\n",
    "\n",
    "    #...clean\n",
    "    os.remove('part.txt')\n",
    "    os.remove('foo.weights')\n",
    "    os.remove('foo.tree')\n",
    "    os.remove('foo.bin')\n",
    "    os.remove('foo.txt')\n",
    "\n",
    "    #... save partitions\n",
    "    #.. I want communtity labels starting from 1 instead of 0 for top level  \n",
    "    for k in louvain_partition[topl-1].keys():\n",
    "      louvain_partition[topl-1][k] += 1\n",
    "    louvain_partition[topl]=part2  \n",
    "    fooname=\"partitions%s.txt\" % (part_suffix)\n",
    "    with open(os.path.join(in_dir, fooname),\"w\") as f_out:\n",
    "      f_out.write('%s' % json.dumps(louvain_partition))\n",
    "\n",
    "    if verbose: print ('..time needed until now: %ds' % (time.time()-t1))\n",
    "\n",
    "  ############################################################\n",
    "  # aux functions python\n",
    "  def runpythonlouvain(XX):\n",
    "    max_mod=-1\n",
    "    for run in range(Nruns):\n",
    "      if Nruns > 1: print (\"......run (%d/%d)\" % (run+1, Nruns))\n",
    "      foodendogram = community.generate_dendogram(XX, part_init=None)\n",
    "      partfoo = community.partition_at_level(foodendogram,len(foodendogram)-1)\n",
    "      mod=community.modularity(partfoo, XX)\n",
    "      if mod > max_mod:\n",
    "        max_mod = mod\n",
    "        part=partfoo.copy()\n",
    "        dendogram=foodendogram.copy()\n",
    "    return [dendogram, part, max_mod]\n",
    "\n",
    "  ############################################################\n",
    "  #... extract louvain partition with python code\n",
    "  if not presaved and (algo_method == 'python'):\n",
    "    if verbose: print (\"..Computing partition with (python) networkx Louvain algo\")\n",
    "    if verbose: print (\"......compute top partition\")\n",
    "    [dendogram, part, mod]=runpythonlouvain(G)\n",
    "    if verbose: print (\"..... splitting BC network in %d top-clusters, Q=%.4f\" % ( len(set(part.values())), mod))\n",
    "\n",
    "    #... second louvain partition\n",
    "    if verbose: print (\"......compute level %d sub-clusters\" % (len(dendogram)))\n",
    "    part2=part.copy()\n",
    "    toupdate={}\n",
    "    for com in set(part.values()) :\n",
    "      list_nodes = [nodes for nodes in part.keys() if part[nodes] == com]\n",
    "      # split clusters of size > SIZECUT \n",
    "      if len(list_nodes) > SIZECUT: \n",
    "        H = G.subgraph(list_nodes).copy()\n",
    "        [dendo2, partfoo, mod]=runpythonlouvain(H)\n",
    "        dendo2 = community.generate_dendogram(H, part_init=None)\n",
    "        partfoo = community.partition_at_level(dendo2,len(dendo2)-1)\n",
    "        # add prefix code\n",
    "        for aaa in partfoo.keys():\n",
    "          partfoo[aaa] = (com+1)*1000 + partfoo[aaa]\n",
    "        nb_comm = len(set(partfoo.values()))\n",
    "        if verbose: print (\"... ==> cluster %d (N=%d records) was split in %d sub-clusters, Q=%.3f\" % (com, len(list_nodes), nb_comm, mod))\n",
    "        part2.update(partfoo)\n",
    "      else: # for communities of less than SIZECUT nodes, shift the com label as well\n",
    "        for n in list_nodes: toupdate[n]=''\n",
    "    for n in toupdate: part2[n]+=1\n",
    "    del toupdate\n",
    "\n",
    "    #... save partitions\n",
    "    louvain_partition=dict()\n",
    "    for lev in range(len(dendogram)):\n",
    "      louvain_partition[lev]=community.partition_at_level(dendogram, lev) \n",
    "    #.. I want communtity labels starting from 1 instead of 0 for top level  \n",
    "    for k in louvain_partition[len(dendogram)-1].keys():\n",
    "      louvain_partition[len(dendogram)-1][k] += 1\n",
    "    louvain_partition[len(dendogram)]=part2  \n",
    "\n",
    "    fooname=\"partitions%s.txt\" % (part_suffix)\n",
    "    with open(os.path.join(in_dir, fooname),\"w\") as f_out:\n",
    "      f_out.write('%s' % json.dumps(louvain_partition))\n",
    "\n",
    "    num_levels=len(louvain_partition)-1\n",
    "    if verbose: print ('..time needed until now: %ds' % (time.time()-t1))\n",
    "    \n",
    "  ############################################################\n",
    "  #... upload previously computed partition\n",
    "  if presaved:\n",
    "    if verbose: print (\"....uploading previously computed clusters\")\n",
    "    #... upload partition\n",
    "    fooname=\"partitions%s.txt\" % (part_suffix)\n",
    "    filename = os.path.join(in_dir, fooname)\n",
    "    if not os.path.isfile(filename):\n",
    "      print ('....file %s does not exists' % filename)\n",
    "      return\n",
    "    ffin = open(filename,'r')\n",
    "    foo = ffin.read()\n",
    "    lines = foo.split('\\n')\n",
    "    #\n",
    "    louvain_partition = json.loads(lines[0])\n",
    "    num_levels=len(louvain_partition)-1\n",
    "    if verbose: print (\"....%d+1 levels\" % num_levels)\n",
    "    #... convert keys back into integer (json dump put them in strings)\n",
    "    auxlist = list(louvain_partition.keys())\n",
    "    for k in auxlist:\n",
    "      louvain_partition[int(k)]=dict() \n",
    "      for kk in louvain_partition[k].keys():\n",
    "        louvain_partition[int(k)][int(kk)]=louvain_partition[k][kk] \n",
    "      del louvain_partition[k] \n",
    "\n",
    "  # compute modularities Qs\n",
    "  Qtop=community.modularity(louvain_partition[num_levels-1], G)\n",
    "  Qsub=community.modularity(louvain_partition[num_levels], G)\n",
    "\n",
    "  ############################################################\n",
    "  #... output infos\n",
    "  print (\"....There are %d publications in the database\" % (nb_art))\n",
    "  print (\"....There are %d publications in the BC network\" % (len(G.nodes())))\n",
    "  if len(part_suffix)>len(ini_suffix): print(\"\\n......BEWARE: THRESHOLDS HAVE BEEN MODIFIED FROM THEIR DEFAULT VALUES IN THE PYTHON FILE\\n......(%s)\\n\" % (initexthr))\n",
    "  print (\"....BC networks: Average degree: %.3f, average weight: 1/%d, Qtop: %.3f, Qsub: %.3f\" % (avg_degree, round(1/avg_weight), Qtop, Qsub)) \n",
    "  if verbose: print ('..time needed until now: %ds' % (time.time()-t1))\n",
    "\n",
    "\n",
    "  ########################################################################################################################\n",
    "  ########################################################################################################################\n",
    "  ## EXTRACT CLUSTERS TABLES / INFOS\n",
    "  if ask:\n",
    "    confirm = input(\"..Prep and output json, gephi and tex files at 'clusters' level? (y/n): \")\n",
    "  else: confirm='y'\n",
    "  if confirm == 'y':\n",
    "\n",
    "    # prep partitions\n",
    "    if verbose: print ('..Prep partitions')\n",
    "    num_levels=len(louvain_partition)-1\n",
    "    part=louvain_partition[(num_levels - 1)].copy()\n",
    "    part2=louvain_partition[num_levels].copy()\n",
    "    #.. top partition\n",
    "    list_nodes = dict();\n",
    "    size_top = dict();\n",
    "    dgcl_id=dict();\n",
    "    wdgcl_id=dict();\n",
    "    w_id=dict();\n",
    "    Qint=dict();\n",
    "    for com in set(part.values()):\n",
    "      auxlist=[nodes for nodes in part.keys() if part[nodes] == com]\n",
    "      list_nodes[com] = dict()\n",
    "      for elt in auxlist: list_nodes[com][elt]='' \n",
    "      size_top[com] = len(list_nodes[com])\n",
    "      H = G.subgraph(auxlist).copy()\n",
    "      partfoo = dict((k, part2[k]) for k in list_nodes[com])\n",
    "      mod = community.modularity(partfoo, H)\n",
    "      Qint[com]=mod+0.0000001\n",
    "      # density\n",
    "      for id1 in list_nodes[com]:\n",
    "        dgcl_id[id1]=len([id2 for id2 in list_nodes[com] if (id2 in G.adj[id1] and id1!=id2)])\n",
    "        wdgcl_id[id1]=sum([G.adj[id1][id2][\"weight\"] for id2 in list_nodes[com] if (id2 in G.adj[id1] and id1!=id2)])\n",
    "        w_id[id1]=sum([G.adj[id1][id2][\"weight\"] for id2 in G.adj[id1] if (id1!=id2)])\n",
    "        #dgcl_id[id1]=len([id2 for id2 in list_nodes[com] if ((id1,id2) in G.edges() and id1!=id2)])\n",
    "        #wdgcl_id[id1]=sum([G.edges[(id1,id2)][\"weight\"] for id2 in list_nodes[com] if ((id1,id2) in G.edges and id1!=id2)])\n",
    "        #w_id[id1]=sum([G.edges[(id1,id2)][\"weight\"] for id2 in list_nodes[com] if ((id1,id2) in G.edges and id1!=id2)])\n",
    "    #.. subtop partition\n",
    "    list_nodes2 = dict();\n",
    "    size_subtop = dict();\n",
    "    dgcl_id2=dict();\n",
    "    wdgcl_id2=dict();\n",
    "    my_top = dict();\n",
    "    for com in set(part2.values()) :\n",
    "      auxlist=[nodes for nodes in part2.keys() if part2[nodes] == com]\n",
    "      list_nodes2[com] = dict()\n",
    "      for elt in auxlist: list_nodes2[com][elt]=''       \n",
    "      #list_nodes2[com] = [nodes for nodes in part2.keys() if part2[nodes] == com]\n",
    "      size_subtop[com] = len(list_nodes2[com])   \n",
    "      my_top[com] = part[auxlist[0]]\n",
    "      # density\n",
    "      for id1 in list_nodes2[com]:\n",
    "        dgcl_id2[id1]=len([id2 for id2 in list_nodes2[com] if id2 in G.adj[id1] and id1!=id2])\n",
    "        wdgcl_id2[id1]=sum([G.adj[id1][id2][\"weight\"] for id2 in list_nodes2[com] if (id2 in G.adj[id1] and id1!=id2)])\n",
    "        #dgcl_id2[id1]=len([id2 for id2 in list_nodes2[com] if ((id1,id2) in G.edges and id1!=id2)])\n",
    "        #wdgcl_id2[id1]=sum([G.edges[(id1, id2)][\"weight\"] for id2 in list_nodes2[com] if ((id1,id2) in G.edges and id1!=id2)])\n",
    "    t2=time.time()\n",
    "    print ('..time needed until now: %ds' % (t2-t1))\n",
    "\n",
    "    #.. extract\n",
    "    print (\"..Choose thresholds (by default, the top clusters for which a sub-partition has been computed are those of size > %d)\" % SIZECUT)\n",
    "    if ask:\n",
    "      topthr  = input(\"....keep top clusters of size > to:\")\n",
    "      subtopthr  = input(\"....keep subtop clusters of size > to:\")\n",
    "    else: \n",
    "      topthr=min(SIZECUT, len(G.nodes())/100);\n",
    "      subtopthr=min(10, len(G.nodes())/100);\n",
    "\n",
    "    confirm='n'\n",
    "    while confirm != 'y': \n",
    "      topthr=int(topthr);\n",
    "      subtopthr=int(subtopthr); \n",
    "      #..infos  top        \n",
    "      keep=[com for com in size_top if size_top[com]>topthr]\n",
    "      top_n_sup_thr = len(keep);\n",
    "      top_size_sup_thr = sum([size_top[com] for com in keep]);\n",
    "      print (\"....Top clusters with size > %d: %d publications gathered in %d clusters\" % ( topthr, top_size_sup_thr, top_n_sup_thr))\n",
    "      #..infos  subtop      \n",
    "      keep2=[com for com in size_subtop if size_subtop[com]>subtopthr and my_top[com] in keep]\n",
    "      sub_n_sup_thr = len(keep2);\n",
    "      sub_size_sup_thr = sum([size_subtop[com] for com in keep2]);\n",
    "      print (\"....Their subtop clusters: %d publications gathered in %d clusters\" % ( sub_size_sup_thr, sub_n_sup_thr))\n",
    "      #..confirm\n",
    "      if ask: confirm = input(\"....do you confirm? (y/n): \")\n",
    "      else: confirm='y' \n",
    "      if confirm == 'n':\n",
    "        topthr  = input(\"......keep top clusters of size > to:\")\n",
    "        subtopthr  = input(\"......keep subtop clusters of size > to:\")\n",
    "\n",
    "    # order by size\n",
    "    fff=[[com, size_top[com]] for com in size_top if size_top[com]>topthr]\n",
    "    fff.sort(key=lambda e:-e[1])\n",
    "    keep=[f[0] for f in fff]\n",
    "    keep2=dict()\n",
    "    for topcom in keep:\n",
    "      fff=[[com, size_subtop[com]] for com in size_subtop if size_subtop[com]>subtopthr and my_top[com]==topcom]\n",
    "      fff.sort(key=lambda e:-e[1])\n",
    "      keep2[topcom]=[f[0] for f in fff]\n",
    "    \n",
    "    t2=time.time()\n",
    "    if verbose: print ('....total time needed: %ds' % (t2-t1))\n",
    "    ############################################################\n",
    "    ## PREP LINKS \n",
    "    \n",
    "    #.. compute links\n",
    "    if verbose: print (\"..Compute clusters links\")\n",
    "    linkW=dict();\n",
    "    \n",
    "    #...top-top\n",
    "    if verbose: print (\".... top-top links\")\n",
    "    myL = mylinks(list_nodes,list_nodes,keep,keep,G,0)\n",
    "    linkW.update(myL)\n",
    "    maxtopL = max([0.000001]+[elt[0] for elt in myL.values()]);\n",
    "    del myL;\n",
    "    #...subtop-subtop\n",
    "    if verbose: print (\".... subtop-subtop links\")\n",
    "    fkeep=[]\n",
    "    for topcom in keep: fkeep+=[com for com in keep2[topcom]]\n",
    "    myL=mylinks(list_nodes2,list_nodes2,fkeep,fkeep,G,0)\n",
    "    linkW.update(myL);\n",
    "    maxsubL = max([0.000001]+[elt[0] for elt in myL.values()]);\n",
    "    del myL;\n",
    "    \n",
    "    t2=time.time()\n",
    "    if verbose: print ('....total time needed: %ds' % (t2-t1))\n",
    "\n",
    "    ############################################################\n",
    "    #####################   QUANTITATIVES MEASURES\n",
    "    ############################################################\n",
    "    if verbose: print (\"..Computing quantitative measures\")\n",
    "\n",
    "    # aux function to compute h index\n",
    "    def extract_h(mylist):\n",
    "      mylist.sort()\n",
    "      h=1\n",
    "      if (len(mylist)==0): \n",
    "        h=0\n",
    "      else: \n",
    "        while (h < len(mylist) and mylist[-h] > h): h+=1\n",
    "      #  \n",
    "      return h\n",
    "\n",
    "    ## global measures\n",
    "    Wtot=sum([w_id[id1] for id1 in w_id])*1.0/2\n",
    "    NN = len(G.nodes());\n",
    "    QQ = community.modularity(part, G);\n",
    "    TCTC = sum([tc_id[id1] for id1 in part])*1.0/NN;\n",
    "    HH = extract_h([tc_id[id1] for id1 in part]);\n",
    "    PYPY = sum([py_id[id1] for id1 in part])*1.0/NN;\n",
    "    YMIN = min([py_id[id1] for id1 in part]);\n",
    "    YMAX = max([py_id[id1] for id1 in part]);\n",
    "    NRNR = sum([nR[id1] for id1 in part])*1.0/NN;     \n",
    "    ARAR = sum([nR[id1]*py_id[id1]-py_ref[id1] for id1 in part])*1.0/(NN*NRNR)\n",
    "    # different ways of computing the degree depending on you having networkx 1 or 2\n",
    "    try:  h=dict(G.degree).values()\n",
    "    except: h=nx.degree(G).values()\n",
    "    avg_degree=sum(h)*1.0/len(h)\n",
    "    avg_weight=Wtot*1.0/(NN*(NN-1))\n",
    "\n",
    "    ## top cluster measures\n",
    "    comm_nr=dict()\n",
    "    comm_ar=dict()\n",
    "    comm_py=dict()\n",
    "    comm_tc=dict()\n",
    "    comm_h=dict()\n",
    "    comm_dg=dict()\n",
    "    comm_win=dict()\n",
    "    comm_q=dict()\n",
    "\n",
    "    for com in keep:\n",
    "      # compute stuff\n",
    "      comm_nr[com]=sum([nR[id1] for id1 in list_nodes[com]])*1.0/size_top[com]\n",
    "      comm_ar[com]=sum([nR[id1]*py_id[id1]-py_ref[id1] for id1 in list_nodes[com]])*1.0/(comm_nr[com]*size_top[com])\n",
    "      comm_py[com]=sum([py_id[id1] for id1 in list_nodes[com]])*1.0/size_top[com]\n",
    "      comm_tc[com]=sum([tc_id[id1] for id1 in list_nodes[com]])*1.0/size_top[com]\n",
    "      comm_h[com]=extract_h([tc_id[id1] for id1 in list_nodes[com]])\n",
    "      comm_dg[com]=sum([dgcl_id[id1] for id1 in list_nodes[com]])*1.0/size_top[com]\n",
    "      W=sum([wdgcl_id[id1] for id1 in list_nodes[com]])*1.0/2;\n",
    "      comm_win[com]=2.0*W/(size_top[com]*(size_top[com]-1))\n",
    "      # module\n",
    "      # q1 = fraction of total weight within com \n",
    "      # q2 = fraction of total weight attached to nodes in com\n",
    "      q1=W*1.0/Wtot;\n",
    "      q2=sum([w_id[id1] for id1 in list_nodes[com]])*1.0/(2*Wtot);\n",
    "      comm_q[com]=q1-q2*q2;\n",
    "\n",
    "    ## subtop cluster measures\n",
    "    Bcomm_nr=dict()\n",
    "    Bcomm_ar=dict()\n",
    "    Bcomm_py=dict()\n",
    "    Bcomm_tc=dict()\n",
    "    Bcomm_h=dict()\n",
    "    Bcomm_dg=dict()\n",
    "    Bcomm_win=dict()\n",
    "    Bcomm_q=dict()\n",
    "\n",
    "    for topcom in keep:\n",
    "      for com in keep2[topcom]:\n",
    "        # compute stuff\n",
    "        Bcomm_nr[com]=sum([nR[id1] for id1 in list_nodes2[com]])*1.0/size_subtop[com]\n",
    "        Bcomm_ar[com]=sum([nR[id1]*py_id[id1]-py_ref[id1] for id1 in list_nodes2[com]])*1.0/(Bcomm_nr[com]*size_subtop[com])\n",
    "        Bcomm_py[com]=sum([py_id[id1] for id1 in list_nodes2[com]])*1.0/size_subtop[com]\n",
    "        Bcomm_tc[com]=sum([tc_id[id1] for id1 in list_nodes2[com]])*1.0/size_subtop[com]\n",
    "        Bcomm_h[com]=extract_h([tc_id[id1] for id1 in list_nodes2[com]])\n",
    "        Bcomm_dg[com]=sum([dgcl_id[id1] for id1 in list_nodes2[com]])*1.0/size_subtop[com]\n",
    "        W=sum([wdgcl_id[id1] for id1 in list_nodes2[com]])*1.0/2;\n",
    "        Bcomm_win[com]=2.0*W/(size_subtop[com]*(size_subtop[com]-1))\n",
    "        # module\n",
    "        q1=W*1.0/Wtot;\n",
    "        q2=sum([w_id[id1] for id1 in list_nodes2[com]])*1.0/(2*2*Wtot);\n",
    "        Bcomm_q[com]=q1-q2*q2;\n",
    "\n",
    "\n",
    "    ############################################################\n",
    "    #####################   PREP CLUSTERS STUFF\n",
    "    ############################################################\n",
    "\n",
    "    ## CLUSTERS ITEMS\n",
    "    print(\".... Compute clusters' most frequent items\")\n",
    "    \n",
    "    #.. extract top\n",
    "    if verbose: print (\"....Computing most frequent items in top clusters\")\n",
    "    (quantR, stuff, avail) = BCUtils.comm_tables(in_dir,part,topthr,verbose)\n",
    "    comm_label=dict()\n",
    "    for com in stuff['K']: comm_label[com] = extract_labels(stuff['K'][com])\n",
    "\n",
    "    #.. extract subtop\n",
    "    if verbose: print (\"....Computing most frequent items in subtop clusters\")\n",
    "    (BquantR, Bstuff, Bavail) = BCUtils.comm_tables(in_dir,part2,subtopthr,verbose)  \n",
    "    \n",
    "    Bcomm_label=dict();\n",
    "    for com in Bstuff['K']: Bcomm_label[com] = extract_labels(Bstuff['K'][com]);\n",
    "    \n",
    "    \"\"\"\n",
    "    ## CLUSTERS % IN ALL SUBJECTS\n",
    "    #.. extract top\n",
    "    if verbose: print (\"....Computing frequencies of aggregated items in top clusters\")\n",
    "    (listS, groupS)=BCUtils.comm_groups(in_dir,part,topthr,verbose);\n",
    "\n",
    "    #.. extract subtop \n",
    "    if verbose: print (\"....Computing frequencies of aggregated items in subtop clusters\")\n",
    "    (BlistS, BgroupS)=BCUtils.comm_groups(in_dir,part2,subtopthr,verbose);\n",
    "    \"\"\"\n",
    "\n",
    "    ## CLUSTERS AR\n",
    "    #.. extract top \n",
    "    if verbose: print (\"....Computing most cited publications & authors + most representative publications in top clusters\")\n",
    "    (CR_papers,CR_authors) = BCUtils.comm_AR(in_dir,part,topthr,dgcl_id,verbose)\n",
    "\n",
    "    #.. extract subtop \n",
    "    if verbose: print (\"....Computing most cited publications & authors + most representative publications in subtop clusters\")\n",
    "    (BCR_papers,BCR_authors) = BCUtils.comm_AR(in_dir,part2,subtopthr,dgcl_id2,verbose)  \n",
    "\n",
    "    t2=time.time()\n",
    "    if verbose: print ('....total time needed: %ds' % (t2-t1))\n",
    "\n",
    "\n",
    "    ############################################################\n",
    "    #####################   OUTPUT GEPHI FILES\n",
    "    ############################################################\n",
    "    ## ini\n",
    "    if verbose: print (\"..output gdf file for gephi\")\n",
    "    viz_dir=os.path.join(out_dir,'gdffiles')\n",
    "    if not os.path.exists(viz_dir): os.makedirs(viz_dir)\n",
    "    name = \"BCclusters%s.gdf\" % (part_suffix)\n",
    "    with open(os.path.join(viz_dir, name),'w') as f_gephi:\n",
    "      ## ... prep nodes\n",
    "      if verbose: print (\"....nodes\")\n",
    "      f_gephi.write(\"nodedef>name VARCHAR,type VARCHAR,label_f VARCHAR,label_s VARCHAR,label_fs VARCHAR,label VARCHAR,topcom VARCHAR,colortop VARCHAR,size DOUBLE,Qint DOUBLE\\n\")\n",
    "      for com in keep:\n",
    "        f_gephi.write(\"%d,'top','%s','%s','%s','%s',%d,color%d,%d,%1.0f\\n\" % (com, comm_label[com][0],comm_label[com][1],comm_label[com][2],comm_label[com][3],com,com,size_top[com], Qint[com]) ) \n",
    "        # corresponding subtop\n",
    "        for c in keep2[com]:\n",
    "          f_gephi.write(\"%d,'subtop','%s','%s','%s','%s',%d,color%d,%d,%1.0f\\n\" % (c, Bcomm_label[c][0],Bcomm_label[c][1],Bcomm_label[c][2],Bcomm_label[c][3],com,com,size_subtop[c], Qint[com]) ) \n",
    "\n",
    "      ## ... prep links\n",
    "      if verbose: print (\"....links\")\n",
    "      f_gephi.write(\"edgedef>node1 VARCHAR,node2 VARCHAR,num_links DOUBLE,weight DOUBLE, logweight DOUBLE\\n\")\n",
    "      for elt in linkW:\n",
    "        if linkW[elt][0] > 0.000001:\n",
    "          f_gephi.write('%d,%d,%d,%.9f,%.2f\\n' % (elt[0],elt[1],linkW[elt][1],linkW[elt][0],6 + math.log(linkW[elt][0])/math.log(10)))\n",
    "\n",
    "\n",
    "    ############################################################\n",
    "    #####################   OUTPUT JSON FILES\n",
    "    ############################################################\n",
    "\n",
    "    #.. output stuff\n",
    "    print (\"..output stuff in json files\")\n",
    "    viz_dir=os.path.join(out_dir,'jsonfiles')\n",
    "    if not os.path.exists(viz_dir): os.makedirs(viz_dir)\n",
    "    if verbose: print (\"....clusters network\")\n",
    "    name = \"BCclusters%s.json\" % (part_suffix)\n",
    "    kompt=0\n",
    "    group=1\n",
    "    with open(os.path.join(viz_dir,name),'w', encoding='utf8') as outfile:\n",
    "      # nodes\n",
    "      outfile.write('{\\n\\t\"nodes\":[\\n')\n",
    "      for com in keep:\n",
    "        bigstuff=','.join(['\"'+x+'\":%s' % (cvst(stuff,com,x,howmuch_tokeep[x])) for x in stuff_tokeep])\n",
    "        available=','.join(['\"'+x+'\":%.2f' % (avail[x][com]) for x in stuff_tokeep])\n",
    "        if kompt>0:outfile.write(',\\n')\n",
    "        outfile.write('\\t\\t{\"name\":\"%d\",\"level\":0,\"group\":%d,\"size\":%d,\"id_top\":%d,\"Qint\":%.3f,\"q\":%.3f,\"NR\":%.2f,\"cohesion\":[%.2f,%.3f,%.3f],\"hotness\":[%.2f,%.2f,%.2f,%d],\"label\":\"foolabel%d\",\"available\":{%s},\"stuff\":{%s,\"MCP\":%s,\"MRP\":%s,\"MCAU\":%s} }' % (com, group, size_top[com], com, Qint[com], comm_q[com], comm_nr[com], comm_dg[com], 2*comm_dg[com]/(size_top[com]-1), 1000*comm_win[com], comm_py[com], comm_ar[com], comm_tc[com], comm_h[com], com, available, bigstuff, cvar(CR_papers[com]['MC'],10), cvar(CR_papers[com]['MR'],10), cvau(CR_authors[com]['MC'],20)  ))\n",
    "        kompt+=1;\n",
    "        for c in keep2[com]:\n",
    "          bigstuff=','.join(['\"'+x+'\":%s' % (cvst(Bstuff,c,x,howmuch_tokeep[x])) for x in stuff_tokeep])\n",
    "          available=','.join(['\"'+x+'\":%.2f' % (Bavail[x][c]) for x in stuff_tokeep])\n",
    "          outfile.write(',\\n\\t\\t{\"name\":\"%d\",\"level\":1,\"group\":%d,\"size\":%d,\"id_top\":%d,\"Qint\":%.3f,\"q\":%.3f,\"NR\":%.2f,\"cohesion\":[%.2f,%.3f,%.3f],\"hotness\":[%.2f,%.2f,%.2f,%d],\"label\":\"foolabel%d\",\"available\":{%s},\"stuff\":{%s,\"MCP\":%s,\"MRP\":%s,\"MCAU\":%s} }' % (c, group, size_subtop[c], com, Qint[com], Bcomm_q[c], Bcomm_nr[c], Bcomm_dg[c], 2*Bcomm_dg[c]/(size_subtop[c]-1), 1000*Bcomm_win[c], Bcomm_py[c], Bcomm_ar[c], Bcomm_tc[c], Bcomm_h[c], c, available, bigstuff, cvar(BCR_papers[c]['MC'],10), cvar(BCR_papers[c]['MR'],10), cvau(BCR_authors[c]['MC'],20) ))\n",
    "        group+=1;  \n",
    "      outfile.write('\\n\\t],\\n')  \n",
    "      \n",
    "      # links\n",
    "      numlink=0;\n",
    "      outfile.write('\\t\"links\":[\\n')\n",
    "      for elt in linkW:\n",
    "        if linkW[elt][0] > 0.000001:\n",
    "          if numlink>0:outfile.write(',\\n')\n",
    "          outfile.write('\\t\\t{\"source\":\"%d\",\"target\":\"%d\",\"weight\":%.6f}' % (elt[0],elt[1],linkW[elt][0]))\n",
    "          numlink+=1\n",
    "      outfile.write('\\n\\t]\\n}')\n",
    "      \n",
    "\n",
    "    #.. output default var\n",
    "    if verbose: print (\"....default parameters\")\n",
    "    name = \"BCdefaultVAR%s.json\" % (part_suffix)\n",
    "    with open(os.path.join(viz_dir,name),'w') as outfile:\n",
    "      outfile.write('{\\n')\n",
    "      \"\"\"\n",
    "      outfile.write('\\t\"listS\":[[\"%s\",%.3f]' % (listS[0][0],listS[0][1]) ) \n",
    "      for k in range(1,len(listS)): outfile.write(',[\"%s\",%.3f]' % (listS[k][0],listS[k][1]) ) \n",
    "      outfile.write('],\\n')  \n",
    "      outfile.write('\\t\"BlistS\":[[\"%s\",%.3f]' % (BlistS[0][0],BlistS[0][1]) ) \n",
    "      for k in range(1,len(BlistS)): outfile.write(',[\"%s\",%.3f]' % (BlistS[k][0],BlistS[k][1]) ) \n",
    "      outfile.write('],\\n') \n",
    "      \"\"\"\n",
    "      keepb=[]\n",
    "      for com in keep: keepb+=keep2[com]\n",
    "      outfile.write('\\t\"YMIN\":%d,\\n\\t\"YMAX\":%d,\\n' % (YMIN, YMAX))\n",
    "      outfile.write('\\t\"levels\":[\"Topics\", \"Subtopics\"],\\n')\n",
    "      outfile.write('\\t\"Nbc\":%d,\\n' % (len(G.nodes())))\n",
    "      outfile.write('\\t\"Q\":[%.3f,%.3f],\\n' % (Qtop,Qsub))\n",
    "      outfile.write('\\t\"ncom\":[%d, %d],\\n' % (len(keep), len(keepb)))  \n",
    "      outfile.write('\\t\"TOPmaxNodeSize\":%d,\\n' % (max(size_top.values())) )\n",
    "      outfile.write('\\t\"TOPmaxLinkWeight\":%.6f,\\n' % maxtopL)\n",
    "      outfile.write('\\t\"TOPWrange\":[-6,%.2f,-6],\\n' % (math.log(maxtopL)/math.log(10)) )\n",
    "      outfile.write('\\t\"TOPshowthr\":1.05,\\n' )\n",
    "      outfile.write('\\t\"TOPforce\":[0.2,-1000,25],\\n' )\n",
    "      outfile.write('\\t\"TOPpositions\":[%s],\\n' % (','.join(['['+str(cc)+','+str(\"%.2f\" % (10+10*math.cos(int(cc))))+','+str(\"%.2f\" % (10+10*math.sin(int(cc))))+']' for cc in keep])) )\n",
    "      outfile.write('\\t\"SUBmaxNodeSize\":%d,\\n' % (max(size_subtop.values())))\n",
    "      outfile.write('\\t\"SUBmaxLinkWeight\":%.6f,\\n' % maxsubL)\n",
    "      outfile.write('\\t\"SUBWrange\":[-6,%.2f,%.2f],\\n' % (math.log(maxsubL)/math.log(10),-4+math.log(maxsubL)/math.log(10)/3) )\n",
    "      outfile.write('\\t\"SUBshowthr\":1.1,\\n' )\n",
    "      outfile.write('\\t\"SUBforce\":[0.5,-500,20],\\n' )\n",
    "      outfile.write('\\t\"SUBpositions\":[%s]\\n' % (','.join(['['+str(cc)+','+str(\"%.2f\" % (10+10*math.cos(int(cc))))+','+str(\"%.2f\" % (10+10*math.sin(int(cc))))+']' for cc in keepb])) )\n",
    "      outfile.write('}')\n",
    "\n",
    "\n",
    "    ############################################################\n",
    "    #####################   OUTPUT TEX FILES\n",
    "    ############################################################\n",
    "    myrefL=dict()\n",
    "    for com in keep: myrefL[com]=extract_labels(stuff['K'][com])[1]\n",
    "    for comtop in keep2: \n",
    "      for com in keep2[comtop]: myrefL[com]=extract_labels(Bstuff['K'][com])[1]\n",
    "\n",
    "    #\"\"\"\n",
    "    #.. output dir\n",
    "    print (\"..output stuff in tex files\")\n",
    "    out_dirT=os.path.join(out_dir,'texfiles')\n",
    "    if not os.path.exists(out_dirT): os.makedirs(out_dirT)\n",
    "\n",
    "    if database=='Scopus': rem=5\n",
    "    else: rem=0\n",
    "\n",
    "    # write a .tex for TOP clusters\n",
    "    if verbose: print (\"....TOP clusters\")\n",
    "    name = \"top_clusters%s.tex\" % (part_suffix)\n",
    "    f_out = open(os.path.join(out_dirT,name),\"w\", encoding='utf8')\n",
    "    #.. ini\n",
    "    f_out.write(\"%s\" % ini_tex(initexthr,len(G.nodes()), nb_art, top_size_sup_thr, 'top', topthr, top_n_sup_thr) )\n",
    "\n",
    "    #.. quant\n",
    "    f_out.write(\"%s\" % quant_tex(database,'\\\\footnotesize'))\n",
    "    f_out.write(\"All in BC & %d & %.2f & %.2f & %.3f & %.3f & %.3f & - & - & - & %.2f & %.2f & %.2f & %d\\\\\\\\\\n\\hline\\n\" % (NN, NRNR, avg_degree, avg_degree * 2.0 /(NN-1), 1000*avg_weight, QQ, PYPY, ARAR, TCTC, HH))\n",
    "    kompt=0\n",
    "    for com in keep:\n",
    "      kompt+=1\n",
    "      # go to new page if table too long\n",
    "      if (kompt%48==38): f_out.write(\"\\hline\\n\\end{tabular}}\\n\\end{center}\\n\\end{table}\\n\\clearpage\\n\\\\begin{table}\\n\\\\begin{center}\\n{\\\\footnotesize\\n\\\\begin{tabular}{|l |r r| r r r r r r r|r r r r|}\\n\\hline\\nCorpus & $N$ & $<N_{ref}>$ & $k$ & $d$ & $<\\omega_{in}>*10^3$ & $Q_i$ & $q$ & $h_{ref}$ &\\n $nr_{10}$/$nr_{5}$/$nr_{2}$ & $<PY>$ & $<A>_{refs}$ & $<N_{cit}>$ & $h$\\\\\\\\\\n\\hline\\n\")  \n",
    "      # write table entries\n",
    "      f_out.write(\"Cluster %d  & %d & %.2f & %.2f & %.3f & %.3f & %.3f & %.3f & %d & %d/%d/%d & %.2f & %.2f & %.2f & %d\\\\\\\\\\n\" % (com, size_top[com], comm_nr[com], comm_dg[com], 2*comm_dg[com]/(size_top[com]-1), 1000*comm_win[com], Qint[com], comm_q[com], quantR[com][0], quantR[com][1], quantR[com][2], quantR[com][3], comm_py[com], comm_ar[com], comm_tc[com], comm_h[com]))  \n",
    "    f_out.write(\"\\hline\\n\\end{tabular}}\\n\\\\end{center}\\n\\end{table}\")\n",
    "\n",
    "    #.. output stuff tables\n",
    "    for com in keep:\n",
    "      #\n",
    "      f_out.write(\"\\clearpage\\n\\n\\\\begin{table}[!ht]\\n\\caption*{{\\\\bf Cluster %d (``%s'').} This cluster contains $N = %d$ publications.} \\n\\\\textcolor{white}{aa}\\\\\\\\\\n{\\scriptsize\\\\begin{tabular}{|p{5cm} r r|}\\n\\hline\\n\" % (com, myrefL[com], size_top[com] ) )\n",
    "      #\n",
    "      # label comm_label[com][3]\n",
    "      #\n",
    "      BCUtils.my_write(f_out,'Keywords',stuff['K'],com,20)\n",
    "      BCUtils.my_write(f_out,'Title Words',stuff['TK'],com,10)\n",
    "      BCUtils.my_write(f_out,'Journal',stuff['J'],com,10)\n",
    "      f_out.write(\"\\end{tabular}\\n}\\n{\\scriptsize\\\\begin{tabular}{|p{5cm} r r|}\\n\\hline\\n\")\n",
    "      BCUtils.my_write(f_out,'Institution',stuff['I'],com,20)\n",
    "      BCUtils.my_write(f_out,'Country',stuff['C'],com,10)\n",
    "      BCUtils.my_write(f_out,'Author',stuff['A'],com,10)\n",
    "      f_out.write(\"\\end{tabular}\\n}\\n{\\scriptsize\\\\begin{tabular}{|p{8cm} r r|}\\n\\hline\\n\")\n",
    "      BCUtils.my_write(f_out,'Reference',stuff['R'],com,20-rem)\n",
    "      BCUtils.my_write(f_out,'RefJournal',stuff['RJ'],com,10)\n",
    "      BCUtils.my_write(f_out,'Subject',stuff['S'],com,10-rem)\n",
    "      f_out.write(\"\\end{tabular}\\n}\\n\\end{table}\\n\\n\")\n",
    "      #\n",
    "      # MOST CITED / REPR PAPERS \n",
    "      f_out.write(\"\\clearpage\\n\\n\\\\begin{table}[!ht]\\n\\caption*{{\\\\bf Cluster %d (``%s'').} Most cited publications (according to %s) and most representative publications (in term of in-degree $d_{in}$ measuring the number of publications in the cluster that are linked with it) among all publications in the cluster.}\\n\\\\textcolor{white}{aa}\\\\\\\\\\n\" % (com, myrefL[com], database))\n",
    "      #\n",
    "      # label comm_label[com][3]\n",
    "      #\n",
    "      nbline=14\n",
    "      #\n",
    "      f_out.write(\"{\\scriptsize\\\\begin{tabular}{|r r p{7cm} p{17cm}|}\\n\\\\hline\\n\") \n",
    "      f_out.write('\\multicolumn{4}{|c|}{{\\\\bf Most Cited publications}}\\\\\\\\\\n')\n",
    "      f_out.write(\"{\\\\bf $d_{in}$} & {\\\\bf Times Cited} & {\\\\bf Publication Ref} & {\\\\bf Publication Title} \\\\\\\\\\n\")\n",
    "      BCUtils.my_writeMCP(f_out,CR_papers[com]['MC'],nbline)\n",
    "      f_out.write(\"\\\\hline\\n\")\n",
    "      f_out.write('\\multicolumn{4}{|c|}{{\\\\bf Most Representative Publications}}\\\\\\\\\\n')\n",
    "      f_out.write(\"{\\\\bf $d_{in}$} & {\\\\bf Times Cited} & {\\\\bf Publication Ref} & {\\\\bf Publication Title} \\\\\\\\\\n\")\n",
    "      BCUtils.my_writeMCP(f_out,CR_papers[com]['MR'],nbline)\n",
    "      #f_out.write(\"\\\\hline\\n\")\n",
    "      f_out.write(\"\\end{tabular}\\n}\\n\\end{table}\\n\\n\") \n",
    "      #\n",
    " \n",
    "      # MOST CITED / REPR AUTHORS\n",
    "      f_out.write(\"\\clearpage\\n\\n\\\\begin{table}[!ht]\\n\\caption*{{\\\\bf Cluster %d (``%s'').} Most cited and representative authors. For each author, we display the number $N_a$ of publications their authored in that cluster, the sum $TC_a$ of their number of citations (according to %s), and the sum $k_a$ of their in-degree. }\\n\\\\textcolor{white}{aa}\\\\\\\\\\n\" % (com, myrefL[com], database))\n",
    "      #\n",
    "      # label comm_label[com][3]\n",
    "      #\n",
    "      nbline=20\n",
    "      #     \n",
    "      f_out.write(\"{\\scriptsize\\\\begin{tabular}{|l r r r|}\\n\\\\hline\\n\")\n",
    "      f_out.write('\\multicolumn{4}{|c|}{{\\\\bf Most Cited Authors}}\\\\\\\\\\n&&&\\\\\\\\\\n')\n",
    "      f_out.write(\"{\\\\bf Author} & {\\\\bf $N_a$} & {\\\\bf $TC_a$} & {\\\\bf $k_a$}\\\\\\\\\\n\")\n",
    "      BCUtils.my_writeMCA(f_out,CR_authors[com]['MC'],nbline)\n",
    "      f_out.write(\"\\n\\hline\\n\\hline\\n\")\n",
    "      f_out.write('\\multicolumn{4}{|c|}{{\\\\bf Most Representative Authors}}\\\\\\\\\\n&&&\\\\\\\\\\n')\n",
    "      f_out.write(\"{\\\\bf Author} & {\\\\bf $N_a$} & {\\\\bf $TC_a$} & {\\\\bf $k_a$}\\\\\\\\\\n\")\n",
    "      BCUtils.my_writeMCA(f_out,CR_authors[com]['MR'],nbline)\n",
    "      f_out.write(\"\\end{tabular}\\n}\\n\\end{table}\\n\\n\")   \n",
    "\n",
    "    #.. end\n",
    "    f_out.write(\"\\end{landscape}\\n\\n\\end{document}\\n\")\n",
    "    f_out.close()\n",
    "\n",
    "    ## ###############################\n",
    "    # write a .tex for SUBTOP clusters\n",
    "    if verbose: print (\"....SUBTOP clusters\")\n",
    "    name = \"subtop_clusters%s.tex\" % (part_suffix)\n",
    "    f_out = open(os.path.join(out_dirT,name),\"w\", encoding='utf8')\n",
    "    #.. ini\n",
    "    f_out.write(\"%s\" % ini_tex(initexthr,len(G.nodes()), nb_art, sub_size_sup_thr, 'subtop', subtopthr, sub_n_sup_thr) )\n",
    "\n",
    "    #.. quant\n",
    "    f_out.write(\"%s\" % quant_tex(database,'\\scriptsize'))\n",
    "    f_out.write(\"All in BC & %d & %.2f & %.2f & %.3f & %.3f & %.3f & - & - & - & %.2f & %.2f & %.2f & %d\\\\\\\\\\n\\hline\\n\" % (NN, NRNR, avg_degree, avg_degree * 2.0 /(NN-1), 1000*avg_weight, QQ, PYPY, ARAR, TCTC, HH))\n",
    "    kompt=0\n",
    "    for topcom in keep:\n",
    "      for com in keep2[topcom]:\n",
    "        kompt+=1\n",
    "        # go to new page if table too long\n",
    "        if (kompt%53==40): f_out.write(\"\\hline\\n\\end{tabular}}\\n\\end{center}\\n\\end{table}\\n\\clearpage\\n\\\\begin{table}\\n\\\\begin{center}\\n{\\scriptsize\\n\\\\begin{tabular}{|l |r r| r r r r r r r|r r r r|}\\n\\hline\\nCorpus & $N$ & $<N_{ref}>$ & $k$ & $d$ & $<\\omega_{in}>*10^3$ & $Q_i$ & $q$ & $h_{ref}$ &\\n $nr_{10}$/$nr_{5}$/$nr_{2}$ & $<PY>$ & $<A>_{refs}$ & $<N_{cit}>$ & $h$\\\\\\\\\\n\\hline\\n\")  \n",
    "        # write table entries\n",
    "        f_out.write(\"Cluster %d  & %d & %.2f & %.2f & %.3f & %.3f & - & %.3f & %d & %d/%d/%d & %.2f & %.2f & %.2f & %d\\\\\\\\\\n\" % (com, size_subtop[com], Bcomm_nr[com], Bcomm_dg[com], 2*Bcomm_dg[com]/(size_subtop[com]-1), 1000*Bcomm_win[com], Bcomm_q[com], BquantR[com][0], BquantR[com][1], BquantR[com][2], BquantR[com][3], Bcomm_py[com], Bcomm_ar[com], Bcomm_tc[com], Bcomm_h[com]))  \n",
    "    f_out.write(\"\\hline\\n\\end{tabular}}\\n\\\\end{center}\\n\\end{table}\")\n",
    "\n",
    "    #.. output tables\n",
    "    for comtop in keep:\n",
    "      for com in keep2[comtop]:\n",
    "        #\n",
    "        f_out.write(\"\\clearpage\\n\\n\\\\begin{table}[!ht]\\n\\caption*{{\\\\bf Cluster %d (``%s'').} This cluster contains $N = %d$ publications.} \\n\\\\textcolor{white}{aa}\\\\\\\\\\n{\\scriptsize\\\\begin{tabular}{|p{5cm} r r|}\\n\\hline\\n\" % (com,  myrefL[com], size_subtop[com] ) )\n",
    "        #\n",
    "        BCUtils.my_write(f_out,'Keywords',Bstuff['K'],com,20)\n",
    "        BCUtils.my_write(f_out,'Title Words',Bstuff['TK'],com,10)\n",
    "        BCUtils.my_write(f_out,'Journal',Bstuff['J'],com,10)\n",
    "        f_out.write(\"\\end{tabular}\\n}\\n{\\scriptsize\\\\begin{tabular}{|p{5cm} r r|}\\n\\hline\\n\")\n",
    "        BCUtils.my_write(f_out,'Institution',Bstuff['I'],com,20)\n",
    "        BCUtils.my_write(f_out,'Country',Bstuff['C'],com,10)\n",
    "        BCUtils.my_write(f_out,'Author',Bstuff['A'],com,10)\n",
    "        f_out.write(\"\\end{tabular}\\n}\\n{\\scriptsize\\\\begin{tabular}{|p{8cm} r r|}\\n\\hline\\n\")\n",
    "        BCUtils.my_write(f_out,'Reference',Bstuff['R'],com,20-rem)\n",
    "        BCUtils.my_write(f_out,'RefJournal',Bstuff['RJ'],com,10)\n",
    "        BCUtils.my_write(f_out,'Subject',Bstuff['S'],com,10-rem)\n",
    "        f_out.write(\"\\end{tabular}\\n}\\n\\end{table}\\n\\n\")\n",
    "        #\n",
    "        # MOST CITED PAPERS \n",
    "        f_out.write(\"\\clearpage\\n\\n\\\\begin{table}[!ht]\\n\\caption*{{\\\\bf Cluster %d (``%s'').} Most cited publications (according to %s) and most representative publications (in term of in-degree $d_{in}$ measuring the number of publications in the cluster that are linked with it) among all publications in the cluster.}\\n\\\\textcolor{white}{aa}\\\\\\\\\\n\" % (com, myrefL[com], database))\n",
    "        #\n",
    "        # label comm_label[com][3]\n",
    "        #\n",
    "        nbline=14\n",
    "        #\n",
    "        f_out.write(\"{\\scriptsize\\\\begin{tabular}{|r r p{7cm} p{17cm}|}\\n\\\\hline\\n\") \n",
    "        f_out.write('\\multicolumn{4}{|c|}{{\\\\bf Most Cited Publications}}\\\\\\\\\\n')\n",
    "        f_out.write(\"{\\\\bf $d_{in}$} & {\\\\bf Times Cited} & {\\\\bf Publication Ref} & {\\\\bf Publication Title} \\\\\\\\\\n\")\n",
    "        BCUtils.my_writeMCP(f_out,BCR_papers[com]['MC'],nbline)\n",
    "        f_out.write(\"\\\\hline\\n\")\n",
    "        f_out.write('\\multicolumn{4}{|c|}{{\\\\bf Most Representative Publications}}\\\\\\\\\\n')\n",
    "        f_out.write(\"{\\\\bf $d_{in}$} & {\\\\bf Times Cited} & {\\\\bf Publication Ref} & {\\\\bf Publication Title} \\\\\\\\\\n\")\n",
    "        BCUtils.my_writeMCP(f_out,BCR_papers[com]['MR'],nbline)\n",
    "        f_out.write(\"\\end{tabular}\\n}\\n\\end{table}\\n\\n\") \n",
    "        #\n",
    "        # MOST CITED / REPR AUTHORS\n",
    "        f_out.write(\"\\clearpage\\n\\n\\\\begin{table}[!ht]\\n\\caption*{{\\\\bf Cluster %d (``%s'').} Most cited and representative authors. For each author, we display the number $N_a$ of publications their authored in that cluster, the sum $TC_a$ of their number of citations (according to %s), and the sum $k_a$ of their in-degree. }\\n\\\\textcolor{white}{aa}\\\\\\\\\\n\" % (com, myrefL[com], database))\n",
    "        #\n",
    "        # label comm_label[com][3]\n",
    "        #\n",
    "        nbline=20\n",
    "        #     \n",
    "        f_out.write(\"{\\scriptsize\\\\begin{tabular}{|l r r r|}\\n\\\\hline\\n\")\n",
    "        f_out.write('\\multicolumn{4}{|c|}{{\\\\bf Most Cited Authors}}\\\\\\\\\\n&&&\\\\\\\\\\n')\n",
    "        f_out.write(\"{\\\\bf Author} & {\\\\bf $N_a$} & {\\\\bf $TC_a$} & {\\\\bf $k_a$}\\\\\\\\\\n\")\n",
    "        BCUtils.my_writeMCA(f_out,BCR_authors[com]['MC'],nbline)\n",
    "        f_out.write(\"\\n\\hline\\n\\hline\\n\")\n",
    "        f_out.write('\\multicolumn{4}{|c|}{{\\\\bf Most Representative Authors}}\\\\\\\\\\n&&&\\\\\\\\\\n')\n",
    "        f_out.write(\"{\\\\bf Author} & {\\\\bf $N_a$} & {\\\\bf $TC_a$} & {\\\\bf $k_a$}\\\\\\\\\\n\")\n",
    "        BCUtils.my_writeMCA(f_out,BCR_authors[com]['MR'],nbline)\n",
    "        f_out.write(\"\\end{tabular}\\n}\\n\\end{table}\\n\\n\")   \n",
    "\n",
    "    #.. end\n",
    "    f_out.write(\"\\end{landscape}\\n\\n\\end{document}\\n\")\n",
    "    f_out.close()\n",
    "\n",
    "  ########################################################################################################################\n",
    "  ########################################################################################################################\n",
    "  #####################   OUTPUT GEPHI/JSON FILES AT THE NODE LEVEL\n",
    "  ##... aux functions\n",
    "  def myref(spl):\n",
    "    if spl[4]=='0':\n",
    "      ref=\", \".join(spl[1:4])\n",
    "    else: \n",
    "      ref=\", \".join(spl[1:6])\n",
    "    return ref\n",
    "\n",
    "  def extract_pubstuff(filename, nn):\n",
    "    pubstuff={}\n",
    "    for n in nodes_to_keep: pubstuff[n]=[];\n",
    "    with open(os.path.join(in_dir, filename+\".dat\") ,\"r\", encoding='utf8') as file:\n",
    "      data_lines=file.read().split(\"\\n\")[:-1] \n",
    "    if filename==\"keywords\": \n",
    "      aux = [(int(l.split(\"\\t\")[0]), l.split(\"\\t\")[2]) for l in data_lines if (int(l.split(\"\\t\")[0]) in nodes_to_keep and l.split(\"\\t\")[1]=='IK')]\n",
    "    elif filename==\"references\":\n",
    "      aux = [(int(l.split(\"\\t\")[0]), myref(l.split(\"\\t\"))) for l in data_lines if (int(l.split(\"\\t\")[0]) in nodes_to_keep)]\n",
    "    else:\n",
    "      aux = [(int(l.split(\"\\t\")[0]), l.split(\"\\t\")[nn]) for l in data_lines if (int(l.split(\"\\t\")[0]) in nodes_to_keep)]\n",
    "    for elt in aux: pubstuff[elt[0]].append( elt[1] )\n",
    "\n",
    "    return pubstuff\n",
    "\n",
    "  ##... output the BC network?\n",
    "  num_levels=len(louvain_partition)-1\n",
    "  if ask:\n",
    "    confirm = input(\"..Prep and output json and gephi files describing the BC network at 'publications' level? (y/n): \")\n",
    "  else: confirm='n'\n",
    "  if confirm == 'y':\n",
    "    while confirm == 'y':\n",
    "      p=louvain_partition[num_levels-1]\n",
    "      confirm2 = input(\"....do you want to keep the whole network, ie %d nodes / %d edges (DON'T if network is too big)? (y/n): \" % (len(G.nodes()), len(G.edges()) ))\n",
    "      if confirm2 =='y':\n",
    "        nodes_to_keep=[n for n in p]\n",
    "        name = \"network_all%s\" % (part_suffix)\n",
    "      else:\n",
    "        commtokeep=int(input(\"....keep only top cluster with id:\"))\n",
    "        nodes_to_keep=[n for n in p if p[n]==commtokeep]\n",
    "        name = \"network_topcluster%d%s\" % (commtokeep, part_suffix)\n",
    "\n",
    "      ## export gephi & json\n",
    "      if verbose: print (\"....preping and exporting files\")\n",
    "      json_dir=os.path.join(in_dir,'jsonfiles')\n",
    "      if not os.path.exists(json_dir): os.makedirs(json_dir)\n",
    "      gdf_dir=os.path.join(in_dir,'gdffiles')\n",
    "      if not os.path.exists(gdf_dir): os.makedirs(gdf_dir)\n",
    "      with open(os.path.join(gdf_dir, name+'.gdf'),'w') as f_gephi, open(os.path.join(json_dir, name+'.json'),'w', encoding='utf8') as f_json:\n",
    "        ## ... prep nodes infos\n",
    "        # for gephi: only keep simple info (firstAU, journal, year) \n",
    "        # for json: keep title, journal, year and all subjects, authors, countries, keywords, references \n",
    "        pubstuff={}\n",
    "        pubstuff['K']=extract_pubstuff('keywords', 0)\n",
    "        pubstuff['R']=extract_pubstuff('references', 0)\n",
    "        pubstuff['S']=extract_pubstuff('subjects', 1)\n",
    "        pubstuff['AU']=extract_pubstuff('authors', 2)\n",
    "        pubstuff['C']=extract_pubstuff('countries', 2)\n",
    "        #\n",
    "        f_gephi.write(\"nodedef>name VARCHAR,label VARCHAR,top_com VARCHAR,subtop_com VARCHAR,colortop VARCHAR,firstAU VARCHAR,journal VARCHAR,year VARCHAR,nb_refs DOUBLE,times_cited DOUBLE\\n\")\n",
    "        f_json.write('{\\n\\t\"nodes\":[\\n')\n",
    "        pl = Utils.Article()\n",
    "        pl.read_file(src1)\n",
    "        kompt=0\n",
    "        ymin=3000;ymax=0;\n",
    "        for l in pl.articles:\n",
    "          if l.id in nodes_to_keep:\n",
    "            ymin=min(ymin, l.year)\n",
    "            ymax=max(ymax, l.year) \n",
    "            if kompt>0:f_json.write(',\\n')\n",
    "            kompt+=1\n",
    "            topcom = str(louvain_partition[num_levels-1][l.id])\n",
    "            subtopcom = str(louvain_partition[num_levels][l.id])\n",
    "            #bottomcom = str(louvain_partition[0][l.id])\n",
    "            customlabel = l.firstAU + ', ' + l.journal + ', ' + str(l.year) \n",
    "            f_gephi.write(\"%d,'%s',%s,%s,color%s,'%s','%s',%d,%d,%d\\n\" % (l.id, customlabel, topcom, subtopcom, topcom, l.firstAU, l.journal,l.year, nR[l.id], l.times_cited) ) \n",
    "            bigstuff=','.join(['\"'+x+'\":[%s]' % ( ', '.join(['\"'+st+'\"' for st in pubstuff[x][l.id] ]) ) for x in ['K', 'R', 'S', 'AU', 'C']])\n",
    "            f_json.write('\\t\\t{\"name\":\"%d\",\"id_top\":%s,\"id_sub\":%s,\"title\":\"%s\",\"firstAU\":\"%s\",\"journal\":\"%s\",\"year\":%d,\"doctype\":\"%s\",\"stuff\":{%s} }' % (l.id, topcom, subtopcom, l.title.replace('\"',\"'\"), l.firstAU, l.journal, l.year, l.doctype, bigstuff))\n",
    "\n",
    "        f_json.write('\\n\\t],\\n')\n",
    "\n",
    "        ## ... prep edges\n",
    "        f_gephi.write(\"edgedef>node1 VARCHAR,node2 VARCHAR,weight DOUBLE,nb_comm_refs DOUBLE\")\n",
    "        f_json.write('\\t\"links\":[\\n')\n",
    "        H = G.subgraph(nodes_to_keep).copy()\n",
    "        kompt=0\n",
    "        for e in H.edges(data=True):\n",
    "          f_gephi.write(\"\\n%d,%d,%f,%d\" % (e[0], e[1], e[2]['weight'], e[2]['nc']))\n",
    "          if kompt>0:f_json.write(',\\n')\n",
    "          kompt+=1;\n",
    "          f_json.write('\\t\\t{\"source\":\"%d\",\"target\":\"%d\",\"weight\":%f,\"nc\":%d}' % (e[0], e[1], e[2]['weight'], e[2]['nc']))\n",
    "        f_json.write('\\n\\t]\\n}')\n",
    "\n",
    "      ## default json file\n",
    "      deltaY=ymax-ymin+1\n",
    "      with open(os.path.join(json_dir, name+'_defaultVAR.json'), 'w') as fout:\n",
    "        fout.write('{\\n')\n",
    "        fout.write('\\t\"Wrange\":[0,1,0],\\n')\n",
    "        fout.write('\\t\"NRrange\":[1,50,1],\\n')\n",
    "        fout.write('\\t\"NCrange\":[1,50,1],\\n')\n",
    "        fout.write('\\t\"RTUrange\":[2,25,2],\\n')\n",
    "        fout.write('\\t\"DYrange\":[0,%d,%d],\\n' % (deltaY, deltaY))\n",
    "        fout.write('\\t\"forceparam\":[0.5,-150,5],\\n')\n",
    "        fout.write('\\t\"DefinedPos\":[[%s]]\\n' % ('], ['.join([str(x)+','+str(\"%.2f\" % (10+10*math.cos(int(x))))+','+str(\"%.2f\" % (10+10*math.sin(int(x))))+']' for x in nodes_to_keep])))\n",
    "        fout.write('}')\n",
    "\n",
    "      ## ... end\n",
    "      confirm = input(\"..Do you want to extract another set of json/gephi files describing the BC network at 'publications' level? (y/n): \") \n",
    "\n",
    "  ##\n",
    "  ##    \n",
    "  if verbose: print (\"..Done!\\n\")\n",
    "\n",
    "  ## ###################################\n",
    "  ## END\n",
    "  t2=time.time()\n",
    "  print ('.. Total time needed: %ds' % (t2-t1))\n",
    "  return\n",
    "\n",
    "## ################################################## ##################################################\n",
    "## ################################################## AUX FUNCTIONS\n",
    "## ################################################## ##################################################\n",
    "\n",
    "def extract_labels(stuff):\n",
    "  comm_label=dict()\n",
    "  if len(stuff)>0:\n",
    "    # label 0 is the most freq\n",
    "    comm_label[0] = stuff[0][0]\n",
    "    # label 1 is the most sign\n",
    "    f=0\n",
    "    for ff in range(len(stuff)):\n",
    "      if stuff[ff][2]>stuff[f][2]: \n",
    "        f=ff \n",
    "    comm_label[1] = stuff[f][0]\n",
    "    # label 2 has the max freq * sign\n",
    "    f=0\n",
    "    for ff in range(len(stuff)):\n",
    "      if stuff[ff][1]*stuff[ff][2]>stuff[f][1]*stuff[f][2]: \n",
    "        f=ff \n",
    "    comm_label[2] = stuff[f][0]\n",
    "    # label 3 is the most freq with sign > 1\n",
    "    fff=0; \n",
    "    while (fff < len(stuff)-1) and (stuff[fff][2] <=0):\n",
    "      fff+=1 \n",
    "    while (fff < len(stuff)-1) and (stuff[fff][2] <=1):\n",
    "      fff+=1   \n",
    "    comm_label[3] = stuff[fff][0]\n",
    "    #\n",
    "  else: \n",
    "    for i in range(4):\n",
    "      comm_label[i]=''  \n",
    "  return comm_label\n",
    "\n",
    "## ##################################################\n",
    "\n",
    "def mylinks(listA,listB,keepA,keepB,G,cond):\n",
    "  links=dict()\n",
    "  \n",
    "  for com1 in keepA:\n",
    "    for com2 in keepB:\n",
    "      #links[(com1, com2)]=[1,1]\n",
    "      #\"\"\"\n",
    "      if (cond==0 and com1 > com2) or (cond==1):\n",
    "        W = 0; N = 0;\n",
    "        for id1 in listA[com1]:\n",
    "          for id2 in listB[com2]:\n",
    "            if id2 in G.adj[id1]: \n",
    "              N += 1\n",
    "              W += G.adj[id1][id2]['weight']       \n",
    "        if W>0: \n",
    "          links[(com1, com2)]=[W*1.0/(len(listA[com1])*len(listB[com2])),N];        \n",
    "      #\"\"\"\n",
    "  \n",
    "  return links\n",
    "\n",
    "## ##################################################\n",
    "\n",
    "def cvst(stuff,com,x,ll): \n",
    "  ## most frequent items\n",
    "  if com in stuff[x]:\n",
    "    stuff=list(stuff[x][com].values());\n",
    "    if len(stuff)>0:\n",
    "      d=0\n",
    "      foo='[[\"%s\",%.2f,%.2f]' % (stuff[d][0].replace('\"','\\'').replace('\\&','&'),stuff[d][1],stuff[d][2])\n",
    "      for d in range(1,min(ll,len(stuff))):\n",
    "        foo+=',[\"%s\",%.2f,%.2f]' % (stuff[d][0].replace('\"','\\'').replace('\\&','&'),stuff[d][1],stuff[d][2])\n",
    "      foo +=']';\n",
    "    else: foo='[[\"\",0,0]]'\n",
    "  else: \n",
    "    foo='[[\"\",0,0]]'\n",
    "  return foo\n",
    "\n",
    "def cvau(stuff,ll): \n",
    "  ## most cited authors\n",
    "  stuff=list(stuff.values());\n",
    "  if len(stuff)>0:\n",
    "    d=0\n",
    "    foo='[[\"%s\",%d,%d]' % (stuff[d][0],stuff[d][1][0],stuff[d][1][1])\n",
    "    for d in range(1,min(ll,len(stuff))):\n",
    "      foo+=',[\"%s\",%d,%d]' % (stuff[d][0],stuff[d][1][0],stuff[d][1][1])\n",
    "    foo +=']';\n",
    "  else: foo='[[\"\",0,0]]'\n",
    "\n",
    "  return foo\n",
    "\n",
    "def cvar(stuff,ll):\n",
    "  ## most cited / representative papers\n",
    "  def myfunct(xx):\n",
    "    #.. return number of authors if more than 12\n",
    "    Nb=xx.count(' ');\n",
    "    if Nb > 12:\n",
    "      yy = '[%d different authors]' % Nb\n",
    "    else:\n",
    "      yy = xx\n",
    "    return yy\n",
    "  #\n",
    "  stuff=list(stuff.values());\n",
    "  if len(stuff) > 0:\n",
    "    d=0\n",
    "    #\"\"\"\n",
    "    foo='[[\"%s\",\"%s\",\"%s\",%d,%d,%d,\"%s\"]' % (stuff[d][2].replace('\\&','&').replace('\\\\','').replace('\"',''),myfunct(stuff[d][8]),stuff[d][3].replace('\\&','&'),stuff[d][1],stuff[d][6],stuff[d][7],stuff[d][5])\n",
    "    for d in range(1,min(ll,len(stuff))):\n",
    "      foo+=',[\"%s\",\"%s\",\"%s\",%d,%d,%d,\"%s\"]' % (stuff[d][2].replace('\\&','&').replace('\\\\','').replace('\"',''),myfunct(stuff[d][8]),stuff[d][3].replace('\\&','&'),stuff[d][1],stuff[d][6],stuff[d][7],stuff[d][5])\n",
    "    foo +=']';\n",
    "    foo.replace('\\\\','')\n",
    "  else: foo='[]'\n",
    "    \n",
    "  return foo\n",
    "\n",
    "## ##################################################\n",
    "\n",
    "def ini_tex(a,b,c,d,e,f,g):\n",
    "\n",
    "  mystr= \"\\\\documentclass[a4paper,11pt]{report}\\n\\\\usepackage[english]{babel}\\n\\\\usepackage[latin1]{inputenc}\\n\\\\usepackage{amsfonts,amssymb,amsmath}\\n\\\\usepackage{pdflscape}\\n\\\\usepackage{color,colortbl}\\n\\\\usepackage{graphicx}\\n\\\\usepackage{caption}\\n\\n\\\\usepackage{array}\\n\\def\\ignore#1\\endignore{}\\n\\\\newcolumntype{h}{@{}>{\\ignore}l<{\\endignore}} %%%%  blind out some columns\\n\\n\\\\addtolength{\\evensidemargin}{-60pt}\\n\\\\addtolength{\\oddsidemargin}{-60pt}\\n\\\\addtolength{\\\\textheight}{80pt}\\n\\n\\\\title{{\\\\bf Clusters ID Cards}}\\n\\date{\\\\begin{flushleft}\"\n",
    "\n",
    "  mystr+=\"This document gathers the ``ID Cards'' of the BC clusters found within the studied database.\\\\\\\\\\n %s - %d out of %d publications are in the network. The %d clusters presented here correspond to the ones found in the %s level grouping at least %d publications. They gather a total of %d publications. \\\\\\\\\\n These ID cards displays the most frequent keywords, subject categories, journals of publication, institutions, countries, authors, references and reference journals of the publications of each cluster. The significance of an item $\\sigma = \\sqrt{N} (f - p) / \\sqrt{p(1-p)}$ - where $N$ is the number of publications within the cluster and $f$ and $p$ are the proportion of publications respectively within the cluster and within the database displaying that item - is also given.\\\\\\\\\\n\\\\vspace{1cm}\\n\\copyright Sebastian Grauwin - BIBLIOTOOLS/BiblioTools3.2 (October 2017) \\end{flushleft}}\\n\\n\\\\begin{document}\\n\\\\begin{landscape}\\n\\maketitle\\n\" % (a,b,c,g,e,f,d)\n",
    "\n",
    "  mystr+=\"\\n\\n\\clearpage\\n\\n\\\\begin{figure}[h!]\\n\\\\begin{center}\\n%%\\includegraphics[width=1.3\\\\textwidth]{xxx}\\n\\caption{{\\\\bf %s clusters network.} The sizes of the nodes correspond to the number of publications in each cluster, their color to the top cluster they belong to. The edges reflect shared references between clusters: the thicker an edge between two clusters, the more references they share. Labels correspond to the most significant used keyword. }\\n\\end{center}\\n\\end{figure}\\n\\n\" % (e.capitalize());\n",
    "\n",
    "  return mystr\n",
    " \n",
    "def quant_tex(database,mysize): \n",
    "  mystr=\"\\n\\n\\clearpage\\n\\n\\\\begin{table}[ht]\\n\\caption*{{\\\\bf Quantitative characteristics of the clusters.} $N$ is the number of publications within the cluster, $<N_{ref}>$ the average number of references per publication. The cohesiveness of the cluster can be measured by: the average degree $k$ of its publications (i.e. average number of links within the cluster per publication), its density in terms of BC links $d=2k/(N-1)$, the weighted density $<\\omega_{in}>$, the inner modularity $Q_i$ obtained when splitting the cluster in a sub-partition, and the module $q$ of the cluster within the partition. To quantify how a cluster can concentrate on a given number of references,\"\n",
    "  mystr+=\" we also display the h-index $h_{ref}$ ($h$ references are cited by at least $h$ publications of the cluster) and the numbers $nr_{10}$, $nr_{5}$, $nr_{2}$, where $nr_{x}$ is the number of references cited by at least $x\\%$ of publications within the cluster. \" \n",
    "  mystr+=\"To estimate the `hotness' of a cluster, we display the average publication year of the publications within the cluster $<PY>$, the average age of the references used in the cluster $<A>_{refs}$, the average number of citation per publication (according to %s) $<N_{cit}>$, the h index $h$ of the cluster ($h$ publications have been cited at least $h$ times).}\\n\\\\begin{center}\\n\" % database\n",
    "  \n",
    "  mystr+=\"{\\n%s\\\\begin{tabular}{|l |r r| r r r r r r r|r r r r|}\\n\\hline\\nCorpus & $N$ & $<N_{ref}>$ & $k$ & $d$ & $<\\omega_{in}>*10^3$ & $Q_i$ & $q$ & $h_{ref}$ & $nr_{10}$/$nr_{5}$/$nr_{2}$ & $<PY>$ & $<A>_{refs}$ & $<N_{cit}>$ & $h$ \\\\\\\\\\n\\hline\\n\" % mysize\n",
    "  \n",
    "  return mystr\n",
    " \n",
    "      \n",
    "## ##################################################\n",
    "## ##################################################\n",
    "##########################################################################################################\n",
    "# BC clustering with louvain algo: either use the python version or the faster c++ version if possible\n",
    "algo_method='python';   # either 'python' or 'c++'\n",
    "Nruns=1;             # number of time the louvain algorithm is run for a given network, the best partition being kept (default 1)\n",
    "SIZECUT=50;          # compute partitions of clusters of min(1% of corpus_size, SIZECUT) into sub-clusters (default 50)\n",
    "##########################################################################################################\n",
    "# all thresholds used to define the BC network\n",
    "bcthr=1;     # minimum number of shared references to keep a link (default 1)\n",
    "RTUthr=2;    # minimum time of use in the corpus to count a reference in the shared references (default 2)\n",
    "DYthr=10000; # maximum difference in publication year to keep a link (default 10000)\n",
    "Wthr=0;      # minimum weight to keep a link (default 0)\n",
    "NRthr=1;     # minimum number of references to keep a nodes (default 1)\n",
    "# items to keep in the output json files / how many to keep (the x most frequent items in each cluster) \n",
    "stuff_tokeep=['Y', 'K', 'TK', 'S', 'J', 'R', 'RJ', 'C', 'I', 'A'] # \n",
    "howmuch_tokeep={'Y':100, 'K':20, 'TK':10, 'A':10, 'S':10, 'S2':10, 'J':10, 'R':10, 'RJ':10, 'C':10, 'I':10}\n",
    "##########################################################################################################\n",
    "\n",
    "part_suffixBC=''\n",
    "initexthr='The BC network was built by linking pairs of publications based on the references they share'\n",
    "if (bcthr!=1): part_suffixBC+=('_bcthr%d' % bcthr)\n",
    "if (RTUthr!=2): part_suffixBC+=('_RTUthr%d' % RTUthr)\n",
    "if (DYthr!=10000): part_suffixBC+=('_DYthr%d' % DYthr)\n",
    "if (Wthr!=0): part_suffixBC+=('_Wthr%.3f' % Wthr)\n",
    "#\n",
    "auxs='';\n",
    "if bcthr>1: auxs='s'\n",
    "if part_suffixBC !='': initexthr+='. We only kept links between publications sharing more than %d reference%s' % (bcthr,auxs)\n",
    "if (RTUthr>2): initexthr+=(', counting the number of shared references among those cited more than %d times in the studied corpus' % RTUthr)\n",
    "if (DYthr<10000): initexthr+=(', published less than %d years apart' % DYthr)\n",
    "if (Wthr>0): initexthr+=(' and if their Kessler similarity was greater than %.3f' % Wthr)\n",
    "#\n",
    "if (NRthr>1): \n",
    "  part_suffixBC+=('_NRthr%d' %NRcthr)\n",
    "  initexthr+=('. Nodes with less than %d references were also excluded' %NRcthr)\n",
    "\n",
    "BC_network(in_dir,out_dir,ini_suffix,verbose,presaved,ask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliographic Coupling analysis main\n",
    "Do not run the following cell. It is kept for Terminal launch of work"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def main():\n",
    "# usage: BC.py [-h] [--version] -i DIR -o DIR[-v]\n",
    "#\n",
    "# optional arguments:\n",
    "#   -h, --help            show this help message and exit\n",
    "#   --version             show program's version number and exit\n",
    "#   -i DIR, --input_dir DIR input directory name\n",
    "#   -g \n",
    "#   -o DIR, --output_dir DIR input directory name\n",
    "  # Parse line options.\n",
    "  # Try to have always the same input options\n",
    "  parser = argparse.ArgumentParser(description = 'parser')\n",
    "\n",
    "  parser.add_argument('--version', action='version', version='%(prog)s 1.1')\n",
    "  \n",
    "  parser.add_argument(\"-i\", \"--input_dir\", nargs=1, required=True,\n",
    "          action = \"store\", dest=\"in_dir\",\n",
    "          help=\"input directory name\",\n",
    "          metavar='DIR')\n",
    "\n",
    "  parser.add_argument(\"-o\", \"--output_dir\", nargs=1, required=False,\n",
    "          action = \"store\", dest=\"out_dir\", \n",
    "          default = 'Desktop/',\n",
    "          help=\"output directory name\",\n",
    "          metavar='DIR')\n",
    "\n",
    "  parser.add_argument(\"-p\", \"--presaved\",\n",
    "          action = \"store_true\", dest=\"presaved\",\n",
    "          default = False,\n",
    "          help=\"presaved mode [default %(default)s]\")\n",
    "\n",
    "  parser.add_argument(\"-a\", \"--ask\",\n",
    "          action = \"store_false\", dest=\"ask\",\n",
    "          default = True,\n",
    "          help=\"ask questions? [default %(default)s]\")\n",
    "          \n",
    "  parser.add_argument(\"-v\", \"--verbose\",\n",
    "          action = \"store_true\", dest=\"verbose\",\n",
    "          default = False,\n",
    "          help=\"verbose mode [default %(default)s]\")\n",
    "\n",
    "  #Analysis of input parameters\n",
    "  args = parser.parse_args()\n",
    "  \n",
    "  if not os.path.exists(args.in_dir[0]):\n",
    "      print (\"Error: Input directory does not exist: \", args.in_dir[0])\n",
    "      exit()\n",
    "\n",
    "  if args.out_dir == 'Desktop/':\n",
    "    args.out_dir = args.in_dir\n",
    "\n",
    "  if not os.path.exists(args.out_dir[0]):\n",
    "      print (\"Error: Output directory does not exist: \", args.out_dir[0] )\n",
    "      exit()\n",
    "\n",
    "  ##\n",
    "  BC_network(args.in_dir[0],args.out_dir[0],'',args.verbose,args.presaved,args.ask)  \n",
    "\n",
    "\n",
    "  return\n",
    "\n",
    "\n",
    "    \n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurrence Maps\n",
    "he command line\n",
    "\n",
    "- python BiblioTools3.2/cooc_graphs.py -i myprojectname/ -v -hg <br>\n",
    "\n",
    "will create multiple co-occurence networks, all stored in gdf files that can be opened in Gephi, among which:\n",
    "\n",
    "\n",
    "Example of heterogeneous network generated with BiblioTools and visualized in Gephi.\n",
    "\n",
    "- a co-cocitation network, linking references that are cited in the same publications.\n",
    "- a co-refsources network, linking references's sources that are cited in the same publications.\n",
    "- a co-author network, linking authors that collaborated in some publications.\n",
    "- a co-country network, linking countries with researchers that collaborated in some publications.\n",
    "- a co-institution network, linking institutions with researchers that collaborated in some publications. For this network to be fully useful, you may want to spend some time cleaning the \"institutions.dat\", e.g. by keeping only the big institutions (university level) or by replacing minor name variant by the dominant name variant (\"Ecole Normale Supérieure de Lyon\" → \"ENS Lyon\")\n",
    "- a co-keyword network, linking keywords being co-used in some publications. Be careful about the interpretation: keywords can be polysemic, their meaning differing from field to another (eg \"model\", \"energy\", \"evolution\", etc).\n",
    "- an heterogeneous co-occurrence network, gathering all the items (authors, keywords, journals, subjects, references, institutions, etc), cf example on the side figure. This network will be generated only if the option \"-hg\" is used in the command line above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "\"\"\" \n",
    "   Author : Sebastian Grauwin (http://www.sebastian-grauwin.com/)\n",
    "   Copyright (C) 2017\n",
    "\"\"\"\n",
    "\n",
    "# usage: cooc_graph.py -i DIR [-v]\n",
    "# \n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "def cooc_graph(in_dir, ini_suff, timeWND, prephet, verbose):\n",
    "\n",
    "  THR=COOC_THR\n",
    "  # for heteregeneous graph\n",
    "  global HET_SUFF, HET_TABLE;\n",
    "  HET_TABLE={};\n",
    "  HET_SUFF='';\n",
    "  # default color for gephi display\n",
    "  color_nodes = {'Y': '255,255,0', 'J': '150,0,150', 'AU': '20,50,255', 'K': '255,0,255', 'AK': '255,0,255', 'TK': '205,0,205', 'S': '50,0,150', 'S2': '50,0,150', 'R': '255,0,0', 'RJ': '255,97,0', 'I': '0,255,0', 'CU': '0,255,255', 'LA': '0,180,0', 'DT': '0,180,0'}\n",
    "\n",
    "\n",
    "  ## INITIALIZATION\n",
    "  t1=time.time()\n",
    "  if verbose: print (\"INITIALIZATION\")\n",
    "  with open(os.path.join(in_dir, \"articles.dat\") ,\"r\", encoding='utf-8') as file:\n",
    "    # dat file have one trailing blank line at end of file\n",
    "    data_lines=file.read().split(\"\\n\")[:-1]  \n",
    "  # filter if necessary\n",
    "  if timeWND!=[]: \n",
    "    data_lines=[l for l in data_lines if (int(l.split(\"\\t\")[2])>=timeWND[0] and int(l.split(\"\\t\")[2])<=timeWND[1] )]\n",
    "    tokeep=dict()\n",
    "    for l in data_lines: tokeep[l.split(\"\\t\")[0]]=''\n",
    "    if len(tokeep)==0: return\n",
    "  # total nb_art  \n",
    "  nb_art=len(data_lines)\n",
    "  print (\"... %d publis\" % nb_art)\n",
    "\n",
    "  # prep out folder\n",
    "  outname='gdffiles'\n",
    "  out_dir=os.path.join(in_dir,outname)\n",
    "  if not os.path.exists(out_dir): os.makedirs(out_dir)\n",
    "\n",
    "\n",
    "  ## ##################################################\n",
    "  ## AUX FUNCTIONS\n",
    "  def generate_cooc(stuff,nick):\n",
    "    global HET_TABLE, HET_SUFF\n",
    "    # HOW MUCH TIME AN ITEM APPEARS?\n",
    "    stuff.sort(key=lambda e:e[1])\n",
    "    # duplicates are removed: we count in how many distinct publis an item appears \n",
    "    foo=[(xx,len(set(list(art_id)))) for xx,art_id in itertools.groupby(stuff,key=lambda e:e[1])]\n",
    "    foo.sort(key=lambda x:-x[1])\n",
    "    # keep only the items appearing more than THR\n",
    "    keep=[foo[i] for i in range(len(foo)) if foo[i][1]>=THR[nick] ]\n",
    "\n",
    "    if (len(keep)>0):\n",
    "      if prephet: HET_SUFF+='_'+nick+str(THR[nick])\n",
    "      outfilename=os.path.join(out_dir,'cooc_'+nick+ini_suff)\n",
    "      if timeWND != []: outfilename+='_'+str(timeWND[0])+'_'+str(timeWND[1])\n",
    "      outfilename+='_thr'+str(THR[nick])+'.gdf'\n",
    "      with open(outfilename, 'w') as f_gephi:\n",
    "        nodeID={};aux={};comm={};\n",
    "        # nodes\n",
    "        f_gephi.write(\"nodedef>name VARCHAR,label VARCHAR,type VARCHAR,width DOUBLE,height DOUBLE,size DOUBLE,color VARCHAR\\n\")\n",
    "        for i in range(len(keep)):\n",
    "          nodeID[keep[i][0]]=i\n",
    "          size=keep[i][1]\n",
    "          f_gephi.write(\"%d,'%s',%s,%f,%f,%d,'%s'\\n\" % (i, keep[i][0].replace('&','-'), nick, math.sqrt(size),math.sqrt(size), size, color_nodes[nick]))\n",
    "        # links\n",
    "        f_gephi.write(\"edgedef>node1 VARCHAR,node2 VARCHAR,weight DOUBLE,nb_cooc DOUBLE\\n\")\n",
    "        for elt in stuff:\n",
    "          if elt[1] in nodeID: \n",
    "            if elt[0] not in aux: aux[elt[0]]=[]\n",
    "            aux[elt[0]].append(nodeID[elt[1]])\n",
    "            if prephet: \n",
    "              if (nick,elt[1]) not in HET_TABLE: HET_TABLE[(nick, elt[1])]=[]\n",
    "              HET_TABLE[(nick, elt[1])].append( elt[0] ) \n",
    "        for ee in aux:\n",
    "          for itema in aux[ee]:\n",
    "            for itemb in [ff for ff in aux[ee] if ff > itema]:\n",
    "               if (itema, itemb) not in comm: comm[(itema, itemb)]=0\n",
    "               comm[itema,itemb]+=1\n",
    "        for (a,b) in comm:\n",
    "          f_gephi.write(\"%d,%d,%.6f,%d\\n\" % (a, b, comm[(a,b)]/math.sqrt(keep[a][1]*keep[b][1]), comm[(a,b)]))\n",
    "      # no cooccurence between years or publication sources: remove these files  \n",
    "      if nick in ['J','Y']: os.remove(outfilename)\n",
    "\n",
    "      # free memory\n",
    "      del aux, stuff, keep, nodeID, comm\n",
    "\n",
    "    return\n",
    "\n",
    "  ## ################\n",
    "  def output_hetgraph():\n",
    "    global HET_TABLE, HET_SUFF\n",
    "    print(\"... Prep het graph\")\n",
    "    outfilename=os.path.join(out_dir,'cooc_heterogeneous'+ini_suff)\n",
    "    if timeWND != []: outfilename+='_'+str(timeWND[0])+'_'+str(timeWND[1])\n",
    "    outfilename+=HET_SUFF+'.gdf'\n",
    "    with open(outfilename, 'w') as f_gephi:\n",
    "      nodeID={};aux={};comm={};size={};\n",
    "      # nodes\n",
    "      print(\".... nodes\")\n",
    "      f_gephi.write(\"nodedef>name VARCHAR,label VARCHAR,type VARCHAR,width DOUBLE,height DOUBLE,size DOUBLE,color VARCHAR\\n\")  \n",
    "      i=0\n",
    "      for (t,x) in HET_TABLE:\n",
    "        nodeID[(t,x)]=i\n",
    "        size[i]=len(HET_TABLE[(t,x)])\n",
    "        f_gephi.write(\"%d,'%s',%s,%f,%f,%d,'%s'\\n\" % (i, x.replace('&','-'), t, math.sqrt(size[i]),math.sqrt(size[i]), size[i], color_nodes[t]))\n",
    "        i+=1\n",
    "      # links\n",
    "      e = len(HET_TABLE) * len(HET_TABLE) / 2; ee = 0; p=5; \n",
    "      print(\".... links\")\n",
    "      f_gephi.write(\"edgedef>node1 VARCHAR,node2 VARCHAR,weight DOUBLE,nb_cooc DOUBLE\\n\")\n",
    "      for elt in HET_TABLE:\n",
    "        for pub in HET_TABLE[elt]:\n",
    "          if pub not in aux: aux[pub]=[]\n",
    "          aux[pub].append(nodeID[elt])\n",
    "      for pub in aux:\n",
    "        for itema in aux[pub]:\n",
    "          for itemb in [ff for ff in aux[pub] if ff > itema]:\n",
    "             if (itema, itemb) not in comm: comm[(itema, itemb)]=0\n",
    "             comm[itema,itemb]+=1\n",
    "      for (a,b) in comm:\n",
    "        f_gephi.write(\"%d,%d,%.6f,%d\\n\" % (a, b, comm[(a,b)]/math.sqrt(size[a]*size[b]), comm[(a,b)]))     \n",
    "\n",
    "    # free memory\n",
    "    del aux, comm, nodeID, HET_TABLE\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "  ## ################\n",
    "\n",
    "  def treat_item(item, nick, which):\n",
    "    if verbose: print (\"... dealing with %s\" % (item) )\n",
    "    with open(os.path.join(in_dir, item+\".dat\") ,\"r\",encoding='utf-8') as file:\n",
    "      # dat file have one trailing blank line at end of file\n",
    "      data_lines=file.read().split(\"\\n\")[:-1] \n",
    "    if (timeWND == []): aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[which]) for l in data_lines]\n",
    "    else: aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[which]) for l in data_lines if l.split(\"\\t\")[0] in tokeep]\n",
    "    generate_cooc(aux,nick)  \n",
    "    # free memory\n",
    "    del data_lines \n",
    "    # end\n",
    "    return\n",
    "\n",
    "  ## ##################################################\n",
    "  ## TREAT DATA \n",
    "  if prephet:    \n",
    "    if verbose: print (\"... dealing with years + journals + doctypes + languages\" )\n",
    "    # data_lines was already filtered to keep only publi in the correct time-window\n",
    "    #.. Y\n",
    "    aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[2]) for l in data_lines]\n",
    "    generate_cooc(aux,'Y')\n",
    "    #.. J\n",
    "    aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[3]) for l in data_lines]\n",
    "    generate_cooc(aux,'J')\n",
    "\n",
    "  #.. D\n",
    "  aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[8]) for l in data_lines]\n",
    "  generate_cooc(aux,'DT') \n",
    "  #.. L\n",
    "  aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[9]) for l in data_lines]\n",
    "  generate_cooc(aux,'LA')    \n",
    "  #.. free memory\n",
    "  del data_lines\n",
    "  \n",
    "  treat_item('authors', 'AU', 2)\n",
    "  treat_item('subjects', 'S', 1)\n",
    "  treat_item('institutions', 'I', 2)\n",
    "  treat_item('countries', 'CU', 2)\n",
    "  with open(os.path.join(in_dir, \"database.dat\"),'r') as ff:\n",
    "    if ff.read()=='Scopus': treat_item('subjects2', 'S2', 1)\n",
    "\n",
    "  if verbose: print (\"... dealing with keywords + title words\") \n",
    "  with open(os.path.join(in_dir, \"keywords.dat\") ,\"r\",encoding='utf-8') as file:\n",
    "    # dat file have one trailing blank line at end of file\n",
    "    data_lines=file.read().split(\"\\n\")[:-1]\n",
    "  if timeWND!=[]: data_lines=[l for l in data_lines if (l.split(\"\\t\")[0] in tokeep) ]\n",
    "  #.. IK\n",
    "  aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[2]) for l in data_lines if l.split(\"\\t\")[1]=='IK' ]\n",
    "  generate_cooc(aux,'K')\n",
    "  #.. AK\n",
    "  aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[2]) for l in data_lines if l.split(\"\\t\")[1]=='AK' ]\n",
    "  generate_cooc(aux,'AK')  \n",
    "  #.. TK\n",
    "  aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[2]) for l in data_lines if l.split(\"\\t\")[1]=='TK' ]\n",
    "  generate_cooc(aux,'TK')  \n",
    "  #.. free memory\n",
    "  del data_lines \n",
    "\n",
    "\n",
    "  if verbose: print (\"... dealing with references + references journals\") \n",
    "  with open(os.path.join(in_dir, \"references.dat\") ,\"r\",encoding='utf-8', errors='ignore') as file:\n",
    "    # dat file have one trailing blank line at end of file\n",
    "    data_lines=file.read().split(\"\\n\")[:-1]\n",
    "  if timeWND!=[]: data_lines=[l for l in data_lines if (l.split(\"\\t\")[0] in tokeep) ] \n",
    "  #.. R\n",
    "  aux = [(l.split(\"\\t\")[0],\", \".join(l.split(\"\\t\")[1:5]).replace(',0,0','').replace(', 0, 0','').replace(', 0','')) for l in data_lines]\n",
    "  generate_cooc(aux,'R')  \n",
    "  #.. RJ\n",
    "  aux = [(l.split(\"\\t\")[0], l.split(\"\\t\")[3]) for l in data_lines]\n",
    "  generate_cooc(aux,'RJ')  \n",
    "  #.. free memory\n",
    "  del data_lines\n",
    "\n",
    "  ## ##################################################\n",
    "  ## OUTPUT HETEROGENEOUS GRAPH IF ASKED\n",
    "  if prephet: output_hetgraph()\n",
    "\n",
    "\n",
    "  ## ##################################################\n",
    "  ## END    \n",
    "  print ('.. Time needed: %ds' % (time.time()-t1))\n",
    "  return\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "## ##########################################################################\n",
    "## SOME VARIABLES\n",
    "# default threshold value: \"K\":50 means that the keyword co-occurrence network will features keywords appearing in at least 50 publications of the corpus\n",
    "# if you don't want a certain type of item, put a riducously high threshold, eg 1000000\n",
    "COOC_THR={\"AU\":50,\"CU\":1,\"I\":50,\"S\":1,\"S2\":1,\"K\":50,\"AK\":50,\"TK\":50,\"R\":30,\"RJ\":200,\"Y\":1,\"J\":20,\"DT\":1,\"LA\":1}\n",
    "## ##########################################################################\n",
    "\n",
    "cooc_graph(in_dir, ini_suff, timeWND, prephet, verbose)\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurrence Maps main\n",
    "Do not run the following cell. It is kept for Terminal launch of work"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def main():\n",
    "# usage: cooc_graph.py [-h] [--version] -i DIR [-v] [-H INT] [-O]\n",
    "#\n",
    "# optional arguments:\n",
    "#   -h, --help            show this help message and exit\n",
    "#   --version             show program's version number and exit\n",
    "#   -i DIR, --input_dir DIR input directory name \n",
    "  # Parse line options.\n",
    "  # Try to have always the same input options\n",
    "  parser = argparse.ArgumentParser(description = 'parser')\n",
    "\n",
    "  parser.add_argument('--version', action='version', version='%(prog)s 1.1')\n",
    "  \n",
    "  parser.add_argument(\"-i\", \"--input_dir\", nargs=1, required=True,\n",
    "          action = \"store\", dest=\"in_dir\",\n",
    "          help=\"input directory name\",\n",
    "          metavar='DIR')\n",
    "\n",
    "  parser.add_argument(\"-hg\", \"--prephet\",\n",
    "          action = \"store_true\", dest=\"prephet\",\n",
    "          default = False,\n",
    "          help=\"prepare heterogeneous graph? [default %(default)s]\")\n",
    "          \n",
    "  parser.add_argument(\"-v\", \"--verbose\",\n",
    "          action = \"store_true\", dest=\"verbose\",\n",
    "          default = False,\n",
    "          help=\"verbose mode [default %(default)s]\")\n",
    "\n",
    "  #Analysis of input parameters\n",
    "  args = parser.parse_args()\n",
    "  if (not os.path.exists(args.in_dir[0])):\n",
    "      print (\"Error, Input directory does not exist: \", args.in_dir[0])\n",
    "      exit()\n",
    "\n",
    "  cooc_graph(args.in_dir[0], '', [], args.prephet, args.verbose)\n",
    "\n",
    "\n",
    "  return\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All analyses at once - and on multiple time periods\n",
    "If you want to perform all the analyses at once, you may use the command line:\n",
    "\n",
    "- python BiblioTools3.2/all_in_one.py -i myprojectname/rawdata -o myprojectname/\n",
    "\n",
    "Instructions on how to edit the parameters are provided in the \"all_in_once.py\" script.\n",
    "\n",
    "You may edit the parameter \"whichanalysis\" in the script to select the analysis you want to perform. By default, the code will:\n",
    "\n",
    "- parse the WOS files in the rawdata folder (add the option \"-d scopus\" to parse Scopus data)\n",
    "- perform a frequency analysis and prepare data files for the \"corpus description\" BiblioMaps interface.\n",
    "- perform a BC analysis and prepare data files for the \"BC maps\" BiblioMaps interface or Gephi, + companion documents to compile in laTex. BEWARE: contrary to the biblio_coupling codes that asks you to confirm the values of some parameters along the way, the \"all_in_one\" script does not ask for any confirmation and always use default values.\n",
    "- prepare co-occurrence graphs to be opened in Gephi.\n",
    "Moreover, the \"all_in_one\" scripts allows you to define temporal slices on which to performs all the studies. You may for example want to perform a BC analysis on multiple 5 years periods? Just edit the \"timeWND\" parameter in the script before launching it on the command line: all analyses will be performed directly. To visualize the generated data files in BiblioMaps, you should follow the instructions written on the generated \"AA_logs.txt\" file.\n",
    "\n",
    "Right now, the BC analysis performed on the different temporal slices are independant, although a simple analysis is performed to match clusters from one time period to clusters from the precedent / next one. More complex scripts allowing to analyse / visualize the historical evolution of BC clusters are in preparation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\" \n",
    "   Author : Sebastian Grauwin (http://www.sebastian-grauwin.com/)\n",
    "   Copyright (C) 2017\n",
    "\"\"\"\n",
    "\n",
    "# usage: allinone.py -i DIR [-o DIR] [-d scopus] [-e]\n",
    "# -i dir indicates the folder where your wos / scopus data is\n",
    "# -o dir indicates the main folder where all the files will be created\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy\n",
    "import math\n",
    "import argparse\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import json\n",
    "import Utils.Utils as Utils\n",
    "import Utils.BCUtils as BCUtils\n",
    "import Utils.community as community\n",
    "import networkx as nx\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "\n",
    "from biblio_parser import biblio_parser\n",
    "from describe_corpus import describe_corpus\n",
    "from biblio_coupling import *\n",
    "from cooc_graphs import cooc_graph\n",
    "\n",
    "## ON WHICH TIME PERIOD DO YOU WISH TO MAKE ANALYSIS? \n",
    "# Leave timeWND=[] for all years in corpus, create an array of ymin-ymax otherwise, eg \n",
    "# if you want successive windows of 5 years between 1990 and 2019 (ie 90-94, 95-99, 00-04, etc), write sth like\n",
    "# timeWND=[[yy,yy+4] for yy in range(1990,2019,5)]\n",
    "# if you want windows of 10 years between 1990 and 2019 with a 5 years overlap (ie 90-99, 95-04, 00-09, etc), write sth like\n",
    "# timeWND=[[yy,yy+9] for yy in range(1990,2015,5)]\n",
    "# in general, if you want windows of N years between YMIN and YMAX with a X years overlap, edit the following:\n",
    "# timeWND=[[yy,yy+N-1] for yy in range(YMIN,YMAX,N-X)]\n",
    "timeWND=[]   \n",
    "  \n",
    "\n",
    "## WHICH ANALYSIS SHOULD WE PERFORM? (remove those you don't want from the list) \n",
    "# 'PARSE' will do the first parsing of the data (you NEED to do it at once)\n",
    "# 'CD' stands for corpus description on each time period\n",
    "# 'BC' stands for bibliographic coupling on each time period\n",
    "# 'CO' stands for co-occurrence graph on each time period \n",
    "# 'MATCH' will perform a SIMPLE matching analysis between the BC clusters of different time periods. You may change the parameter THETA: we keep only links between clusters of diff periods whose Kessler's cosines are above this threshold. Default: 0.05\n",
    "#\n",
    "whichanalysis=[\"PARSE\", \"CD\", \"BC\", \"CO\", \"MATCH\"]\n",
    "THETA=0.05\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "def allinone(dataraw_dir, main_dir, database, expert):\n",
    "  ## INITIALIZATION\n",
    "  tzero=time.time()  \n",
    "  \n",
    "  ## 1st step: parse & describe the whole corpus\n",
    "  if \"PARSE\" in whichanalysis:\n",
    "    print(\"PARSE THE WHOLE CORPUS\")\n",
    "    biblio_parser(dataraw_dir,main_dir,database,expert)\n",
    "    print(\"DESCRIBE THE WHOLE CORPUS\")\n",
    "    out_dir=os.path.join(main_dir, 'freqs')\n",
    "    if not os.path.exists(out_dir): os.makedirs(out_dir)\n",
    "    describe_corpus(main_dir, out_dir, False)\n",
    "  \n",
    "\n",
    "  ## 2nd step: do the required analysis on whole corpus if no time window was given\n",
    "  if timeWND==[]: \n",
    "    if \"BC\" in whichanalysis: \n",
    "      print (\"BC ANALYSIS ON WHOLE CORPUS\")\n",
    "      BC_network(main_dir,main_dir,'',False,False,False)\n",
    "    if \"CO\" in whichanalysis:\n",
    "      print (\"COOC ANALYSIS ON WHOLE CORPUS\")\n",
    "      cooc_graph(main_dir,'', [], True, False)\n",
    "\n",
    "\n",
    "  ## 3rd step: filter, describe and analyse the corpus in the different time windows if any\n",
    "  if timeWND!=[]: print(\"studied periods:\", timeWND)\n",
    "  if (len(timeWND)>0 and not os.path.exists(os.path.join(main_dir, 'dataPeriods')) ): os.makedirs(os.path.join(main_dir, 'dataPeriods'))\n",
    "  if (\"CD\" in whichanalysis or \"BC\" in whichanalysis or \"CO\" in whichanalysis):\n",
    "    for period in timeWND:\n",
    "      print (\"DEALING WITH TIME PERIOD %d-%d\" % (period[0],period[1]))\n",
    "      suff='_'+str(period[0])+'_'+str(period[1])\n",
    "      # filter data\n",
    "      print(\"..FILTER\")\n",
    "      datadir=os.path.join(main_dir, 'dataPeriods/data'+suff)\n",
    "      if not os.path.exists(datadir): \n",
    "        os.makedirs(datadir)\n",
    "        PUBYfilter(main_dir, datadir, period)\n",
    "      # describe data\n",
    "      if \"CD\" in whichanalysis: \n",
    "        print(\"..DESCRIBE\")\n",
    "        out_dir=os.path.join(main_dir, 'freqs'+suff)\n",
    "        if not os.path.exists(out_dir): os.makedirs(out_dir)\n",
    "        describe_corpus(datadir, out_dir, False)\n",
    "      # bc analysis?\n",
    "      if \"BC\" in whichanalysis:\n",
    "        print(\"..BC ANALYSIS\")\n",
    "        BC_network(datadir,main_dir,suff,False,False,False)\n",
    "      if \"CO\" in whichanalysis:\n",
    "        print (\"COOC ANALYSIS\")\n",
    "        cooc_graph(datadir,suff,[],True,False)\n",
    "\n",
    "  ## 4th step: matching BC clusters from different time periods\n",
    "  # Right now, IT'S MADE IN A \"QUICK AND DIRTY\" WAY: compute the bc links between clusters of different time periods (and NOT the links between clusters of the same period), keep only those above a given threshold (this way only the strongest links will remain), define the predecessor / successor of each cluster as the one in the period before / after with which the links is higher, build streams or \"meta-clusters\" as chain of clusters which are successor/predessesors to one another.\n",
    "  # ==> this is a SIMPLE attempt at building an history, not a definitive one!  \n",
    "  # ==> the meta-clusters IDs will overwrite the \"group\" data in the json files used for the colors of the clusters in the BiblioMaps viz.  \n",
    "  if \"MATCH\" in whichanalysis:\n",
    "    print(\"..MATCHING CLUSTERS OF DIFF. PERIODS\")\n",
    "    # upload (top) partitions limited to the list of clusters kept in the viz\n",
    "    theparts=dict()\n",
    "    for period in timeWND:\n",
    "      suff='_'+str(period[0])+'_'+str(period[1])\n",
    "      datadir=os.path.join(main_dir, 'dataPeriods/data'+suff)\n",
    "      theparts[suff]=extract_part(datadir, main_dir, suff)\n",
    "    # compute the BC network between these clusters\n",
    "    (GG,LABEL)=prep_bcnetw(main_dir,theparts,THETA)\n",
    "    # detect meta-partition\n",
    "    metapart=detect_metapart(GG, LABEL) \n",
    "    # overwrite the groups in BCclusters_xxx.json files\n",
    "    for period in timeWND:\n",
    "      suff='_'+str(period[0])+'_'+str(period[1])\n",
    "      newgroup=dict()\n",
    "      for elt in [ee for ee in LABEL if ee.split('Z')[1]==suff]:\n",
    "        newgroup[int(elt.split('Z')[0])]=metapart[LABEL[elt]]\n",
    "      cfile=os.path.join(main_dir, 'jsonfiles/BCclusters'+suff+part_suffixBC+'.json')\n",
    "      updategroups(cfile,newgroup)\n",
    "\n",
    "\n",
    "  #... LOG\n",
    "  with open(os.path.join(main_dir,'AA_log.txt'),'a') as out:\n",
    "    mye='';\n",
    "    if (expert): mye='-e';\n",
    "    out.write(\"\\n\\n%s GMT\\nall_in_one.py -i %s -o %s -d %s %s\\n\" % (time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()), dataraw_dir, main_dir, database, mye))\n",
    "    if \"CD\" in whichanalysis and timeWND!=[]:\n",
    "      out.write(\"TO EXPLORE THE CORPUSES OF ALL PERIODS IN BIBLIOMAPS:\\nCopy / paste these lines on appropriate place (should be line 116) in the 'corpus.html' file + switch the 'howmany' variable (should be on line 100) from 'one' to 'several'\\n\")\n",
    "      out.write(\"\\ttimeWND=%s\\n\" % timeWND)\n",
    "    if \"BC\" in whichanalysis and timeWND!=[]:\n",
    "      out.write(\"TO EXPLORE THE BC CLUSTERS OF ALL PERIODS IN BIBLIOMAPS:\\n Copy / paste these lines on appropriate place (should be line 205) in the 'corpus.html' file + switch the 'howmany' variable (should be on line 190) from 'one' to 'several'\\n\")\n",
    "      out.write(\"\\ttimeWND=%s\\n\" % timeWND)\n",
    "      out.write(\"\\tfiles_suffix='%s'\\n\" % part_suffixBC)\n",
    "\n",
    "\n",
    "  ## END\n",
    "  print ('END of allinone - TOTAL TIME NEEDED: %ds' % (time.time()-tzero))\n",
    "\n",
    "  return\n",
    "\n",
    "## ##########################################################################################################\n",
    "## ##########################################################################################################\n",
    "\n",
    "def PUBYfilter(in_dir,out_dir,per):\n",
    "  ## INITIALIZATION\n",
    "  ymin=per[0]\n",
    "  ymax=per[1]\n",
    "  # files to filter (will not be taken into account if they don't exist)\n",
    "  filenames=[\"articles\", \"addresses\", \"authors\", \"countries\", \"institutions\", \"journals\", \"keywords\", \"references\", \"subjects\", \"subjects2\", \"AUaddresses\", \"cities\", \"fundingtext\", \"abstracts\"] \n",
    "\n",
    "  ## ##################################################\n",
    "  ## SELECT ID OF PUBLICATIONS TO KEEP\n",
    "  with open(os.path.join(in_dir, \"articles.dat\") ,\"r\") as file: data_lines=file.read().split(\"\\n\")[:-1]\n",
    "  nbpub=len(data_lines)  \n",
    "  TOKEEP=dict([(l.split(\"\\t\")[0],'') for l in data_lines if (int(l.split(\"\\t\")[2])<=ymax and int(l.split(\"\\t\")[2])>=ymin)])\n",
    "  del data_lines\n",
    "  print (\"..%d publications selected out of %d (%.2f%%)\" % (len(TOKEEP), nbpub, len(TOKEEP)*100.0/nbpub) )\n",
    "\n",
    "  ## ##################################################\n",
    "  ## PROCEED\n",
    "  for fff in filenames:\n",
    "    if (os.path.exists(os.path.join(in_dir, fff+\".dat\"))):\n",
    "      print (\".... filtering %s\" % fff)\n",
    "      with open(os.path.join(in_dir, fff+\".dat\") ,\"r\", encoding='utf8') as file: \n",
    "        data_lines=file.read().split(\"\\n\")[:-1] \n",
    "      with open(os.path.join(out_dir, fff+\".dat\") ,\"w\", encoding='utf8') as outfile: \n",
    "        for l in data_lines:\n",
    "          if l.split(\"\\t\")[0] in TOKEEP: outfile.write(\"%s\\n\" % l)\n",
    "        del data_lines\n",
    "  #\n",
    "  with open(os.path.join(in_dir, \"database.dat\") ,\"r\", encoding='utf8') as file: \n",
    "      foo=file.read()\n",
    "  with open(os.path.join(out_dir, \"database.dat\") ,\"w\", encoding='utf8') as outfile: \n",
    "      outfile.write(\"%s\\n\" % foo)\n",
    "  del foo\n",
    "\n",
    "  ## ##################################################\n",
    "  ## END  \n",
    "  return\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "def extract_part(datadir, maindir, suff):\n",
    "  # upload list of cluster to keep\n",
    "  ffin = open(os.path.join(maindir, \"jsonfiles/BCdefaultVAR\"+suff+part_suffixBC+\".json\"),'r')\n",
    "  defdata = json.loads(ffin.read())\n",
    "  tokeep=dict([(elt[0],'') for elt in defdata[\"TOPpositions\"]])\n",
    "  #\n",
    "  #... upload previously computed partition\n",
    "  fooname=\"partitions%s.txt\" % (suff+part_suffixBC)\n",
    "  ffin = open(os.path.join(datadir, fooname),'r')\n",
    "  foo = ffin.read()\n",
    "  lines = foo.split('\\n')\n",
    "  louvain_partition = json.loads(lines[0])\n",
    "  parttokeep={}\n",
    "  aux=str(len(louvain_partition)-2)\n",
    "  for key in louvain_partition[aux]:\n",
    "    if louvain_partition[aux][key] in tokeep:\n",
    "      parttokeep[int(key)]=louvain_partition[aux][key]\n",
    "  # end\n",
    "  return parttokeep\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "def prep_bcnetw(main_dir,theparts,THETA):\n",
    "  print (\"..Create the 'Bibliographic Coupling' weight table\")\n",
    "  nR = dict() # store the number of ref of each publication\n",
    "  ref_table = dict() # store the id of articles using a given ref\n",
    "  BC_table = dict() # store the number of common refs between pairs of publications\n",
    "\n",
    "  print (\"....loading refs table\")\n",
    "  with open(os.path.join(main_dir, \"references.dat\") ,\"r\", encoding='utf8') as file: \n",
    "    data_lines=file.read().split(\"\\n\")[:-1] \n",
    "  for l in data_lines:\n",
    "    foo =', '.join([l.split(\"\\t\")[k] for k in range(1,6)]).replace(',0,0','').replace(', 0, 0','').replace(', 0','')\n",
    "    pubid=int(l.split(\"\\t\")[0])\n",
    "    pubyear=int(l.split(\"\\t\")[2])\n",
    "    if foo in ref_table: ref_table[foo].append( pubid )\n",
    "    else: ref_table[foo] = [pubid]\n",
    "    if pubid not in nR: nR[pubid]=0\n",
    "    nR[pubid] += 1\n",
    "  del data_lines\n",
    "\n",
    "  print (\"....detecting common references\")\n",
    "  for foo in ref_table:\n",
    "    if len(ref_table[foo]) >= 2:\n",
    "      for i in ref_table[foo]:\n",
    "          for j in ref_table[foo]:\n",
    "              if (i<j):\n",
    "                  if i not in BC_table: BC_table[i] = dict()\n",
    "                  if j not in BC_table[i]: BC_table[i][j] = 0\n",
    "                  BC_table[i][j] += 1 \n",
    "  del ref_table\n",
    "\n",
    "  ## PREP NETWORK \n",
    "  print (\"....define graph in networkx format\")\n",
    "  cluster_links=dict()\n",
    "  LABEL=dict(); kompt=0;\n",
    "\n",
    "  for partA in theparts:\n",
    "    for i in theparts[partA]:\n",
    "      foo=str(theparts[partA][i])+'Z'+partA\n",
    "      if foo not in LABEL:\n",
    "        LABEL[foo]=kompt; kompt+=1;\n",
    "      if i in BC_table:\n",
    "        for partB in [part for part in theparts if part!=partA]:\n",
    "          for j in [jj for jj in BC_table[i] if jj in theparts[partB]]:\n",
    "            w_ij = (1.0 * BC_table[i][j]) / math.sqrt(nR[i] * nR[j])\n",
    "            foo=(str(theparts[partA][i])+'Z'+partA, str(theparts[partB][j])+'Z'+partB)\n",
    "            if foo not in cluster_links: \n",
    "              cluster_links[foo]=[0,0]; cluster_links[(foo[1],foo[0])]=[0,0]\n",
    "            cluster_links[foo][0]+=w_ij\n",
    "            cluster_links[foo][1]+=1\n",
    "      # in case of overlapping periods, we need to add potential self-loops from a publication to itself, EVEN IF i not in BC_table\n",
    "      for partB in [part for part in theparts if part!=partA]:\n",
    "        if i in theparts[partB]:\n",
    "          foo=(str(theparts[partA][i])+'Z'+partA, str(theparts[partB][i])+'Z'+partB)\n",
    "          if foo not in cluster_links: \n",
    "            cluster_links[foo]=[0,0]; cluster_links[(foo[1],foo[0])]=[0,0]\n",
    "          cluster_links[foo][0]+=1\n",
    "          cluster_links[foo][1]+=1\n",
    "  del(BC_table)\n",
    "\n",
    "  ## keep only links above THETA\n",
    "  GG=nx.Graph()\n",
    "\n",
    "  for (a,b) in cluster_links:\n",
    "    if (a<=b):\n",
    "      W=cluster_links[(a,b)][0]+cluster_links[(b,a)][0]\n",
    "      N=cluster_links[(a,b)][1]+cluster_links[(b,a)][1]\n",
    "      if W/N>=THETA:\n",
    "        GG.add_edge(LABEL[a], LABEL[b], weight=W/N)\n",
    "\n",
    "  return (GG, LABEL)\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "def detect_metapart(GG, LABEL):\n",
    "  # louvain partition ==> problem: may group together clusters on same period with same predecessor!\n",
    "  # foodendogram = community.generate_dendogram(GG, part_init=None)\n",
    "  # metapart = community.partition_at_level(foodendogram,len(foodendogram)-1)\n",
    "  # mod=community.modularity(metapart, GG)\n",
    "  # print (\"....top-clusters partition in %d meta-clusters, Q=%.4f\" % ( len(set(metapart.values())), mod))\n",
    " \n",
    "  # list all clusters by periods \n",
    "  allsuff=['_'+str(period[0])+'_'+str(period[1]) for period in timeWND]\n",
    "  byperiod=dict([(suff,[]) for suff in allsuff])\n",
    "  for lab in LABEL: byperiod[lab.split('Z')[1]].append(LABEL[lab])  \n",
    "  for per in allsuff:\n",
    "    print(\".... %s: %d top clusters\" % (per, len(byperiod[per]) ))\n",
    "\n",
    "  # aux function\n",
    "  def findmax(mylinks, suff):\n",
    "    lookin=[(com, mylinks[com][\"weight\"]) for com in mylinks if com in byperiod[suff]]\n",
    "    lookin.sort(key=lambda e:-e[1])\n",
    "    if len(lookin)>0: return lookin[0][0]\n",
    "    else: return -1\n",
    "\n",
    "  # find successors / predecessors\n",
    "  pred=dict();pred[-1]=-1;pred2=dict();pred2[-1]=-1;\n",
    "  succ=dict();succ[-1]=-1;succ2=dict();succ2[-1]=-1;\n",
    "  TT=len(allsuff)\n",
    "  for per in range(TT):\n",
    "    for com in byperiod[allsuff[per]]:\n",
    "      pred[com]=-1; succ[com]=-1;\n",
    "      pred2[com]=-1; succ2[com]=-1;\n",
    "      if com in GG.edge:\n",
    "        # predecessors\n",
    "        if per>0: pred[com]=findmax(GG.edge[com], allsuff[per-1])\n",
    "        if per>1: pred2[com]=findmax(GG.edge[com], allsuff[per-2])\n",
    "        # successors\n",
    "        if per<TT-1: succ[com]=findmax(GG.edge[com], allsuff[per+1])\n",
    "        if per<TT-2: succ2[com]=findmax(GG.edge[com], allsuff[per+2])\n",
    "      #print(com, pred[com], succ[com])\n",
    "\n",
    "  \"\"\"\n",
    "  foo=[(xx,len(set(list(aux)))) for xx,aux in itertools.groupby(list(pred.items()),key=lambda e:e[1])]\n",
    "  print (\"%d 2-splits\" % len([elt for elt in foo if elt[1]==2]))\n",
    "  print (\"%d 3p-splits\" % len([elt for elt in foo if elt[1]>2]))\n",
    "\n",
    "  foo=[(xx,len(set(list(aux)))) for xx,aux in itertools.groupby(list(succ.items()),key=lambda e:e[1])]\n",
    "  print (\"%d 2-merges\" % len([elt for elt in foo if elt[1]==2]))\n",
    "  print (\"%d 3p-merges\" % len([elt for elt in foo if elt[1]>2]))\n",
    "  \"\"\"\n",
    "\n",
    "  # metapart = streams of succ / pred\n",
    "  def buildstream(com):\n",
    "    stream=[com]\n",
    "    c=com;\n",
    "    while (c in succ and pred[succ[c]]==c): \n",
    "      stream.append(succ[c]); c=succ[c]; \n",
    "    c=com;\n",
    "    while (c in pred and succ[pred[c]]==c):\n",
    "      stream.append(pred[c]); c=pred[c];\n",
    "    return stream\n",
    "  ##\n",
    "  metapart={}\n",
    "  metakompt=0;\n",
    "  streams=[];\n",
    "  statstream=[0 for kk in range(TT)]\n",
    "  for com in LABEL:\n",
    "    if LABEL[com] not in metapart:\n",
    "      stream=buildstream(LABEL[com])\n",
    "      streams.append(stream)\n",
    "      for cc in stream: metapart[cc]=metakompt\n",
    "      metakompt+=1\n",
    "      statstream[len(stream)-1]+=1\n",
    "\n",
    "  for kk in range(TT):\n",
    "    if statstream[kk]>0: print(\".... %d streams spread over %d periods\" % (statstream[kk],kk+1))\n",
    "\n",
    "  return metapart\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "def updategroups(myfile, newgroup):\n",
    "\n",
    "  with open(myfile, 'r') as data_file: data = json.load(data_file)\n",
    "  old_new=dict()  \n",
    "  for n in data['nodes']: old_new[n['group']]=newgroup[n['id_top']]  \n",
    "  #\n",
    "  with open(myfile, 'r') as data_file: stuff = data_file.read()\n",
    "  stuff=stuff.replace('\"group\":','\"groupX\":')\n",
    "  #  \n",
    "  for g in old_new:\n",
    "    aa='\"groupX\":%d,' % g\n",
    "    bb='\"group\":%d,' % old_new[g]\n",
    "    stuff=stuff.replace(aa,bb)\n",
    "  # \n",
    "  with open(myfile, 'w') as f_out: f_out.write('%s' % stuff)\n",
    "\n",
    "  return\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "allinone(args.in_dir[0],args.out_dir[0],args.database,args.expert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All analyses at once - and on multiple time periods main\n",
    "Do not run the following cell. It is kept for Terminal launch of work"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def main():\n",
    "# usage: parser.py [-h] [--version] -i DIR [-o DIR] [-v] [-e]\n",
    "# \n",
    "# optional arguments:\n",
    "#   -h, --help            show this help message and exit\n",
    "#   --version             show program's version number and exit\n",
    "#   -o DIR, --output_dir DIR\n",
    "#                         output directory name\n",
    "#   -i DIR, --input_dir DIR\n",
    "#                         input directory name\n",
    "  # Parse line options.\n",
    "  # Try to have always the same input options\n",
    "  parser = argparse.ArgumentParser(description = 'parser')\n",
    "\n",
    "  parser.add_argument('--version', action='version', version='%(prog)s 1.1')\n",
    "  \n",
    "  parser.add_argument(\"-i\", \"--input_dir\", nargs=1, required=True,\n",
    "          action = \"store\", dest=\"in_dir\",\n",
    "          help=\"input directory name\",\n",
    "          metavar='DIR')\n",
    "          \n",
    "  parser.add_argument(\"-o\", \"--output_dir\", nargs=1, required=False,\n",
    "          action = \"store\", dest=\"out_dir\",\n",
    "          help=\"output directory name\",\n",
    "          default = \"blah\",\n",
    "          metavar='DIR')\n",
    "\n",
    "  parser.add_argument(\"-d\", \"--database\",\n",
    "          action = \"store\", dest=\"database\",\n",
    "          default = 'wos',\n",
    "          help=\"database [default %(default)s]\",\n",
    "          metavar='string')  \n",
    "\n",
    "  parser.add_argument(\"-e\", \"--expert\",\n",
    "          action = \"store_true\", dest=\"expert\",\n",
    "          default = False,\n",
    "          help=\"expert mode [default %(default)s]\")\n",
    "\n",
    "  #Analysis of input parameters\n",
    "  args = parser.parse_args()\n",
    "  \n",
    "  if (not os.path.exists(args.in_dir[0])):\n",
    "    print (\"Error: Input directory does not exist: \", args.in_dir[0])\n",
    "    exit()\n",
    "\n",
    "  \"\"\"\n",
    "  if (not os.path.exists(args.out_dir[0])):\n",
    "    args.out_dir[0]=args.in_dir[0]\n",
    "    print (\"Error: Output directory does not exist: \", args.out_dir[0])\n",
    "    exit()\n",
    "  \"\"\"\n",
    "\n",
    "  if args.out_dir == 'blah':\n",
    "    args.out_dir = args.in_dir\n",
    "\n",
    "  if args.database not in ['wos','scopus']:\n",
    "    print (\"Error: database must be either 'wos' or 'scopus'\")\n",
    "    exit()\n",
    "\n",
    "  ##      \n",
    "\n",
    "  allinone(args.in_dir[0],args.out_dir[0],args.database,args.expert)\n",
    "\n",
    "  return\n",
    "\n",
    "\n",
    "    \n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "print(Path.home())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
