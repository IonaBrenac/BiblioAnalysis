{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiblioAnalysis\n",
    "\n",
    "### Version: 4.2.0\n",
    "\n",
    "### Aims\n",
    "- This jupyter notebook results from the use analysis of BiblioTools2jupyter notebook and a new implementation of the following parts:\n",
    "    - Parsing: replaced and tested \n",
    "    - Corpus description: replaced and tested\n",
    "    - Filtering: replaced and tested, integrating the \"EXCLUSION\" mode and the recursive filtering\n",
    "    - Cooccurrence analysis : replaced and tested, integrating graph plot and countries GPS coordinates\n",
    "    - Coupling analyis : replaced and tested\n",
    "    \n",
    "### Created modules in the package BiblioAnalysis_Utils\n",
    "    - BiblioCooc.py\n",
    "    - BiblioCoupling.py\n",
    "    - BiblioDescription.py\n",
    "    - BiblioFilter.py    \n",
    "    - BiblioGeneralGlobals.py\n",
    "    - BiblioGraphPlot.py\n",
    "    - BiblioGui.py\n",
    "    - BiblioNltk.py\n",
    "    - BiblioParsingConcat.py\n",
    "    - BiblioParsingInstitutions.py\n",
    "    - BiblioParsingScopus.py\n",
    "    - BiblioParsingUtils.py\n",
    "    - BiblioParsingWos.py  \n",
    "    - BibloRefs.py\n",
    "    - BiblioSpecificGlobals.py\n",
    "    - BiblioSys.py\n",
    "    - BiblioTempDev.py\n",
    "\n",
    "### BiblioTool3.2 source\n",
    "http://www.sebastian-grauwin.com/bibliomaps/download.html \n",
    "\n",
    "### List of initial Python packages extracted from  BiblioTool3.2\n",
    "- biblio_parser.py\t⇒ pre-processes WOS / Scopus data files,\n",
    "- corpus_description.py\t⇒ performs a frequency analysis of the items in corpus,\n",
    "- filter.py\t⇒ filters the corpus according to a range of potential queries but still too specific\n",
    "- biblio_coupling.py\t⇒ performs a BC anaysis of the corpus,\n",
    "- cooc_graphs.py\t⇒ produces various co-occurrence graphs based on the corpus (call parameters changed)\n",
    "\n",
    "### Specifically required list of pip install \n",
    "(to be integrated in the setup.py of BiblioAnalysis_Utils)\n",
    "- !pip3 install fuzzywuzzy\n",
    "- !pip3 install squarify \n",
    "- !pip3 install python-louvain\n",
    "- !pip3 install python-Levenshtein\n",
    "- !pip3 install pyvis\n",
    "- !pip3 install screeninfo\n",
    "\n",
    "### Specifically required nltk downloads \n",
    "(integrated in BiblioNltk.py of BiblioAnalysis_Utils)\n",
    "- import nltk\n",
    "    - nltk.download('punkt')\n",
    "    - nltk.download('averaged_perceptron_tagger')\n",
    "    - nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary instructions\n",
    "#### These actions will be interactively performed in the next version of the Jupyter notebook\n",
    "- Create the 'BiblioAnalysis_Files/' folder in your 'Users/' folder\n",
    "<br>\n",
    "<br>\n",
    "- Create in this 'BiblioAnalysis_Files/' folder, the 'Configuration_Files/' folder\n",
    "<br>\n",
    "- Store the configuration files (config_filter.json) a the 'Configuration_Files/' folder that are:\n",
    "    - 'config_filter.json' used for the filtering of a corpus\n",
    "    - 'congig_temporal.json'used for the temporal development of item values in a set of annual coupuses \n",
    "<br>\n",
    "<br>\n",
    "- Create, in the 'Configuration_Files/' folder, your additional_files folder to be named 'Selection_Files/' \n",
    "<br>\n",
    "- Store your files (free names) of selected item values in this additional_files folder together with:\n",
    "    - 'TempDevK_full.txt' used to select the words to search in the description files of the corpuses for the temporal development of item values in the set of annual coupuses\n",
    "<br>\n",
    "<br>\n",
    "- Create, in the 'BiblioAnalysis_Files/' folder, your project folder\n",
    "<br>\n",
    "- Create the 'rawdata/' folder in your project folder\n",
    "<br>\n",
    "- Store your corpus file (either wos or scopus extraction) in the 'rawdata/' folder of your project folder\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0- User environment setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import platform\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "clear_output(wait=True)\n",
    "bold_text = bau.BOLD_TEXT\n",
    "light_text = bau.LIGHT_TEXT\n",
    "\n",
    "# Set the venv use status\n",
    "venv = False\n",
    "print('Virtual environment: ', venv)\n",
    "\n",
    "# Get the information of current operating system\n",
    "os_name = platform.uname().system\n",
    "print('Operating system:    ', os_name)\n",
    "if os_name=='Darwin':bau.add_site_packages_path(venv)\n",
    "\n",
    "# User identification\n",
    "user_root = Path.home()\n",
    "user_id =  str(user_root)[str(user_root).rfind('/')+1:]\n",
    "print('User:                ', user_id)\n",
    "expert =  False\n",
    "\n",
    "# Getting the corpuses folder\n",
    " # Setting the GUI titles\n",
    "gui_titles = {'main':   'Corpuses folder selection window',\n",
    "              'result': 'Selected folder'}\n",
    "gui_buttons = ['SELECTION','HELP']\n",
    "\n",
    "corpuses_folder = bau.select_folder_gui_new(user_root, gui_titles, gui_buttons, bau.GUI_DISP)\n",
    "print('\\nCorpuses folder:', corpuses_folder)\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following cells untill chapter 1 (parsings merging) allow to test the addresses parsing to identify the institutions of the publication authors\n",
    "## These cells will be suppressed after integration of the useful functions to the BiblioParsingInstitutions.py module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_symbol_remove(text, only_ascii = True, strip = True):\n",
    "    '''The function `special_symbol_remove` remove accentuated characters in the string 'text'\n",
    "    and ignore non-ascii characters if 'only_ascii' is true. Finally, spaces at the ends of 'text'\n",
    "    are removed if strip is true.\n",
    "    \n",
    "    Args:\n",
    "        text (str): the text where to remove special symbols.\n",
    "        only_ascii (boolean): True to remove non-ascii characters from 'text' (default: True).\n",
    "        strip (boolean): True to remove spaces at the ends of 'text' (default: True).\n",
    "        \n",
    "    Returns:\n",
    "        (str): the modified string 'text'.\n",
    "    \n",
    "    '''\n",
    "    # Standard library imports\n",
    "    import functools\n",
    "    import unicodedata\n",
    "\n",
    "    if only_ascii:\n",
    "        nfc = functools.partial(unicodedata.normalize,'NFD')\n",
    "        text = nfc(text). \\\n",
    "                   encode('ascii', 'ignore'). \\\n",
    "                   decode('utf-8')\n",
    "    else:\n",
    "        nfkd_form = unicodedata.normalize('NFKD',text)\n",
    "        text = ''.join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "    if strip:\n",
    "        text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def town_names_uniformization(text):\n",
    "    '''the `town_names_uniformization` function replaces in the string 'text'\n",
    "    symbols and words defined by the keys of the dictionaries 'DIC_TOWN_SYMBOLS'\n",
    "    and 'DIC_TOWN_WORDS' by their corresponding values in these dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The string where changes will be done.\n",
    "        \n",
    "    Returns:\n",
    "        (str): the modified string.\n",
    "        \n",
    "    Notes:\n",
    "        The globals 'DIC_TOWN_SYMBOLS' and 'DIC_TOWN_WORDS' are imported from\n",
    "        `BiblioSpecificGlobals` module of `BiblioAnalysis_Utils' package.\n",
    "    \n",
    "    '''\n",
    "    # Local imports\n",
    "    from BiblioAnalysis_Utils.BiblioSpecificGlobals import DIC_TOWN_SYMBOLS\n",
    "    from BiblioAnalysis_Utils.BiblioSpecificGlobals import DIC_TOWN_WORDS\n",
    "    \n",
    "    # Uniformizing symbols in town names using the dict 'DIC_TOWN_SYMBOLS'\n",
    "    for town_symb in DIC_TOWN_SYMBOLS.keys():\n",
    "        text = text.replace(town_symb, DIC_TOWN_SYMBOLS[town_symb])\n",
    "\n",
    "    # Uniformizing words in town names using the dict 'DIC_TOWN_WORDS'\n",
    "    for town_word in DIC_TOWN_WORDS.keys():\n",
    "        text = text.replace(town_word, DIC_TOWN_WORDS[town_word])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals for institutions\n",
    "\n",
    "#############################################\n",
    "# Specific globals for institutions parsing #\n",
    "#############################################\n",
    "\n",
    "# Standard library imports\n",
    "import re\n",
    "\n",
    "# Local imports \n",
    "#from BiblioAnalysis_Utils.BiblioParsingUtils import special_symbol_remove\n",
    "#from BiblioAnalysis_Utils.BiblioParsingUtils import town_names_uniformization\n",
    "\n",
    "# For replacing symbols in town names\n",
    "DIC_TOWN_SYMBOLS = {\"-\": \" \",\n",
    "                     }\n",
    "\n",
    "# For replacing names in town names\n",
    "DIC_TOWN_WORDS = {\" lez \":  \" les \",\n",
    "                   \"saint \": \"st \",\n",
    "                   } \n",
    "\n",
    "# For replacing aliases of a word by a word (case sensitive)\n",
    "DIC_WORD_RE_PATTERN = {}\n",
    "DIC_WORD_RE_PATTERN['University'] = re.compile(r'\\bUniv[aàädeéirstyz]{0,8}\\b\\.?')\n",
    "DIC_WORD_RE_PATTERN['Laboratory'] = re.compile(r\"'?\\bLab\\b\\.?\" \\\n",
    "                                                    +  \"|\" \\\n",
    "                                                    + r\"'?\\bLabor[aeimorstuy]{0,7}\\b\\.?\")\n",
    "DIC_WORD_RE_PATTERN['Center'] = re.compile(r\"\\b[CZ]ent[erum]{1,3}\\b\\.?\")\n",
    "DIC_WORD_RE_PATTERN['Department'] = re.compile(r\"\\bD[eé]{1}p[artemnot]{0,9}\\b\\.?\")\n",
    "DIC_WORD_RE_PATTERN['Institute'] = re.compile(r\"\\bInst[ituteosky]{0,7}\\b\\.?\")\n",
    "DIC_WORD_RE_PATTERN['Faculty'] = re.compile(r\"\\bFac[lutey]{0,4}\\b\\.?\")\n",
    "DIC_WORD_RE_PATTERN['School'] = re.compile(r\"\\bSch[ol]{0,3}\\b\\.?\")\n",
    "\n",
    "\n",
    "# For keeping chunks of addresses (without accents and in lower case)\n",
    "    # Setting a list of keeping words\n",
    "        # Setting a basic list of keeping words\n",
    "_BASIC_KEEPING_WORDS = list(DIC_WORD_RE_PATTERN.keys())\n",
    "        # Setting a user list of keeping words\n",
    "_USER_KEEPING_WORDS = ['Beamline', 'CEA', 'CNRS', 'EA', 'ED', 'FR', 'IMEC', 'INES', 'IRCELYON', \\\n",
    "                      'LaMCoS', 'LEPMI', 'LITEN', 'LOCIE', 'STMicroelectronics', \\\n",
    "                       'TNO', 'ULR', 'UMR', 'VTT']\n",
    "_KEEPING_WORDS = _BASIC_KEEPING_WORDS + _USER_KEEPING_WORDS\n",
    "        # Removing accents keeping non adcii characters and converting to lower case the keeping words, by default\n",
    "KEEPING_WORDS =[special_symbol_remove(x, only_ascii = False, strip = False).lower() for x in _KEEPING_WORDS]\n",
    "\n",
    "\n",
    "# For droping chunks of addresses (without accents and in lower case)\n",
    "    # Setting a list of droping suffixes\n",
    "_DROPING_SUFFIX = [\"platz\", \"strae\", \"strasse\", \"straße\", \"vej\"] # added \"ring\" but drops chunks containing \"Engineering\"\n",
    "        # Removing accents keeping non adcii characters and converting to lower case the droping suffixes, by default\n",
    "DROPING_SUFFIX = [special_symbol_remove(x, only_ascii = False, strip = False).lower() for x in _DROPING_SUFFIX]\n",
    "\n",
    "    # Setting a list of droping words\n",
    "_DROPING_WORDS = [\"allee\", \"av\", \"avda\", \"ave\", \"avenue\", \"bat\", \"batiment\", \"boulevard\", \"blv.\", \"box\", \"bp\", \"calle\", \n",
    "                 \"campus\", \"carrera\", \"cedex\", \"cesta\", \"chemin\", \"ch.\", \"city\", \"ciudad\", \"cours\", \"cs\", \"district\", \n",
    "                 \"lane\", \"mall\", \"no.\", \"po\", \"p.\", \"rd\", \"route\", \"rue\", \"road\", \"sec.\", \"st.\", \"strada\",\n",
    "                 \"street\", \"str.\", \"via\", \"viale\", \"villa\"]\n",
    "        # Removing accents keeping non adcii characters and converting to lower case the droping words, by default\n",
    "_DROPING_WORDS = [special_symbol_remove(x, only_ascii = False, strip = False).lower() for x in _DROPING_WORDS]\n",
    "        # Escaping the regex meta-character \".\" from the droping words, by default\n",
    "DROPING_WORDS = [x.replace(\".\", r\"\\.\") for x in _DROPING_WORDS]\n",
    "\n",
    "\n",
    "# For droping towns in addresses \n",
    "    # Setting string listing raw french-town names \n",
    "_FR_UNIVERSITY_TOWNS = '''Aix-Marseille,Aix-en-Provence,Amiens,Angers,Arras,Aulnay-sous-bois,Avignon,Aulnoye-Aymeries,\n",
    "                         Besançon,Bordeaux,Bouguenais,Brest,Caen,Chambéry,Clermont-Ferrand,Dijon,\n",
    "                         Fraisses,Gif-sur-Yvette,Grenoble,La Rochelle,Le Bourget-du-Lac,\n",
    "                         Le Havre,Le Mans,Lille,Limoges,Lyon,Marseille,Metz,Montbonnot,Montpellier,Mulhouse,Moret-Sur-Loing,\n",
    "                         Nancy,Nantes,Nice,Nîmes,Orléans,Paris,Pau,Palaiseau,Perpignan,Pointe-à-Pitre,\n",
    "                         Poitiers,Reims,Rennes,Rouen,Saint-Denis de La Réunion,Saint-Étienne,\n",
    "                         Saint-Paul-lez-Durance,Saint-Nazaire,Strasbourg,Toulon,Toulouse,Tours,Troyes,\n",
    "                         Valenciennes,Villeurbanne'''\n",
    "\n",
    "    # Converting to lower case\n",
    "_FR_UNIVERSITY_TOWNS = _FR_UNIVERSITY_TOWNS.lower() \n",
    "\n",
    "    # Uniformizing town names \n",
    "_FR_UNIVERSITY_TOWNS = town_names_uniformization(_FR_UNIVERSITY_TOWNS)\n",
    "\n",
    "    # Removing accents keeping non adcii characters\n",
    "_FR_UNIVERSITY_TOWNS = special_symbol_remove(_FR_UNIVERSITY_TOWNS, only_ascii = False, strip = False)\n",
    "\n",
    "    # Converting to list of lower-case stripped names of towns \n",
    "FR_UNIVERSITY_TOWNS = [x.strip() for x in _FR_UNIVERSITY_TOWNS.split(',')]\n",
    "\n",
    "\n",
    "###################\n",
    "# General globals #\n",
    "###################\n",
    "\n",
    "# For changing particularly encoded symbols\n",
    "DIC_CHANGE_APOST = {\"”\": \"'\",\n",
    "                    \"’\": \"'\",   \n",
    "                    '\"': \"'\",\n",
    "                    \"“\": \"'\",   \n",
    "                    \"'\": \"'\",   \n",
    "                    } \n",
    "\n",
    "APOSTROPHE_CHANGE = str.maketrans(DIC_CHANGE_APOST)\n",
    "\n",
    "# For replacing dashes by hyphen-minus\n",
    "DIC_CHANGE_DASHES = {\"‐\": \"-\",   # Non-Breaking Hyphen to hyphen-minus\n",
    "                     \"—\": \"-\",   # En-dash to hyphen-minus\n",
    "                     \"–\": \"-\",   # Em-dash to hyphen-minus\n",
    "                     \"–\": \"-\",\n",
    "                     }\n",
    "\n",
    "DASHES_CHANGE = str.maketrans(DIC_CHANGE_DASHES)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def address_standardization(raw_address):\n",
    "    \n",
    "    '''The `address_standardization` function standardizes the string 'raw_address' by replacing\n",
    "    all aliases of a word, such as 'University', 'Institute', 'Center' and' Department', \n",
    "    by a standardized version.\n",
    "    The aliases of a given word are captured using a specific regex which is case sensitive defined \n",
    "    by the global 'DIC_WORD_RE_PATTERN'.\n",
    "    The aliases may contain symbols from a given list of any language including accentuated ones. \n",
    "    The length of the alliases is limited to a maximum according to the longest alias known.\n",
    "        ex: The longest alias known for the word 'University' is 'Universidade'. \n",
    "            Thus, 'University' aliases are limited to 12 symbols begenning with the base 'Univ' \n",
    "            + up to 8 symbols from the list '[aàädeéirstyz]' and possibly finishing with a dot.\n",
    "            \n",
    "    Then, dashes are replaced by a hyphen-minus using 'DASHES_CHANGE' global and apostrophes are replaced \n",
    "    by the standard cote using 'APOSTROPHE_CHANGE' global.         \n",
    "    \n",
    "    Args:\n",
    "        raw_address (str): the full address to be standardized.\n",
    "\n",
    "        \n",
    "    Returns:\n",
    "        (str): the full standardized address.\n",
    "        \n",
    "    Notes:\n",
    "        The global 'DIC_WORD_RE_PATTERN' and 'UNKNOWN' are imported from `BiblioSpecificGlobals` module \n",
    "        of `BiblioAnalysis_Utils` package.\n",
    "        The globals 'DASHES_CHANGE' and 'APOSTROPHE_CHANGE' are imported from `BiblioGeneralGlobals` module \n",
    "        of `BiblioAnalysis_Utils` package.\n",
    "        The function `country_normalization` is imported from `BiblioParsingInstitutions` module \n",
    "        of `BiblioAnalysis_Utils` package.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Standard library imports\n",
    "    import re\n",
    "    \n",
    "    # Local imports\n",
    "    from BiblioAnalysis_Utils.BiblioParsingUtils import country_normalization\n",
    "    #from BiblioAnalysis_Utils.BiblioGeneralGlobals import APOSTROPHE_CHANGE\n",
    "    #from BiblioAnalysis_Utils.BiblioGeneralGlobals import DASHES_CHANGE\n",
    "    #from BiblioAnalysis_Utils.BiblioSpecificGlobals import DIC_WORD_RE_PATTERN\n",
    "    from BiblioAnalysis_Utils.BiblioSpecificGlobals import UNKNOWN\n",
    "    \n",
    "    # Uniformizing words\n",
    "    standard_address = raw_address\n",
    "    for word_to_subsitute, re_pattern in DIC_WORD_RE_PATTERN.items():\n",
    "        standard_address = re.sub(re_pattern,word_to_subsitute + ' ',standard_address)\n",
    "    standard_address = re.sub(r'\\s+',' ',standard_address)\n",
    "    standard_address = re.sub(r'\\s,',',',standard_address)\n",
    "    \n",
    "    # Uniformizing dashes\n",
    "    standard_address = standard_address.translate(DASHES_CHANGE)\n",
    "    \n",
    "    # Uniformizing apostrophes\n",
    "    standard_address = standard_address.translate(APOSTROPHE_CHANGE)\n",
    "    \n",
    "    # Uniformizing countries\n",
    "    country_pos = -1\n",
    "    first_raw_affiliations_list = standard_address.split(',')\n",
    "    raw_affiliations_list = sum([x.split(' - ') for x in first_raw_affiliations_list],[])\n",
    "    country = country_normalization(raw_affiliations_list[country_pos].strip())\n",
    "    space = \" \"\n",
    "    if country != UNKNOWN:\n",
    "        standard_address = ','.join(raw_affiliations_list[:-1] + [space + country])\n",
    "    else:\n",
    "        standard_address = ','.join(raw_affiliations_list + [space + country])\n",
    "\n",
    "    return standard_address\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_address = \" STMicroelect Crolles 2 SAS, 850 Rue Jean Monnet, F-38926 Crolles, France\"\n",
    "address_standardization(raw_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_affiliations_list(std_address, drop_to_end = False, verbose = False):\n",
    "    \n",
    "    '''The `get_affiliations_list` function extracts first, the country and then, the list \n",
    "    of institutions from a standardized address. It splits the address in list of chuncks \n",
    "    separated by coma or isolated hyphen-minus.\n",
    "    The country is present as the last chunk of the spliting.\n",
    "    The other chunks are kept as institutions if they contain at least one word among \n",
    "    those listed in the 'KEEPING_WORDS' global or if they do not contain any item \n",
    "    searched by the `search_droping_items` function.\n",
    "    The first chunck is always kept in the final institutions list.\n",
    "    The spaces at the ends of the items of the final institutions list are removed.\n",
    "    \n",
    "    Args:\n",
    "        std_address (str): the full address to be parsed in list of institutions and country.\n",
    "        drop_to_end (boolean): if true, all chuncks are dropped after the first found to drop,\n",
    "                               (default: False).\n",
    "        verbose (boolean): if true, prints are run (default: False). \n",
    "        \n",
    "    Returns:\n",
    "        (tuple): the tuple with list of kept chuncks, country and list of dropped chuncks .\n",
    "        \n",
    "    Notes:\n",
    "        The function `search_droping_items` is imported from `BiblioParsingInstitutions` module \n",
    "        of `BiblioAnalysis_Utils` package.\n",
    "        The function `country_normalization` is imported from `BiblioParsingInstitutions` module \n",
    "        of `BiblioAnalysis_Utils` package.\n",
    "        The globals 'KEEPING_WORDS' and 'UNKNOWN' are imported from `BiblioSpecificGlobals` module \n",
    "        of `BiblioAnalysis_Utils` package.        \n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Standard library imports\n",
    "    import re\n",
    "    from string import Template\n",
    "    \n",
    "    # Local imports\n",
    "    #from BiblioAnalysis_Utils.BiblioParsingInstitutions import search_droping_items \n",
    "    #from BiblioAnalysis_Utils.BiblioSpecificGlobals import KEEPING_WORDS\n",
    "    \n",
    "    def _search_keaping_words(text):\n",
    "        '''The `_search_keaping_words` internal function searches in 'text' for isolated words \n",
    "        given by the 'KEEPING_WORDS' global using a templated regex.\n",
    "        \n",
    "        Args:\n",
    "            text (str): the string where the words are searched after being converted to lower case.\n",
    "            \n",
    "        Returns:\n",
    "            (boolean): True if a word given by the 'KEEPING_WORDS' global is found.\n",
    "            \n",
    "        Notes:\n",
    "            The global 'KEEPING_WORDS' is imported from `BiblioSpecificGlobals` module \n",
    "            of `BiblioAnalysis_Utils` package.               \n",
    "\n",
    "        '''\n",
    "        keeping_words_template = Template(r'\\b$word\\b')\n",
    "\n",
    "        keeping_word_found = False\n",
    "        for word_to_keep in KEEPING_WORDS:\n",
    "            re_keep_words = re.compile(keeping_words_template.substitute({\"word\":word_to_keep}))\n",
    "            result = re.search(re_keep_words,text.lower())\n",
    "            if result is not None:\n",
    "                keeping_word_found = True\n",
    "                break\n",
    "        return keeping_word_found\n",
    "\n",
    "    # Splitting standard address in chuncks set in a raw-affiliations list\n",
    "    first_raw_affiliations_list = std_address.split(',')\n",
    "    raw_affiliations_list = sum([x.split(' - ') for x in first_raw_affiliations_list],[])\n",
    "    if verbose:\n",
    "        print('Full standard address:',std_address)\n",
    "        print('first_raw_affiliations_list:',first_raw_affiliations_list)\n",
    "        print('raw_affiliations_list flattenned:',raw_affiliations_list)\n",
    "        print()\n",
    "    \n",
    "    # Setting country index in raw-affiliations list\n",
    "    country_pos = -1\n",
    "    country = raw_affiliations_list[country_pos].strip()\n",
    "    if verbose:\n",
    "        print('country:', country)\n",
    "    \n",
    "    # Initializing the affiliations list by keeping systematically the first chunck of the full address\n",
    "    affiliations_list = [raw_affiliations_list[0]]\n",
    "    \n",
    "    # Initializing the list of chuncks to drop from the raw-affiliations list\n",
    "    affiliation_drop = []                                                                 #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!           \n",
    "    \n",
    "    # Searching for chuncks to keep and chuncks to drop in the raw-affiliations list, the first chunck and the country excepted\n",
    "    if len(raw_affiliations_list)>2:   \n",
    "        for affiliation in raw_affiliations_list[1:country_pos]:\n",
    "            \n",
    "            keeping_word_found = _search_keaping_words(affiliation)            \n",
    "            if keeping_word_found: \n",
    "                affiliations_list.append(affiliation)\n",
    "                if verbose: \n",
    "                    print('Keeping word found in:',affiliation)\n",
    "                    print()\n",
    "\n",
    "            else:\n",
    "                droping_item_found = search_droping_items(affiliation, country, verbose = verbose)\n",
    "\n",
    "                if verbose:\n",
    "                    print('No keeping word found in:',affiliation)\n",
    "                    print()\n",
    "\n",
    "                if not droping_item_found:\n",
    "                    if verbose:\n",
    "                        print('  No droping item found in:',affiliation)\n",
    "                        print()\n",
    "                    affiliations_list.append(affiliation)\n",
    "\n",
    "                else:\n",
    "                    affiliation_drop.append(affiliation)                                                 #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                    if verbose:\n",
    "                        print('  Droping item found in:',affiliation)\n",
    "                        print()\n",
    "                    if drop_to_end: break \n",
    "                \n",
    "    # Removing spaces from the affiliations kept \n",
    "    affiliations_list = [x.strip() for x in affiliations_list]\n",
    "    if verbose:\n",
    "        print('affiliations_list stripped:',affiliations_list)\n",
    "        print()\n",
    "    \n",
    "    return (affiliations_list,country,affiliation_drop)                                            #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_droping_items(affiliation, country, verbose = False):\n",
    "    \n",
    "    '''The `search_droping_items` function searches for several item types in 'affiliation' after accents removal \n",
    "    and converting in lower case even if the search is case sensitive.\n",
    "    It uses the following internal functions:\n",
    "        - The `_search_droping_words` function searches for words given by the 'DROPING_WORDS' global \n",
    "        such as 'Avenue'.\n",
    "        - The `_search_droping_suffix` function searches for words ending by a suffix among \n",
    "        those given by the 'DROPING_SUFFIX' global such as 'platz'.\n",
    "        - The `_search_droping_bp` function searches for words that are postal-box numbers such as 'BP54'.\n",
    "        - The `_search_droping_zip` function searches for words that are zip codes such as 'F-38000'.\n",
    "        - The `_search_droping_town` function searches for words that are french towns \n",
    "        listed in the 'FR_UNIVERSITY_TOWNS' global.\n",
    "    \n",
    "    It is to remind that in a regex:\n",
    "        - '\\b' captures the transition between a non-alphanumerical symbol and an alphanumerical symbol \n",
    "        and vice-versa.\n",
    "        - '\\B' captures the transition between two alphanumerical symbols.\n",
    "    \n",
    "    Args:\n",
    "        affiliation (str): a chunck of a standardized address where droping items are searched.\n",
    "        country (str): the string that contains the country.\n",
    "       \n",
    "    Returns:\n",
    "        (boolean): True if at least one droping item is found.\n",
    "    \n",
    "    Notes:\n",
    "        The function `special_symbol_remove` is imported from `BiblioParsingUtils` module \n",
    "        of `BiblioAnalysis_Utils` package.\n",
    "        The globals 'DROPING_SUFFIX', 'DROPING_WORDS', 'KEEPING_PREFIX' are imported from `BiblioSpecificGlobals` module \n",
    "        of `BiblioAnalysis_Utils` package.\n",
    "        The global 'FR_UNIVERSITY_TOWNS' is imported from `BiblioGeneralGlobals` module \n",
    "        of `BiblioAnalysis_Utils` package.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Standard library imports\n",
    "    import re\n",
    "    from string import Template\n",
    "    \n",
    "    # Local imports \n",
    "    #from BiblioAnalysis_Utils.BiblioParsingUtils import special_symbol_remove\n",
    "    #from BiblioAnalysis_Utils.BiblioParsingUtils import town_names_uniformization\n",
    "    #from BiblioAnalysis_Utils.BiblioSpecificGlobals import DROPING_SUFFIX\n",
    "    #from BiblioAnalysis_Utils.BiblioSpecificGlobals import DROPING_WORDS\n",
    "    #from BiblioAnalysis_Utils.BiblioSpecificGlobals import KEEPING_PREFIX\n",
    "    #from BiblioAnalysis_Utils.BiblioSpecificGlobals import FR_UNIVERSITY_TOWNS\n",
    "    #from BiblioAnalysis_Utils.BiblioSpecificGlobals import ZIP_CODES\n",
    "\n",
    "    def _search_droping_words():\n",
    "        '''The `_search_droping_words` internal function searches in 'affiliation_mod' for isolated words \n",
    "        given by the 'DROPING_WORDS' global using a templated regex.\n",
    "        \n",
    "        Args:\n",
    "            affiliation_mod (str): the string where the words are searched.\n",
    "            \n",
    "        Returns:\n",
    "            (boolean): True if a word given by the 'DROPING_WORDS' global is found.\n",
    "            \n",
    "        Notes:\n",
    "            The global 'DROPING_WORDS' is imported from `BiblioSpecificGlobals` module \n",
    "            of `BiblioAnalysis_Utils` package.               \n",
    "\n",
    "        '''\n",
    "        \n",
    "        droping_words_template = Template(  r'[\\s(]$word[\\s)]'     # For instence capturing \"avenue\" in \"12 Avenue Azerty\" or \" cedex\" in \"azert cedex\".\n",
    "                                                                # in \"12 Avenue Azerty\" or \" cedex\" in \"azert cedex\".\n",
    "                                          + '|'\n",
    "                                          + r'[\\s]$word$$')\n",
    "                                                               \n",
    "\n",
    "        flag = False\n",
    "        for word_to_drop in DROPING_WORDS:\n",
    "            re_drop_words = re.compile(droping_words_template.substitute({\"word\":word_to_drop}))\n",
    "            result = re.search(re_drop_words,affiliation_mod)\n",
    "            if result is not None:\n",
    "                flag = True\n",
    "                if verbose:\n",
    "                    print('Droping word (full word):', word_to_drop)\n",
    "                break\n",
    "        return flag\n",
    "\n",
    "    def _search_droping_suffix():\n",
    "        '''The `_search_droping_suffix` internal function searches in 'affiliation_mod' for words \n",
    "        ending by a suffix among those given by the 'DROPING_SUFFIX' global \n",
    "        using a templated regex.\n",
    "        \n",
    "        Args:\n",
    "            affiliation_mod (str): the string where the suffixes given by the 'DROPING_SUFFIX' global\n",
    "                                   are searched.\n",
    "            \n",
    "        Returns:\n",
    "            (boolean): True if a suffix given by the 'DROPING_SUFFIX' global is found.\n",
    "            \n",
    "        Notes:\n",
    "            The global 'DROPING_SUFFIX' is imported from `BiblioSpecificGlobals` module \n",
    "            of `BiblioAnalysis_Utils` package.               \n",
    "\n",
    "        '''\n",
    "        \n",
    "        droping_suffix_template = Template(r'\\B$word\\b')    # For instence capturing \"platz\" \n",
    "                                                            # in \"Azertyplatz uiops12\".\n",
    "\n",
    "        flag = False\n",
    "        for word_to_drop in DROPING_SUFFIX:\n",
    "            re_drop_words = re.compile(droping_suffix_template.substitute({\"word\":word_to_drop}))\n",
    "            result = re.search(re_drop_words,affiliation_mod)\n",
    "            if result is not None:\n",
    "                flag = True\n",
    "                if verbose:\n",
    "                    print('Droping word (suffix):', word_to_drop)\n",
    "                break\n",
    "        return flag\n",
    "\n",
    "\n",
    "    def  _search_droping_bp():\n",
    "        '''The `_search_droping_bp` internal function searches in 'affiliation_mod' for words \n",
    "        begenning with 'bp' followed by digits using a non case sensitive regex.\n",
    "        \n",
    "        Args:\n",
    "            affiliation_mod (str): the string where the words are searched.\n",
    "            \n",
    "        Returns:\n",
    "            (boolean): True if a word is found.\n",
    "            \n",
    "        '''\n",
    "\n",
    "        re_bp = re.compile(r'\\bbp\\d+\\b')     # For instence capturing \"bp12\" in \"azert BP12 yui_OP\".\n",
    "\n",
    "        flag = False\n",
    "        result = re.search(re_bp,affiliation_mod)\n",
    "        if result is not None:\n",
    "            if verbose:\n",
    "                print('Droping word: postal box')\n",
    "            flag = True\n",
    "        return flag \n",
    "\n",
    "    def _search_droping_zip():\n",
    "        '''The `_search_droping_zip` internal function searches in 'affiliation_mod' for words \n",
    "        similar to zip codes except those begenning with a prefix from the 'KEEPING_PREFIX' global\n",
    "        followed by 4 digits using case-sensitive regexes. \n",
    "        Regex for zip-codes search uses the 'ZIP_CODES' dict global for countries from 'ZIP_CODES.keys()'.\n",
    "        Specific regex are set for ''\n",
    "\n",
    "        Args:\n",
    "            affiliation_mod (str): the string where the words are searched.\n",
    "            country (str): the string that contains the country.\n",
    "\n",
    "        Returns:\n",
    "            (boolean): True if a word different from those begenning with 'umr' \n",
    "                       followed by 4 digits is found.\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Setting regex for zip-codes search\n",
    "        if country in ZIP_CODES.keys():\n",
    "            zip_template = Template(r'\\b($zip_letters)[\\s-]?(\\d{$zip_digits})\\b')\n",
    "            letters_list, digits_list = ZIP_CODES[country]['letters'], ZIP_CODES[country]['digits']\n",
    "            letters_join = '|'.join(letters_list) if len(letters_list) else ''\n",
    "            pattern_zip_list = [zip_template.substitute({\"zip_letters\": letters_join, \"zip_digits\":digits})\n",
    "                                for digits in digits_list]     \n",
    "            re_zip = re.compile('|'.join(pattern_zip_list))\n",
    "\n",
    "        elif country == 'United Kingdom':\n",
    "            # Capturing: for instence, \" BT7 1NN\" or \" WC1E 6BT\" or \" G128QQ\"\n",
    "            #            \" a# #a\", \" a# #az\", \" a# ##a\", \" a# ##az\",\n",
    "            #            \" a##a\", \" a##az\", \" a###a\", \" a###az\",\n",
    "            #\n",
    "            #            \" a#a #a\", \" a#a #az\", \" a#a ##a\", \" a#a ##az\",\n",
    "            #            \" a#a#a\", \" a#a#az\", \" a#a##a\", \" a#a##az\",\n",
    "            #\n",
    "            #            \" a## #a\", \" a## #az\", \" a## ##a\", \" a## ##az\",\n",
    "            #            \" a###a\", \" a###az\", \" a####a\", \" a####az\",\n",
    "            #            \n",
    "            #            \" a##a #a\", \" a##a #az\", \" a##a ##a\", \" a##a ##az\",\n",
    "            #            \" a##a#a\", \" a##a#az\", \" a##a##a\", \" a##a##az\",\n",
    "            #            \n",
    "            #            \" az# #a\", \" az# #az\", \" az# ##a\", \" az# ##az\",\n",
    "            #            \" az##a\", \" az##az\", \" az###a\", \" az###az\",\n",
    "            #\n",
    "            #            \" az#a #a\", \" az#a #az\", \" az#a ##a\", \" az#a ##az\",\n",
    "            #            \" az#a#a\", \" az#a#az\", \" az#a##a\", \" az#a##az\",\n",
    "            #\n",
    "            #            \" az## #a\", \" az## #az\", \" az## ##a\", \" az## ##az\",\n",
    "            #            \" az###a\", \" az###az\", \" az###a\", \" az####az\",\n",
    "            #\n",
    "            #            \" az##a #a\", \" az##a #az\", \" az##a ##a\", \" az##a ##az\",\n",
    "            #            \" az##a#a\", \" az##a#az\", \" az##a#a\", \" az##a##az\",\n",
    "\n",
    "            re_zip = re.compile(r'^\\s?[a-z]{1,2}\\d{1,2}[a-z]{0,1}\\s?\\d{1,2}[a-z]{1,2}$')\n",
    "\n",
    "        elif country == 'United States' or country == 'Canada':\n",
    "            # Capturing: for instence, \" NY\" or ' NI BT48 0SG' or \" ON K1N 6N5\" \n",
    "            #            \" az\" or \" az \" + 6 or 7 characters in 2 parts separated by spaces\n",
    "\n",
    "            re_zip = re.compile(r'^\\s?[a-z]{2}$' + '|' + r'^\\s?[a-z]{2}\\s[a-z0-9]{3,4}\\s[a-z0-9]{2,3}$')\n",
    "        else:\n",
    "            print('country not found:', country)\n",
    "            return False\n",
    "\n",
    "        # Setting search regex of embedding digits\n",
    "        re_digits = re.compile(r'\\s\\d+(-\\d+)?\\b'      # For instence capturing \" 1234\" in \"azert 1234-yui_OP\"\n",
    "                                                      # or \" 1\" in \"azert 1-yui_OP\" or \" 1-23\" in \"azert 1-23-yui\".                            \n",
    "                               + '|'\n",
    "                               + r'\\b[a-z]+(-)?\\d{2,}\\b') # For instence capturing \"azert12\" in \"azert12 UI_OPq\" \n",
    "                                                      # or \"azerty1234567\" in \"azerty1234567 ui_OPq\".\n",
    "\n",
    "        # Setting search regex of keeping-prefix\n",
    "        # for instence, capturing \"umr1234\" in \"azert UMR1234 YUI_OP\" or \"fr1234\" in \"azert-fr1234 Yui_OP\".\n",
    "        prefix_template = Template(r'\\b$prefix[-]?\\d{4}\\b')\n",
    "        pattern_prefix_list = [prefix_template.substitute({\"prefix\": prefix})\n",
    "                               for prefix in KEEPING_PREFIX]   \n",
    "        re_prefix = re.compile('|'.join(pattern_prefix_list))\n",
    "        #print(re_prefix)              \n",
    "\n",
    "        flag = False\n",
    "        prefix_result = False if (re.search(re_prefix,affiliation_mod) is None) else True\n",
    "        if prefix_result and verbose: print('Keeping prefix: True')\n",
    "        zip_result = False if (re.search(re_zip,affiliation_mod) is None) else True\n",
    "        digits_result = False if (re.search(re_digits,affiliation_mod) is None) else True\n",
    "        if not prefix_result and (zip_result or digits_result):\n",
    "            if verbose:\n",
    "                print('Droping word: zip code') if zip_result else  print('Droping word: digits code')   \n",
    "            flag = True\n",
    "        return flag\n",
    "    \n",
    "    def _search_droping_town():\n",
    "        '''The `_search_droping_town` internal function searches in 'affiliation_mod' for words in lower case\n",
    "        that are french towns listed in the 'FR_UNIVERSITY_TOWNS' global.\n",
    "        \n",
    "        Args:\n",
    "            affiliation_mod (str): the string where the words are searched.\n",
    "            \n",
    "        Returns:\n",
    "            (boolean): True if a word listed in the 'FR_UNIVERSITY_TOWNS' global is equal to 'affiliation_mod' \n",
    "                       after spaces removal at ends.\n",
    "            \n",
    "        '''\n",
    "        flag = False\n",
    "        text_mod = town_names_uniformization(affiliation_mod)\n",
    "        for word_to_drop in FR_UNIVERSITY_TOWNS:\n",
    "            if word_to_drop == text_mod.strip():\n",
    "                if verbose:\n",
    "                    print('Droping word: french town')\n",
    "                flag = True\n",
    "                break\n",
    "        return flag        \n",
    "\n",
    "    funct_list = [_search_droping_words, _search_droping_bp, _search_droping_zip, _search_droping_suffix, _search_droping_town]\n",
    "    affiliation_mod  = special_symbol_remove(affiliation, only_ascii = False, strip = False).lower()\n",
    "    droping_word_found = any([funct() for funct in funct_list])\n",
    "\n",
    "    return droping_word_found\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following cell is preparing zip code capture using a specific templated regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def  _search_droping_zip_old(text):\n",
    "        '''The `_search_droping_zip` internal function searches in 'text' for words \n",
    "        similar to zip codes except those begenning with 'umr' followed by 4 digits\n",
    "        using non-case-sensitive regexes.\n",
    "        \n",
    "        Args:\n",
    "            text (str): the string where the words are searched.\n",
    "            \n",
    "        Returns:\n",
    "            (boolean): True if a word different from those begenning with 'umr' \n",
    "                       followed by 4 digits is found.\n",
    "            \n",
    "        '''\n",
    "        #To Do : CH-5232 A-1060 '1721 PW' D-48149  'CV4 7AL'\n",
    "\n",
    "        re_zip = re.compile(r'\\bb-?\\d{4}\\b'         # For instence capturing \"b-1234\" in \"azert B-1234 yui_OP\"\n",
    "                                                    # or \"b1234\" in \"azert B1234 yui_OP\".\n",
    "                            + '|'\n",
    "                            + r'\\bbe-?\\d{4}\\b'      # For instence capturing \"be-1234\" in \"azert Be-1234 yui_OP\"\n",
    "                                                    # or \"be1234\" in \"azert BE1234 yui_OP\".                            \n",
    "                            + '|'\n",
    "                            + r'\\bf-?\\d{5}\\b')      # For instence capturing \"f-12345\" in \"azert F-12345 yui_OP\"\n",
    "                                                    # or \"f12345\" in \"azert F12345 yui_OP\". \n",
    "                            \n",
    "                            \n",
    "        re_digits = re.compile(r'\\s\\d+(-\\d+)?\\b'      # For instence capturing \" 1234\" in \"azert 1234-yui_OP\"\n",
    "                                                      # or \" 1\" in \"azert 1-yui_OP\" or \" 1-23\" in \"azert 1-23-yui\".                            \n",
    "                               + '|'\n",
    "                               + r'\\b[a-z]+\\d{2,}\\b') # For instence capturing \"azert12\" in \"azert12 UI_OPq\" \n",
    "                                                      # or \"azerty1234567\" in \"azerty1234567 ui_OPq\".\n",
    "\n",
    "\n",
    "        re_umr = re.compile(r'\\bumr\\d{4}\\b'         # For instence capturing \"umr1234\" in \"azert UMR1234 YUI_OP\" \n",
    "                                                    # or \"umr1234\" in \"azert-umr1234 Yui_OP\".\n",
    "                            + '|'\n",
    "                            + r'\\bfr\\d{4}\\b')        # For instence capturing \"fr1234\" in \"azert fr1234 YUI_OP\" \n",
    "                                                     # or \"fr1234\" in \"azert-fr1234 Yui_OP\".                 \n",
    "\n",
    "        flag = False\n",
    "        umr_result = False if (re.search(re_umr,text) is None) else True\n",
    "        zip_result = False if (re.search(re_zip,text) is None) else True\n",
    "        digits_result = False if (re.search(re_digits,text) is None) else True\n",
    "        if not umr_result and (zip_result or digits_result):\n",
    "            if verbose:\n",
    "                print('Droping word: zip or digits code')\n",
    "            flag = True\n",
    "        return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals for '_search_droping_zip' internal function of 'search_droping_items' function \n",
    "\n",
    "# ' xxxx' is droped but \" xxxx\" is not droped\n",
    "ZIP_CODES = {'Algeria':            {'letters':[],            'digits': [5]},\n",
    "             'Austria':            {'letters':['A'],         'digits': [4]},\n",
    "             'Belgium':            {'letters':['B','Be'],    'digits': [4]},\n",
    "             'Bulgaria':           {'letters':[],            'digits': [4]},\n",
    "             'Brazil':             {'letters':[],            'digits': [5]},   # ' ddddd-ddd'\n",
    "             'Chile':              {'letters':[],            'digits': [6]},\n",
    "             'China':              {'letters':[],            'digits': [6]},\n",
    "             'Cuba':               {'letters':[],            'digits': [5]},\n",
    "             'Denmark':            {'letters':['DK'],        'digits': [4]},\n",
    "             'Ecuador':            {'letters':[],            'digits': [6]},\n",
    "             'Estonia':            {'letters':['EE'],        'digits': [5]},\n",
    "             'Finland':            {'letters':['FI'],        'digits': [5]},\n",
    "             'France':             {'letters':['F','FR'],    'digits': [5,6]},   # ' dd ddd'\n",
    "             'Germany':            {'letters':['D','DE'],    'digits': [5]},\n",
    "             'Greece':             {'letters':['GR'],        'digits': [5]},\n",
    "             'Hungary':            {'letters':[],            'digits': [4]},\n",
    "             'India':              {'letters':[],            'digits': [6]},\n",
    "             'Indonesia':          {'letters':[],            'digits': [5]},\n",
    "             'Israel':             {'letters':[],            'digits': [7]},\n",
    "             'Italy':              {'letters':['I'],         'digits': [5]},\n",
    "             'Japan':              {'letters':[],            'digits': [3]},   # ' ddd-dddd'\n",
    "             'Latvia':             {'letters':[],            'digits': [4]},\n",
    "             'Lebanon':            {'letters':[],            'digits': [4]},\n",
    "             'Luxembourg':         {'letters':['L'],         'digits': [4]},\n",
    "             'Mexico':             {'letters':['C.P.'],      'digits': [5]},\n",
    "             'Morocco':            {'letters':[],            'digits': [5]},\n",
    "             'Netherlands':        {'letters':[],            'digits': [4]},   # ' dddd az' | \" ddddaz\" | ' az dddd'\n",
    "             'Norway':             {'letters':['N','NO'],    'digits': [4]},\n",
    "             'Pakistan':           {'letters':[],            'digits': [5]},\n",
    "             'Poland':             {'letters':[],            'digits': [2]},   # ' dd-ddd'\n",
    "             'Portugal':           {'letters':['P'],         'digits': [4,7]},   # ' dddd-ddd'\n",
    "             'Romania':            {'letters':[],            'digits': [6]},\n",
    "             'Russian Federation': {'letters':[],            'digits': [6]},\n",
    "             'Singapore':          {'letters':['Singapore'], 'digits': [6]},\n",
    "             'Slovenia':           {'letters':['Sl','SI'],   'digits': [4]},\n",
    "             'Spain':              {'letters':['E'],         'digits': [5]},\n",
    "             'Sri Lanka':          {'letters':[],            'digits': [5]},\n",
    "             'Sweden':             {'letters':['SE','S'],        'digits': [5]},\n",
    "             'Switzerland':        {'letters':['CH'],        'digits': [4]},\n",
    "             'Thailand':           {'letters':[],            'digits': [5]},\n",
    "             'Togo':               {'letters':[],            'digits': [5]},\n",
    "             'Tunisia':            {'letters':[],            'digits': [4]},\n",
    "             'Turkey':             {'letters':[],            'digits': [5]},\n",
    "             'Viet Nam':           {'letters':[],            'digits': [5,6]},\n",
    "             }\n",
    "for country in ZIP_CODES.keys(): ZIP_CODES[country]['letters'] = [x.replace(\".\", r\"\\.\").lower() for x in ZIP_CODES[country]['letters']]\n",
    "\n",
    "\n",
    "_KEEPING_PREFIX = ['UMR','ULR','FR']\n",
    "KEEPING_PREFIX = [x.lower() for x in _KEEPING_PREFIX]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path_home = Path.home()\n",
    "\n",
    "df = pd.DataFrame.from_dict(ZIP_CODES, orient = 'columns').T\n",
    "\n",
    "file = path_home / Path('Temp/ZipCodes.xlsx')\n",
    "df.to_excel(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path_home = Path.home()\n",
    "\n",
    "file = path_home / Path('Temp/ZipCodes.xlsx')\n",
    "df = pd.read_excel(file)\n",
    "ZIP_CODES = df\n",
    "#json.loads('[1, 2, 3]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing regex for '_search_droping_zip' internal function of 'search_droping_items' function  \n",
    "\n",
    "import re\n",
    "from string import Template\n",
    "\n",
    "country = 'Mexico'\n",
    "text = \" C.P.38000\".lower()\n",
    "\n",
    "zip_template = Template(r'\\b($zip_letters)[\\s-]?(\\d{$zip_digits})\\b')\n",
    "\n",
    "#letters_list, digits_list = ZIP_CODES[country]['letters'], ZIP_CODES[country]['digits']\n",
    "letters_list = ['c\\\\.p\\\\.']\n",
    "#letters_list = ['cp']\n",
    "digits_list = [5]\n",
    "letters_join = '|'.join(letters_list) if len(letters_list) else ''\n",
    "pattern_zip_list = [zip_template.substitute({\"zip_letters\": letters_join, \"zip_digits\":digits})\n",
    "                    for digits in digits_list]     \n",
    "re_zip = re.compile('|'.join(pattern_zip_list))\n",
    "print('re_zip:',re_zip)\n",
    "\n",
    "zip_result = False if (re.search(re_zip,text) is None) else True\n",
    "print('zip_result:',zip_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing '_search_droping_zip' internal function of 'search_droping_items' function  \n",
    "\n",
    "def _search_droping_zip_test(text, country):\n",
    "    '''The `_search_droping_zip` internal function searches in 'text' for words \n",
    "    similar to zip codes except those begenning with a prefix from the 'KEEPING_PREFIX' global\n",
    "    followed by 4 digits using case-sensitive regexes. \n",
    "    Regex for zip-codes search uses the 'ZIP_CODES' dict global for countries from 'ZIP_CODES.keys()'.\n",
    "    Specific regex are set for ''\n",
    "\n",
    "    Args:\n",
    "        text (str): the string where the words are searched.\n",
    "        country (str): the string that contains the country.\n",
    "\n",
    "    Returns:\n",
    "        (boolean): True if a word different from those begenning with 'umr' \n",
    "                   followed by 4 digits is found.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Setting regex for zip-codes search\n",
    "    if country in ZIP_CODES.keys():\n",
    "        print(country)\n",
    "        zip_template = Template(r'\\b($zip_letters)[\\s-]?(\\d{$zip_digits})\\b')\n",
    "        letters_list, digits_list = ZIP_CODES[country]['letters'], ZIP_CODES[country]['digits']\n",
    "        letters_join = '|'.join(letters_list) if len(letters_list) else ''\n",
    "        pattern_zip_list = [zip_template.substitute({\"zip_letters\": letters_join, \"zip_digits\":digits})\n",
    "                            for digits in digits_list]     \n",
    "        re_zip = re.compile('|'.join(pattern_zip_list))\n",
    "        \n",
    "    elif country == 'United Kingdom':\n",
    "        print('United Kingdom')\n",
    "        # Capturing: for instence, \" BT7 1NN\" or \" WC1E 6BT\" or \" G128QQ\"\n",
    "        #            \" a# #a\", \" a# #az\", \" a# ##a\", \" a# ##az\",\n",
    "        #            \" a##a\", \" a##az\", \" a###a\", \" a###az\",\n",
    "        #\n",
    "        #            \" a#a #a\", \" a#a #az\", \" a#a ##a\", \" a#a ##az\",\n",
    "        #            \" a#a#a\", \" a#a#az\", \" a#a##a\", \" a#a##az\",\n",
    "        #\n",
    "        #            \" a## #a\", \" a## #az\", \" a## ##a\", \" a## ##az\",\n",
    "        #            \" a###a\", \" a###az\", \" a####a\", \" a####az\",\n",
    "        #            \n",
    "        #            \" a##a #a\", \" a##a #az\", \" a##a ##a\", \" a##a ##az\",\n",
    "        #            \" a##a#a\", \" a##a#az\", \" a##a##a\", \" a##a##az\",\n",
    "        #            \n",
    "        #            \" az# #a\", \" az# #az\", \" az# ##a\", \" az# ##az\",\n",
    "        #            \" az##a\", \" az##az\", \" az###a\", \" az###az\",\n",
    "        #\n",
    "        #            \" az#a #a\", \" az#a #az\", \" az#a ##a\", \" az#a ##az\",\n",
    "        #            \" az#a#a\", \" az#a#az\", \" az#a##a\", \" az#a##az\",\n",
    "        #\n",
    "        #            \" az## #a\", \" az## #az\", \" az## ##a\", \" az## ##az\",\n",
    "        #            \" az###a\", \" az###az\", \" az###a\", \" az####az\",\n",
    "        #\n",
    "        #            \" az##a #a\", \" az##a #az\", \" az##a ##a\", \" az##a ##az\",\n",
    "        #            \" az##a#a\", \" az##a#az\", \" az##a#a\", \" az##a##az\",\n",
    "        \n",
    "        re_zip = re.compile(r'^\\s?[a-z]{1,2}\\d{1,2}[a-z]{0,1}\\s?\\d{1,2}[a-z]{1,2}$')\n",
    "        \n",
    "    elif country == 'United States':\n",
    "        print('United States')\n",
    "        # Capturing: for instence, \" NY\" or ' NI BT48 0SG' or \" ON K1N 6N5\" \n",
    "        #            \" az\" or \" az \" + 6 or 7 characters in 2 parts separated by spaces\n",
    "        \n",
    "        re_zip = re.compile(r'^\\s?[a-z]{2}$' + '|' + r'^\\s?[a-z]{2}\\s[a-z0-9]{3,4}\\s[a-z0-9]{2,3}$')\n",
    "        \n",
    "    #print(re_zip)\n",
    "    \n",
    "    # Setting search regex of embedding digits\n",
    "    re_digits = re.compile(r'\\s\\d+(-\\d+)?\\b'      # For instence capturing \" 1234\" in \"azert 1234-yui_OP\"\n",
    "                                                  # or \" 1\" in \"azert 1-yui_OP\" or \" 1-23\" in \"azert 1-23-yui\".                            \n",
    "                           + '|'\n",
    "                           + r'\\b[a-z]+\\d{2,}\\b') # For instence capturing \"azert12\" in \"azert12 UI_OPq\" \n",
    "                                                  # or \"azerty1234567\" in \"azerty1234567 ui_OPq\".\n",
    "\n",
    "    # Setting search regex of keeping-prefix\n",
    "    # for instence, capturing \"umr1234\" in \"azert UMR1234 YUI_OP\" or \"fr1234\" in \"azert-fr1234 Yui_OP\".\n",
    "    prefix_template = Template(r'\\b$prefix[-]?\\d{4}\\b')\n",
    "    pattern_prefix_list = [prefix_template.substitute({\"prefix\": prefix})\n",
    "                           for prefix in KEEPING_PREFIX]   \n",
    "    re_prefix = re.compile('|'.join(pattern_prefix_list))\n",
    "    #print(re_prefix)              \n",
    "\n",
    "    flag = False\n",
    "    prefix_result = False if (re.search(re_prefix,text) is None) else True\n",
    "    if prefix_result and verbose: print('Keeping prefix: True')\n",
    "    zip_result = False if (re.search(re_zip,text) is None) else True\n",
    "    digits_result = False if (re.search(re_digits,text) is None) else True\n",
    "    if not prefix_result and (zip_result or digits_result):\n",
    "        if verbose:\n",
    "            print('Droping word: zip code') if zip_result else  print('Droping word: digits code')   \n",
    "        flag = True\n",
    "    return flag\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "# Standard library import\n",
    "import re\n",
    "from string import Template\n",
    "\n",
    "# Local imports\n",
    "#from BiblioAnalysis_Utils.BiblioSpecificGlobals import KEEPING_PREFIX\n",
    "#from BiblioAnalysis_Utils.BiblioSpecificGlobals import ZIP_CODES\n",
    "\n",
    "verbose = True\n",
    "\n",
    "country = 'France'\n",
    "\n",
    "text = ' FR-38900'.lower()\n",
    "\n",
    "print('Droping zip result:', _search_droping_zip_test(text, country))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For testing the affiliations list extraction on specific addresses\n",
    "\n",
    "# \" Facultad de Ciencias Químicas, University Autónoma de Chihuahua, \\\n",
    "#Campus Universitario #2, Circuito Universitario, Chih, Chihuahua, C.P.31125, Mexico\"\n",
    "\n",
    "# \" Laboratory of Multifunctional Materials and Structures, \\\n",
    "#National Institute of Materials Physics, Atomistilor Str. 405A, Magurele, 077125, Romania\"\n",
    "\n",
    "#\" STMicroelectronics (Crolles 2) SAS\"\n",
    "\n",
    "#\" University Savoie Mont Blanc, LOCIE UMR CNRS/USMB 5271, FédESol FR3344, Bâtiment Hélios, \\\n",
    "#Avenue du Lac Léman, Le Bourget-du-LacF-73376, France\"\n",
    "\n",
    "#\" University Autónoma de Chihuahua, Campus Universitario #2, Mexico\"\n",
    "\n",
    "#\" SOLIDpower S.p.A, Mezzolombardo38017, Italy\"\n",
    "\n",
    "#\" STMicroelect Crolles 2 SAS, 850 Rue Jean Monnet, F-38926 Crolles, France\"\n",
    "\n",
    "#\" Faculty of Electrical Engineering, University of Ljubljana, Ljubljana, SI-1000, Slovenia\"\n",
    "\n",
    "\n",
    "raw_address = \" Faculty of Electrical Engineering, University of Ljubljana, Ljubljana, SI-1000, Slovenia\"\n",
    "std_address = address_standardization(raw_address)\n",
    "(affiliations_list,country,affiliation_drop) = get_affiliations_list(std_address, verbose = True)\n",
    "print('country:',country)\n",
    "print('affiliations_list:',affiliations_list)\n",
    "print('affiliation_drop:',affiliation_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet to check the robustness of the affiliations list extraction\n",
    "# from all addresses of a wos or scopus corpus\n",
    "\n",
    "# Standard library import\n",
    "from pathlib import Path\n",
    "\n",
    "# 3rd party imports\n",
    "import pandas as pd\n",
    "\n",
    "# Internal imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "find_address = lambda  affiliation : bau.RE_ADDRESS.findall(affiliation)\n",
    "\n",
    "for type_corpus in [\"wos\",\"scopus\"]:\n",
    "    for year in ['2018','2019','2020','2021']:\n",
    "\n",
    "        path_home = Path.home()\n",
    "\n",
    "        # read the 'Affiliations' columns of the df corpus with one row per address\n",
    "        if type_corpus == \"wos\":\n",
    "            file = path_home / Path('BiblioMeter_Files/' + year + '/Corpus/wos/rawdata/savedrecs.txt')\n",
    "            df_corpus = bau.read_database_wos(file)\n",
    "            df_affiliations_raw = df_corpus['C1'].apply(find_address).explode()\n",
    "        elif type_corpus == \"scopus\":\n",
    "            file = path_home / Path('BiblioMeter_Files/' + year + '/Corpus/scopus/rawdata/scopus.csv')\n",
    "            df_corpus = bau.read_database_scopus(file)\n",
    "            df_affiliations_raw = df_corpus['Affiliations'].apply(lambda x: x.split(';')).explode()\n",
    "        else:\n",
    "            raise Exception(f\"unknown corpus type :{type_corpus}. Must be wos or scopus\")\n",
    "\n",
    "        df_affiliations_std = df_affiliations_raw.apply(address_standardization)\n",
    "        df_affiliations_list = df_affiliations_std.apply(get_affiliations_list)\n",
    "\n",
    "        df_affiliations = pd.concat([df_affiliations_raw,df_affiliations_std,df_affiliations_list], axis = 1)\n",
    "        df_affiliations.columns = ['Raw address','Standard address', 'Affiliations tuple']\n",
    "\n",
    "        df_affiliations[['Affiliations', 'Country', 'Affiliation_drop']] = pd.DataFrame(df_affiliations['Affiliations tuple'].to_list(), \\\n",
    "                                                                                        index=df_affiliations.index)\n",
    "\n",
    "        # Save the results as csv files\n",
    "        file = path_home / Path('Temp/affiliations_' + type_corpus + '_' + year + '.xlsx')\n",
    "        df_affiliations.to_excel(file,index=False)\n",
    "        df_affiliations\n",
    "\n",
    "# Save the results as csv files\n",
    "#file_raw = r'c:\\Temp\\affiliations_raw.csv'\n",
    "#df_affiliations_raw.to_csv(file_raw,index=False)\n",
    "#file_uniform = r'c:\\Temp\\affiliations_uniform.csv'\n",
    "#df_affiliations_std.to_csv(file_uniform,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following cells to be deeply updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Testing \"address_inst_full_list\" function for \"_build_authors_countries_institutions_scopus\" function\n",
    "\n",
    "# Standard library imports\n",
    "import itertools\n",
    "import re\n",
    "from colorama import Fore\n",
    "from collections import namedtuple\n",
    "from string import Template\n",
    "\n",
    "# 3rd party imports\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Local imports\n",
    "# from BiblioAnalysis_Utils.BiblioParsingInstitutions import affiliation_uniformization            !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# from BiblioAnalysis_Utils.BiblioParsingInstitutions import address_inst_full_list                !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n",
    "from BiblioAnalysis_Utils.BiblioParsingInstitutions import build_institutions_dic\n",
    "from BiblioAnalysis_Utils.BiblioParsingUtils import country_normalization\n",
    "\n",
    "# Globals, namedtuples and templates  used in \"_build_authors_countries_institutions_scopus\" function\n",
    "\n",
    "rep_utils = Path(bau.REP_UTILS)\n",
    "\n",
    "author_address_tup = namedtuple('author_address','author address')\n",
    "    \n",
    "template_inst = Template('[$symbol1]?($inst)[$symbol2].*($country)(?:$$|;)')\n",
    "\n",
    "addr_country_inst = namedtuple('address',['Pub_id',\n",
    "                                 'Idx_author',\n",
    "                                 'Address',\n",
    "                                 'Country',\n",
    "                                 'Norm_institutions',\n",
    "                                 'Raw_institutions',])\n",
    "\n",
    "author_address_tup = namedtuple('author_address','author address')\n",
    "\n",
    "# End of globals, namedtuples and templates  used in \"_build_authors_countries_institutions_scopus\" function\n",
    "\n",
    "# scopus\n",
    "pub_id = 6 # 1, 3, 6, 7, 46, 55, 164, 177\n",
    "\n",
    "affiliations = affiliations_dic[pub_id]\n",
    "authors_affiliations = authors_affiliations_dic[pub_id]\n",
    "\n",
    "# Initializations for test\n",
    "inst_dic = build_institutions_dic(rep_utils, dic_inst_filename = None)\n",
    "\n",
    "# Part of \"_build_authors_countries_institutions_scopus\" function\n",
    "list_addr_country_inst = [] \n",
    "\n",
    "idx_author, last_author = -1, '' # Initialization for the author and address counter\n",
    "\n",
    "list_affiliations = affiliations.split(';')\n",
    "list_authors_affiliations = authors_affiliations.split(';')\n",
    "\n",
    "for x in list_authors_affiliations:\n",
    "    author = (','.join(x.split(',')[0:2])).strip()\n",
    "    if last_author != author:\n",
    "        idx_author += 1\n",
    "    last_author = author\n",
    "    \n",
    "    author_list_addresses = ','.join(x.split(',')[2:])\n",
    "    author_address_list_raw = []\n",
    "    for affiliation_raw in list_affiliations:\n",
    "        if affiliation_raw in author_list_addresses:\n",
    "            affiliation = affiliation_uniformization_test(affiliation_raw)\n",
    "            author_address_list_raw.append(affiliation)\n",
    "\n",
    "        for address in author_address_list_raw:\n",
    "            print('address',address)\n",
    "            author_country_raw = address.split(',')[-1].strip()\n",
    "            author_country = country_normalization(author_country_raw)\n",
    "\n",
    "            author_institutions_tup = address_inst_full_list_test(address, inst_dic)               # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            list_addr_country_inst.append(addr_country_inst(pub_id,\n",
    "                                                            idx_author,\n",
    "                                                            address,\n",
    "                                                            author_country,\n",
    "                                                            author_institutions_tup.norm_inst_list,\n",
    "                                                            author_institutions_tup.raw_inst_list,))\n",
    "# End of part of \"_build_authors_countries_institutions_scopus\" function\n",
    "\n",
    "for tup in list_addr_country_inst:\n",
    "    print(tup.Pub_id)\n",
    "    print(tup.Idx_author)\n",
    "    print(tup.Address)\n",
    "    print(tup.Country)\n",
    "    print(tup.Norm_institutions)\n",
    "    print(tup.Raw_institutions)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For changing particularly encoded symbols\n",
    "DIC_CHANGE_SYMB_test = {\"&\": \"and\",\n",
    "                        \"’\": \"'\",   # Particular cote to standard cote\n",
    "                        #\".\": \"\",\n",
    "                        #\"-\": \" \",   # To Do: to be tested from the point of view of the effect on raw institutions\n",
    "                        #\"§\": \" \",\n",
    "                        #\"(\": \" \",\n",
    "                        #\")\": \" \",\n",
    "                        #\"/\": \" \",\n",
    "                        #\"@\": \" \"\n",
    "                       } \n",
    "\n",
    "SYMB_CHANGE_test = str.maketrans(DIC_CHANGE_SYMB_test)\n",
    "\n",
    "DIC_AMB_WORDS_test = {' des ': ' ', # Conflict with DES institution\n",
    "                 ' @ ': ' ', # Management conflict with '@' between texts\n",
    "                }\n",
    "\n",
    "def affiliation_uniformization_test(affiliation_raw):    # En refonte\n",
    "    \n",
    "    '''The `affiliation_uniformization' function aims at getting rid \n",
    "    of heterogeneous typing of affilations. \n",
    "    It first replaces particular characters by standard ones \n",
    "    using 'DASHES_CHANGE' and 'SYMB_CHANGE' globals.\n",
    "    Then, it substitutes by 'University' its aliases using specific \n",
    "    regular expressions set in 'RE_SUB',and 'RE_SUB_FIRST' globals.\n",
    "    Finally, it removes accents using `special_symbol_remove` function.\n",
    "    \n",
    "    Args:\n",
    "        affiliation_raw (str): the raw affiliation to be normalized.\n",
    "\n",
    "    Returns:\n",
    "        (str): the normalized affiliation.\n",
    "        \n",
    "    Notes:\n",
    "        The globals 'DASHES_CHANGE' and 'SYMB_CHANGE' are imported\n",
    "        from `BiblioGeneralGlobals` module of `BiblioAnalysis_Utils` package.\n",
    "        The globals 'RE_SUB',and 'RE_SUB_FIRST' are imported\n",
    "        from `BiblioSpecificGlobals` module of `BiblioAnalysis_Utils` package.\n",
    "        #The function `special_symbol_remove` is used from `BiblioParsingUtils` of `BiblioAnalysis_utils` package.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Standard library imports\n",
    "    import re\n",
    "    \n",
    "    # Local imports\n",
    "    from BiblioAnalysis_Utils.BiblioParsingUtils import special_symbol_remove\n",
    "    from BiblioAnalysis_Utils.BiblioGeneralGlobals import DASHES_CHANGE\n",
    "    from BiblioAnalysis_Utils.BiblioGeneralGlobals import SYMB_CHANGE\n",
    "    from BiblioAnalysis_Utils.BiblioSpecificGlobals import DIC_AMB_WORDS\n",
    "\n",
    "    \n",
    "    def _normalize_amb_words(text):  \n",
    "        for amb_word in DIC_AMB_WORDS_test.keys():                                       #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            text = text.replace(amb_word, DIC_AMB_WORDS_test[amb_word]).strip()          #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        text = \" \".join(text.split())\n",
    "        return text\n",
    "    \n",
    "    affiliation_raw = _normalize_amb_words(affiliation_raw)\n",
    "    affiliation_raw = affiliation_raw.translate(DASHES_CHANGE)\n",
    "    affiliation_raw = affiliation_raw.translate(SYMB_CHANGE_test)                       #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    affiliation_uniform = special_symbol_remove(affiliation_raw, only_ascii=True, skip=True )  #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    #affiliation_uniform = standard_words_uniformization(affiliation_uniform)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return affiliation_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def address_inst_full_list_test(full_address, inst_dic):    # En refonte\n",
    "\n",
    "    '''The `address_inst_full_list` function allows building the affiliations list of a full address\n",
    "    using the internal function `_check_institute`of `BiblioParsingUtils` module\n",
    "    \n",
    "    Args:\n",
    "       full_address (str): the full address to be parsed in institutions and country.\n",
    "       inst_dic (dict): a dict used for the normalization of the institutions names, \n",
    "                        with the raw names as keys and the normalized names as values.\n",
    "        \n",
    "    Returns:\n",
    "        (namedtuple): tuple of two strings. \n",
    "                      - The first is the joined list of normalized institutions names \n",
    "                      found in the full address.\n",
    "                      - The second is the joined list of raw institutions names of the full address \n",
    "                      with no fully corresponding normalized names.\n",
    "        \n",
    "    Notes:\n",
    "        The globals 'RE_ZIP_CODE' and 'EMPTY' are imported from `BiblioSpecificGlobals` module \n",
    "        of `BiblioAnalysis_Utils` package.\n",
    "        The function `country_normalization` is imported from `BiblioParsingUtils` module\n",
    "        of `BiblioAnalysis_utils` package.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Standard library imports\n",
    "    import re\n",
    "    from collections import namedtuple\n",
    "    from string import Template\n",
    "\n",
    "    # 3rd party imports\n",
    "    import pandas as pd\n",
    "    from fuzzywuzzy import process\n",
    "    \n",
    "    # Local imports\n",
    "    from BiblioAnalysis_Utils.BiblioParsingUtils import country_normalization\n",
    "    from BiblioAnalysis_Utils.BiblioSpecificGlobals import INST_BASE_LIST\n",
    "    from BiblioAnalysis_Utils.BiblioSpecificGlobals import RE_ZIP_CODE\n",
    "    from BiblioAnalysis_Utils.BiblioSpecificGlobals import EMPTY\n",
    "\n",
    "    inst_full_list_ntup = namedtuple('inst_full_list_ntup',['norm_inst_list','raw_inst_list'])\n",
    "    \n",
    "    country_raw = full_address.split(\",\")[-1].strip()\n",
    "    country = country_normalization(country_raw)\n",
    "    add_country = \"_\" + country\n",
    "    \n",
    "    if RE_ZIP_CODE.findall(full_address):\n",
    "        address_to_keep = re.sub(RE_ZIP_CODE,\"\",full_address) + \",\"\n",
    "    else:\n",
    "        address_to_keep = \", \".join(full_address.split(\",\")[:-1])    \n",
    "    address_to_keep = address_to_keep.lower()\n",
    "    print('address_to_keep:',address_to_keep)\n",
    "    \n",
    "    # Building the list of normalized institutions which raw institutions are found in 'address_to_keep' \n",
    "    # and building the corresponding list of raw institutions found in 'address_to_keep'\n",
    "    norm_inst_full_list = [] \n",
    "    raw_inst_found_list = []\n",
    "    for raw_inst, norm_inst in inst_dic.items():        \n",
    "        raw_inst_lower = raw_inst.lower()\n",
    "        raw_inst_split = raw_inst_lower.split()                     \n",
    "        if _check_institute_test(address_to_keep,raw_inst_split):          #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  \n",
    "            norm_inst_full_list.append(norm_inst + add_country)            \n",
    "            raw_inst_found_list.append(raw_inst_lower)\n",
    "\n",
    "    # Cleaning 'raw_inst_found_list' from partial institution names\n",
    "    for inst_base in INST_BASE_LIST:\n",
    "        if (inst_base.lower() in raw_inst_found_list) and (inst_base.lower()+\",\" not in address_to_keep) : \n",
    "            raw_inst_found_list.remove(inst_base.lower())\n",
    "    for raw_inst_found in raw_inst_found_list:\n",
    "        other_raw_inst_found_list = raw_inst_found_list.copy()\n",
    "        other_raw_inst_found_list.remove(raw_inst_found)        \n",
    "        for other_raw_inst_found in other_raw_inst_found_list:\n",
    "            if raw_inst_found in other_raw_inst_found:\n",
    "                raw_inst_found_list = other_raw_inst_found_list\n",
    "\n",
    "    # Removing 'raw_inst_found_list' items from 'address_to_keep'             \n",
    "    for raw_inst_found in raw_inst_found_list:            \n",
    "        if raw_inst_found in address_to_keep: \n",
    "            address_to_keep = address_to_keep.replace(raw_inst_found,\"\")  \n",
    "            address_to_keep = \", \".join(x.strip() for x in address_to_keep.split(\",\"))\n",
    "\n",
    "    while (address_to_keep and address_to_keep[0] == \"-\"): address_to_keep = address_to_keep[1:]\n",
    "    address_to_keep = address_to_keep.replace(\"-\", \" \")\n",
    "    address_to_keep = \" \".join(x.strip() for x in address_to_keep.split(\" \") if x!=\"\")\n",
    "    print('address_to_keep:',address_to_keep)        \n",
    "            \n",
    "    # # Building the list of raw institutions remaning in 'address_to_keep'\n",
    "    raw_inst_full_list = [x.strip() + add_country for x in address_to_keep.split(\",\") if (x!=\"\" and x!=\" \")]\n",
    "\n",
    "    # Building a string from the final list of raw institutions  \n",
    "    if raw_inst_full_list:\n",
    "        raw_inst_full_list_str = \";\".join(raw_inst_full_list)       \n",
    "    else:\n",
    "        raw_inst_full_list_str = EMPTY \n",
    "    print('raw_inst_full_list_str:',raw_inst_full_list_str)\n",
    "    \n",
    "    # Building a string from the final list of normalized institutions without duplicates\n",
    "    norm_inst_full_list = list(set(norm_inst_full_list))\n",
    "    if norm_inst_full_list:\n",
    "        norm_inst_full_list_str = \";\".join(norm_inst_full_list)\n",
    "    else:\n",
    "        norm_inst_full_list_str = EMPTY \n",
    "    print('norm_inst_full_list_str:', norm_inst_full_list_str)\n",
    "    print()\n",
    "    \n",
    "    # Setting the namedtuple to return\n",
    "    inst_full_list_tup =  inst_full_list_ntup(norm_inst_full_list_str,raw_inst_full_list_str) \n",
    "    \n",
    "    return inst_full_list_tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_institute_test(address,raw_inst_split):    # En refonte\n",
    "\n",
    "    '''The funstion `_check_institute` checks if all the words contained in the list 'raw_inst_split'\n",
    "    are part of the string 'address'.\n",
    "    \n",
    "    A word is defined as a string beginning and ending with a letter.\n",
    "    For instance, 'cea-leti' or 'Laue-Langevin' are words but not 'Kern-' or '&aaZ)'.\n",
    "    \n",
    "    The regexp used is based on the following rules:\n",
    "        - Alphanumerical (AM) characters are {a…z,A…Z,0…9,_}.\n",
    "        - Non-alphanumerical (NAM) characters are all other characters.\n",
    "        - '\\b' detects transition between NAM and AM such as '@a', '<space>a', 'a-', '(a', 'a.', etc.\n",
    "        - '\\B' detects transition between AM and AM such as '1a', 'za', 'a_', '_a', etc.\n",
    "    \n",
    "    Matches are found between a word 'WORD' of the list `raw_inst_split` and a substring 'WORDS'\n",
    "    of the string 'address' in 4 cases:\n",
    "\n",
    "        - 'WORD' matches in '...dWORD' or '...dWORDd...' by the regexp \"'\\d+\\BWORD\\B\\d+'\". \n",
    "        ex: 'UMR' matches in 'a6UMR7040.' '@6UMR' or 'matches in '...*WORDd*...'', etc. \n",
    "                  doesn't match in 'aUMR', '6UMR-CNRS', '6@UMR7040', etc.\n",
    "\n",
    "        - 'WORD' matches '...dWORD*...'  by the regexp \"'\\d+\\BWORD\\b'\".  \n",
    "        ex: 'UMR' matches in 'a6UMR-CNRS', etc.\n",
    "                  doesn't match in '@6UMRCNRS', '@UMR-CNRS', etc.\n",
    "\n",
    "        - 'WORD' matches in '...*WORD*...' by the regexp \"'\\bWORD\\b'\". \n",
    "        ex: 'UMR' matches in '(UMR7040)', '@UMR-CNRS', etc. where 'UMR'is in between NAM\n",
    "                  doesn't match in 'UMR7040', '@6UMR_CNRS', etc, where an NAM at least misses around it.\n",
    "\n",
    "        - 'WORD' matches in '...*WORDd...', by the regexp \"'\\bWORD\\B\\d+'\"\n",
    "        ex UMR = %(UMR43)+#\n",
    "        ex: 'UMR' matches in '6@UMR7040', 'CNRS-UMR7040', etc.\n",
    "                  doesn't match in 'CNRS_UMR7040', '#UMR_CNRS', etc.\n",
    "\n",
    "    where '...' stands for any characters, 'd' stands for a digit and '*' stands for an NAM character. \n",
    "    \n",
    "    The match is case insensitive.\n",
    "    \n",
    "    According the mentionned 4 rules an isolated '&' such as in 'Art & Metiers' \n",
    "    and words ending by a minus (ex: Kern-) are not catched. \n",
    "    A specific pretreatment of `raw_inst_split` and `address` should be done before calling this function.\n",
    "    \n",
    "    Examples:\n",
    "        - _check_institute(['den'],'dept. of energy conversion, university of denmark,') is False.\n",
    "        - _check_institute(['den','dept'],'DEN dept. of energy conversion, university of denmark,') is True.\n",
    "    \n",
    "    Args:\n",
    "        address (str): the address where to check the matching of words.\n",
    "        raw_inst_split (list): list of words to be found to match in the string 'address'.\n",
    "    \n",
    "    Returns:\n",
    "        (boolean): 'True' for a full match.\n",
    "                   'False' otherwise.\n",
    "    '''\n",
    "    # Standard library imports\n",
    "    import re\n",
    "    from string import Template\n",
    "    \n",
    "    raw_inst_split = list(set(raw_inst_split))\n",
    "    \n",
    "    # Taking care of the potential isolated special characters '&' and '-'   \n",
    "    raw_inst_split = [x.replace(\"&\",\"and\") for x in raw_inst_split]    \n",
    "    raw_inst_split_init = raw_inst_split.copy()\n",
    "\n",
    "    # Removing small words\n",
    "    small_words_list = ['a','et','de','and','for','of','the']\n",
    "    for word in small_words_list:\n",
    "        if word in raw_inst_split_init:\n",
    "            raw_inst_split.remove(word)\n",
    "    \n",
    "    raw_inst_split_init = raw_inst_split.copy()\n",
    "    for word in raw_inst_split_init:\n",
    "        if len(word)==1:\n",
    "            raw_inst_split.remove(word)\n",
    "    \n",
    "    # Adding \\ to escape regexp reserved char\n",
    "    for char in [\"$\",'(',')','[',']','^','-']: \n",
    "        escaped_char = '\\\\'+ char\n",
    "        raw_inst_split = [x.replace(char,escaped_char) for x in raw_inst_split]\n",
    "        \n",
    "    items_number = len(raw_inst_split)\n",
    "\n",
    "    # Building the 're_inst' regexp searching for 'raw_inst_split' items in 'address' \n",
    "    dic = {\"inst\"+str(i):inst for i,inst in enumerate(raw_inst_split)}\n",
    "\n",
    "    template_inst = Template(r'|'.join([r'\\d+\\B$inst'+ str(i) + r'\\B\\d+'\n",
    "                                      + '|'\n",
    "                                      + r'\\d+\\B$inst' + str(i) + r'\\b'\n",
    "                                      + '|'\n",
    "                                      + r'\\b$inst' + str(i) + r'\\b'\n",
    "                                      + '|'\n",
    "                                      + r'\\b$inst' + str(i) + r'\\B\\d+'\n",
    "                                      for i in range(items_number)]))\n",
    "\n",
    "    re_inst = re.compile(template_inst.substitute(dic), re.IGNORECASE)\n",
    "\n",
    "    # Checking mach of 'raw_inst_split' items in 'address' using 're_inst' regexp\n",
    "    items_set = set(re_inst.findall(address))\n",
    "    if  len(items_set) == items_number:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I- Merging corpuses from different databases for single year corpus analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "# Setting global aliases\n",
    "concat_folder_alias = bau.FOLDER_NAMES['concat']\n",
    "corpus_folder_alias = bau.FOLDER_NAMES['corpus']\n",
    "rational_folder_alias = bau.FOLDER_NAMES['dedup']\n",
    "parsing_folder_alias = bau.FOLDER_NAMES['parsing']\n",
    "rawdata_folder_alias = bau.FOLDER_NAMES['rawdata']\n",
    "scopus_folder_alias = bau.FOLDER_NAMES['scopus']\n",
    "wos_folder_alias = bau.FOLDER_NAMES['wos']\n",
    "\n",
    "# Setting global regular expressions\n",
    "re_year = bau.RE_YEAR\n",
    "\n",
    "# Setting default values of inputs\n",
    "ref_files_check ='y' \n",
    "\n",
    "# Selecting corpus folder (year)\n",
    "corpusfiles_list =[file for file in os.listdir(corpuses_folder) if re_year.findall(file)]\n",
    "corpusfiles_list.sort()\n",
    "print('Please select the corpus via the tk window')\n",
    "myprojectname = bau.Select_multi_items(corpusfiles_list,'single',2)[0]+'/'\n",
    "project_folder = corpuses_folder / Path(myprojectname) / Path(corpus_folder_alias) \n",
    "print(bold_text + f'\\nThe corpus folder selected is: {project_folder}' + light_text)\n",
    "\n",
    "# Setting the useful paths\n",
    "path_scopus_parsing = project_folder / Path(scopus_folder_alias) / Path(parsing_folder_alias)\n",
    "path_scopus_rawdata = project_folder / Path(scopus_folder_alias) / Path(rawdata_folder_alias) \n",
    "path_wos_parsing = project_folder / Path(wos_folder_alias) / Path(parsing_folder_alias)\n",
    "path_wos_rawdata = project_folder / Path(wos_folder_alias) / Path(rawdata_folder_alias) \n",
    "path_concat = project_folder / Path(concat_folder_alias)\n",
    "path_concat_parsing = path_concat / Path(parsing_folder_alias)\n",
    "if not os.path.exists(path_concat_parsing):\n",
    "    if not os.path.exists(path_concat): os.mkdir(path_concat)\n",
    "    os.mkdir(path_concat_parsing)\n",
    "path_rational = project_folder / Path(rational_folder_alias)\n",
    "path_rational_parsing = path_rational / Path(parsing_folder_alias)\n",
    "if not os.path.exists(path_rational_parsing):\n",
    "    if not os.path.exists(path_rational): os.mkdir(path_rational)\n",
    "    os.mkdir(path_rational_parsing)\n",
    "\n",
    "# Setting the useful-paths lists     \n",
    "database_list = [(bau.WOS, path_wos_parsing, path_wos_rawdata), (bau.SCOPUS, path_scopus_parsing, path_scopus_rawdata)]\n",
    "useful_path_list = [path_scopus_parsing,path_wos_parsing,path_concat_parsing,path_rational_parsing] \n",
    "\n",
    "# Checking availibility of parsing for each of the corpuses to be merged\n",
    "parsing_files_check = input('\\n Parsings available for each corpus (y/n, default: n) ?')\n",
    "if parsing_files_check == 'n' or parsing_files_check == '':\n",
    "    # Get the folder for the general files\n",
    "    # and specific files for scopus type database in this folder\n",
    "    rep_package = Path('BiblioAnalysis_Utils')\n",
    "    rep_utils = Path(bau.REP_UTILS) \n",
    "    actual_folder = Path.cwd()\n",
    "    print('\\nWorking directory: ', actual_folder)\n",
    "    print('Default folder for the reference files: ', actual_folder / rep_package / rep_utils)  \n",
    "    \n",
    "    ref_files_check = input(' Are all reference files in this folder (y/n, default: y) ?')\n",
    "    if ref_files_check == 'y' or ref_files_check == '':\n",
    "        for database_tup in database_list:\n",
    "            database_type = database_tup[0]\n",
    "            database_parsing_path = database_tup[1]\n",
    "            database_rawdata_path = database_tup[2]\n",
    "                # Folder containing the wos or scopus file to process\n",
    "            in_dir_parsing = database_rawdata_path\n",
    "\n",
    "                # Folder containing the output files of the data parsing \n",
    "            out_dir_parsing = database_parsing_path \n",
    "            if not os.path.exists(out_dir_parsing):\n",
    "                os.mkdir(out_dir_parsing)\n",
    "\n",
    "            ## Running function biblio_parser\n",
    "            inst_filter_list_init = None\n",
    "            bau.biblio_parser(in_dir_parsing, out_dir_parsing, database_type, expert, rep_utils, inst_filter_list_init)\n",
    "\n",
    "            # Useful printings\n",
    "            with open(Path(out_dir_parsing) / Path(bau.PARSING_PERF), 'r') as failed_json:\n",
    "                    data_failed=failed_json.read()\n",
    "            dic_failed = json.loads(data_failed)\n",
    "            articles_number = dic_failed[\"number of article\"]\n",
    "            print('\\n' + bold_text + f'Parsing processed on full {database_type} corpus' + light_text)\n",
    "            print(\"\\n Success rates\")\n",
    "            del dic_failed['number of article']\n",
    "            for item, value in dic_failed.items():\n",
    "                print(f'    {item}: {value[\"success (%)\"]:.2f}%')\n",
    "    \n",
    "    else:\n",
    "        print('\\n' + bold_text + 'Please put all reference files in the specified folder and run again cell' + light_text)\n",
    "        \n",
    "if ref_files_check == 'y': \n",
    "    second_inst = input(\"\\n Secondary institutions to be parsed (y/n, default = y)? \")\n",
    "    if second_inst == '': second_inst = 'y' \n",
    "        \n",
    "    # Setting the specific affiliations filter     \n",
    "    if second_inst == 'y':   \n",
    "        inst_filter_list = bau.INST_FILTER_LIST\n",
    "        print(f' Default secondary institutions filter is: {inst_filter_list}' )\n",
    "        change_inst_filter = input(\" Do you want to change it (y/n, default = n)? \")\n",
    "        if change_inst_filter == '': change_inst_filter = 'n'\n",
    "        if change_inst_filter == 'y':\n",
    "            # Setting the specific affiliations filter \n",
    "            inst_filter_list = bau.setting_secondary_inst_filter(path_concat_parsing)\n",
    "    else:\n",
    "        inst_filter_list = None\n",
    "    \n",
    "    # Concatenating and deduplicating parsing saved in 'project_folder' folder\n",
    "    bau.parsing_concatenate_deduplicate(useful_path_list, inst_filter_list )\n",
    "    print(bold_text + '\\nCell-run completed' + light_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II- Single year corpus analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &emsp;&emsp;II-1 Selection of the corpus file for BiblioAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "## Selection of corpus file\n",
    "corpusfiles_list = os.listdir(corpuses_folder)\n",
    "corpusfiles_list.sort()\n",
    "print('Please select the corpus via the tk window')\n",
    "myprojectname = bau.Select_multi_items(corpusfiles_list,'single',2)[0]+'/'\n",
    "project_folder = corpuses_folder /Path(myprojectname)\n",
    "database_type = input('Corpus file type (scopus, wos - default: \"wos\")? ')\n",
    "if not database_type: database_type = 'wos' \n",
    "project_folder = corpuses_folder / Path(myprojectname) / Path(database_type)\n",
    "\n",
    " # Get the folder for the general files\n",
    " # and specific files for scopus type database in this folder\n",
    "rep_package = Path('BiblioAnalysis_Utils')\n",
    "rep_utils = Path(bau.REP_UTILS) \n",
    "actual_folder = Path.cwd()\n",
    "print('\\nWorking directory: ', actual_folder)\n",
    "print('Default folder for the reference files: ', actual_folder / rep_package / rep_utils)   \n",
    "ref_files_check = 'y'\n",
    "ref_files_check = input(' Are all reference files in this folder (y/n, default: y) ?')\n",
    "if ref_files_check == 'y':\n",
    "    ## Setting the  graph main heading\n",
    "    digits_list = list(filter(str.isdigit, myprojectname))\n",
    "    corpus_year = ''\n",
    "    for i in range(len(digits_list)):corpus_year = corpus_year + digits_list[i]\n",
    "    init = str(corpuses_folder).rfind(\"_\")+1\n",
    "    corpus_state = str(corpuses_folder)[init:]\n",
    "    main_heading = corpus_year + ' Corpus:' + corpus_state\n",
    "\n",
    "    ## Printing useful information\n",
    "    dict_print = {'Specific-paths set for user:': user_id,\n",
    "                  'Project folder:': project_folder,\n",
    "                  'Corpus year:': corpus_year,\n",
    "                  'Corpus status:': corpus_state,\n",
    "                  'Project name:': myprojectname,\n",
    "                  'Corpus file type:':database_type}\n",
    "\n",
    "    pad = 3\n",
    "    max_len_str = max( [len(str(x)) for x in dict_print.values()]) + pad\n",
    "    print('\\n')\n",
    "    for key,val in dict_print.items():\n",
    "        print(key.ljust(max_len_str),val)\n",
    "    print('\\n' + bold_text + 'Cell-run completed' + light_text)\n",
    "    \n",
    "else:\n",
    "    print('\\n' + bold_text +'Please put all reference files in the specified folder' + light_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &emsp;&emsp;II-2 Data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "## Building the names of the useful folders\n",
    "\n",
    "    # Folder containing the wos or scopus file to process\n",
    "in_dir_parsing = project_folder / Path(bau.FOLDER_NAMES['rawdata'])\n",
    "\n",
    "    # Folder containing the output files of the data parsing \n",
    "out_dir_parsing = project_folder / Path(bau.FOLDER_NAMES['parsing'])\n",
    "if not os.path.exists(out_dir_parsing):\n",
    "    os.mkdir(out_dir_parsing)\n",
    "\n",
    "## Running function biblio_parser\n",
    "parser_done = input(\"Parsing available (y/n)? \")\n",
    "if parser_done == \"n\":\n",
    "    inst_filter_list_init = None\n",
    "    bau.biblio_parser(in_dir_parsing, out_dir_parsing, database_type, expert, rep_utils, inst_filter_list_init)\n",
    "         \n",
    "    second_inst = input(\"Secondary institutions to be parsed (y/n)? \")\n",
    "    if second_inst=='y': \n",
    "        inst_filter_list = bau.INST_FILTER_LIST\n",
    "        print(f' Default secondary institutions filter is: {inst_filter_list}' )\n",
    "        change_inst_filter = input(\" Do you want to change it (y/n, default = n)? \")\n",
    "        if change_inst_filter == '': change_inst_filter = 'n'\n",
    "        if change_inst_filter == 'y':\n",
    "            # Setting the specific affiliations filter \n",
    "            inst_filter_list = bau.setting_secondary_inst_filter(out_dir_parsing)\n",
    "        # Extending the author with institutions parsing file\n",
    "        bau.extend_author_institutions(out_dir_parsing, inst_filter_list)\n",
    "    \n",
    "    # Useful printings\n",
    "    PARSING_PERF = 'failed.json'\n",
    "    with open(Path(out_dir_parsing) / Path(bau.PARSING_PERF), 'r') as failed_json:\n",
    "            data_failed=failed_json.read()\n",
    "    dic_failed = json.loads(data_failed)\n",
    "    articles_number = dic_failed[\"number of article\"]\n",
    "    print(\"Parsing processed on full corpus\")\n",
    "    print(\"\\n\\nSuccess rates\")\n",
    "    del dic_failed['number of article']\n",
    "    for item, value in dic_failed.items():\n",
    "        print(f'    {item}: {value[\"success (%)\"]:.2f}%')\n",
    "else:\n",
    "    second_inst = input(\"Secondary institutions to be parsed (y/n)? \")\n",
    "    if second_inst=='y' : \n",
    "        # Setting the specific affiliations filter \n",
    "        inst_filter_list = bau.setting_secondary_inst_filter(out_dir_parsing)\n",
    "        # Extending the author with institutions parsing file\n",
    "        bau.extend_author_institutions(out_dir_parsing, inst_filter_list)\n",
    "        \n",
    "    parser_filt = input(\"Parsing available without rawdata -from filtering- (y/n)? \")\n",
    "    if parser_filt == \"n\": \n",
    "        # Reading json file of parsing performances\n",
    "        with open(Path(out_dir_parsing) / Path(bau.PARSING_PERF), 'r') as failed_json:\n",
    "            data_failed=failed_json.read()\n",
    "        dic_failed = json.loads(data_failed)\n",
    "        articles_number = dic_failed[\"number of article\"]\n",
    "        # Usefull printings\n",
    "        print(\"Parsing available from full corpus\")\n",
    "        print(\"\\n\\nSuccess rates\")\n",
    "        del dic_failed['number of article']\n",
    "        for item, value in dic_failed.items():\n",
    "            print(f'    {item}: {value[\"success (%)\"]:.2f}%')\n",
    "    else:\n",
    "        #clear_output(wait=True)\n",
    "        print(\"Parsing available from filtered corpus without rawdata\")\n",
    "        file = project_folder /Path('parsing/' + 'articles.dat')\n",
    "        with open(file) as f:\n",
    "            lines = f.readlines()\n",
    "        articles_number = len(lines)\n",
    "\n",
    "print(\"\\n\\nCorpus parsing saved in folder:\\n\", str(out_dir_parsing))\n",
    "print('\\nNumber of articles in the corpus : ', articles_number)\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  &emsp;&emsp;II-2.1 Data parsing / Corpus description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "## Building the names of the useful folders\n",
    "\n",
    "    # Folder containing the wos or scopus parsed files\n",
    "in_dir_corpus = out_dir_parsing\n",
    "\n",
    "    # Folder containing the wos or scopus parsed and analysed files\n",
    "out_dir_corpus = project_folder / Path(bau.FOLDER_NAMES['description'])\n",
    "if not os.path.exists(out_dir_corpus):\n",
    "    os.mkdir(out_dir_corpus)    \n",
    "\n",
    "## Running describe_corpus\n",
    "description_done = input(\"Description available (y/n)? \")\n",
    "#clear_output(wait=True)\n",
    "if description_done == \"n\":\n",
    "    verbose = False\n",
    "    bau.describe_corpus(in_dir_corpus, out_dir_corpus, database_type, verbose)\n",
    "    print(\"Corpus description saved in folder:\", str(out_dir_corpus))\n",
    "else:\n",
    "    print(\"Corpus description available in folder:\", str(out_dir_corpus))\n",
    "\n",
    "# Building the name of file for histogram plot of an item\n",
    "fullpath_distrib_item = out_dir_corpus / Path(bau.DISTRIBS_ITEM_FILE)\n",
    "\n",
    "## Running plot of treemap, scatter plot and histogram for a selected item_treemap\n",
    "do_treemap = input(\"Treemap for an item of the corpus description (y/n)? \")\n",
    "if do_treemap == 'y':\n",
    "    renew_treemap = 'y'\n",
    "    while renew_treemap == 'y' :\n",
    "        print(\"Choose the item for treemap in the tk window\")\n",
    "        item_treemap = bau.treemap_item_selection()\n",
    "        fullpath_file_treemap = out_dir_corpus / Path('freq_'+ item_treemap +'.dat')\n",
    "        print(\"Item selected:\",item_treemap)\n",
    "        bau.treemap_item(item_treemap, fullpath_file_treemap)\n",
    "        do_scatter = input(\"Scatter plot for the item (y/n)? \")\n",
    "        if do_scatter == 'y':\n",
    "            bau.plot_counts(item_treemap, fullpath_file_treemap)\n",
    "        do_histo = input(\"Histogram plot for the item (y/n)? \")\n",
    "        if do_histo == 'y':\n",
    "            bau.plot_histo(item_treemap, fullpath_distrib_item)\n",
    "        renew_treemap = input(\"\\n\\nTreemap for a new item (y/n)? \")\n",
    "\n",
    "# Initialize the variable G_coupl that will receive the biblioanalysis coupling graphs\n",
    "try: G_coupl\n",
    "except NameError: G_coupl = None\n",
    "\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &emsp;&emsp;II-2.1.1 Data parsing / Corpus description / Filtering the data and filtered corpus description\n",
    "To be run after corpus description to allow using the following functions : describe_corpus() , treemap_item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil                      \n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "## Recursive filtering\n",
    "\n",
    "# Allows prints in filter_corpus_new function\n",
    "verbose = False\n",
    "\n",
    "# Initialization of parameters for recursive filtering\n",
    "filtering_step = 1\n",
    "while True:\n",
    "\n",
    "    ## Building the names of the useful folders and creating the output folder if not find \n",
    "    if filtering_step == 1:\n",
    "        in_dir_filter = out_dir_parsing\n",
    "        ### Get the folder for the filter configuration file         \n",
    "        gui_titles = {'main':   'Folder selection GUI for config_filters.json file ',\n",
    "                      'result': 'Selected folder'}\n",
    "        gui_buttons = ['SELECTION','HELP']\n",
    "        filter_config_folder = bau.select_folder_gui_new(user_root, gui_titles, gui_buttons, bau.GUI_DISP,\n",
    "                                                         widget_ratio=1, button_ratio=1, \n",
    "                                                         max_lines_nb=3)\n",
    "        \n",
    "        print('Filter configuration folder:', filter_config_folder)\n",
    "        file_config_filters = filter_config_folder / Path('config_filters.json')\n",
    "        print('Filter configuration file:',file_config_filters)\n",
    "        modif_filtering = input(\"Modification of item-values list from a predefined file (y/n)? \")\n",
    "        if modif_filtering == \"y\":\n",
    "            bau.filters_modification(filter_config_folder,file_config_filters)    \n",
    "    else:\n",
    "        renew_filtering = input(\"Apply a new filtering process (y/n)? \") \n",
    "        if renew_filtering == \"n\": break\n",
    "        in_dir_filter = project_folder / Path(bau.FOLDER_NAMES['filtering'] + '_' + str(filtering_step-1))\n",
    "        file_config_filters = in_dir_filter / Path('save_config_filters.json')\n",
    "        print('Filter configuration file:',file_config_filters) \n",
    "        \n",
    "    out_dir_filter = project_folder / Path(bau.FOLDER_NAMES['filtering'] + '_' + str(filtering_step))\n",
    "            \n",
    "    if not os.path.exists(out_dir_filter):\n",
    "        os.mkdir(out_dir_filter)\n",
    "    else:\n",
    "        print('out_dir_filter exists')\n",
    "        files = glob.glob(str(out_dir_filter) + '/*.*')\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "\n",
    "    # Building the absolute file name of filter configuration file to save for the filtering step\n",
    "    save_config_filters = out_dir_filter / Path(bau.SAVE_CONFIG_FILTERS)\n",
    "    print('\\nSaving filter configuration file:',save_config_filters)\n",
    "    \n",
    "    # Configurating the filtering through a dedicated GUI or getting it from the existing file\n",
    "    bau.filters_selection(file_config_filters,save_config_filters,in_dir_filter,\n",
    "                         fact=3, win_widthmm=85, win_heightmm=115, font_size=16)\n",
    "    shutil.copyfile(save_config_filters, file_config_filters)\n",
    "\n",
    "    # Read the filtering status\n",
    "    combine,exclusion,filter_param = bau.read_config_filters(file_config_filters) \n",
    "    print(\"\\nFiltering status:\")\n",
    "    print(\"   Combine   :\",combine)\n",
    "    print(\"   Exclusion :\",exclusion)\n",
    "    for key,value in filter_param.items():\n",
    "        print(f\"   Item      : {key}\\n   Values    : {value}\\n\")\n",
    "\n",
    "    # Running function filter_corpus_new\n",
    "    bau.filter_corpus_new(in_dir_filter, out_dir_filter, verbose, file_config_filters) # <---???\n",
    "    file = out_dir_filter /Path('articles.dat')\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        articles_number = len(lines)\n",
    "    if articles_number == 0:\n",
    "        print('Filtered corpus empty !')\n",
    "        break\n",
    "    print(\"Filtered-corpus parsing saved in folder \", \n",
    "            str(out_dir_filter),\n",
    "            \" with the corresponding filters configuration\")\n",
    "\n",
    "        # Folder containing the wos or scopus parsed and filtered files\n",
    "    in_dir_freq_filt = out_dir_filter\n",
    "\n",
    "        # Folder containing the wos or scopus parsed, filtered and analysed files\n",
    "    out_dir_freq_filt = project_folder / Path(bau.FOLDER_NAMES['description'] + '_' + str(filtering_step))\n",
    "    if not os.path.exists(out_dir_freq_filt): os.mkdir(out_dir_freq_filt)\n",
    "\n",
    "        # Running describe_corpus \n",
    "    verbose = False\n",
    "    bau.describe_corpus(in_dir_freq_filt, out_dir_freq_filt, database_type, verbose)\n",
    "    print(\"Filtered corpus description saved in folder:\", str(out_dir_freq_filt))\n",
    "\n",
    "    # Treemap plot by a corpus item after filtering\n",
    "    make_treemap = 'n'\n",
    "    make_treemap = input(\"\\n\\nDraw treemap (y/n)?\")\n",
    "    if make_treemap == 'y' :\n",
    "\n",
    "            # Running plot of treemap for selected item_treemap\n",
    "        renew_treemap = 'y'    \n",
    "        while renew_treemap == 'y' :\n",
    "            print('\\n\\nChoose the item for treemap of the filtered corpus description in the tk window')\n",
    "            item_treemap = bau.treemap_item_selection()\n",
    "            file_name_treemap = project_folder / Path(bau.FOLDER_NAMES['description'] + '_'\\\n",
    "                                                      + str(filtering_step) + '/' + 'freq_'+ item_treemap +'.dat')\n",
    "            print(\"Item selected:\",item_treemap)\n",
    "            bau.treemap_item(item_treemap, file_name_treemap)\n",
    "            renew_treemap = input(\"\\n\\nTreemap for a new item (y/n)? \") \n",
    "\n",
    "    filtering_step += 1\n",
    "\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &emsp;&emsp;II-2.1.2 Data parsing / Corpus Description / Bibliographic Coupling analysis\n",
    "To be run after corpus description to use the frequency analysis. You may execute the bibliographic coupling script several times successively on unfiltered corpus and on available filtering steps of the corpus.\n",
    "The result files are saved in independant folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "# Building the names of the useful folders and creating the output folder if not find  \n",
    "filtering = input(\n",
    "                  \"Corpus filtered (y/n)? \"\n",
    "                 )   \n",
    "if filtering == \"y\":\n",
    "    filtering_step = input(\n",
    "                            \"Enter filtering step : \"\n",
    "                          ) \n",
    "    in_dir_coupling = project_folder / Path(bau.FOLDER_NAMES['filtering'] + '_' + str(filtering_step))\n",
    "    in_dir_freq= project_folder / Path(bau.FOLDER_NAMES['description'] + '_' + str(filtering_step))\n",
    "    out_dir_coupling = project_folder / Path(bau.FOLDER_NAMES['coupling'] + '_' + str(filtering_step))\n",
    "else:\n",
    "    in_dir_coupling = out_dir_parsing\n",
    "    in_dir_freq= out_dir_corpus    \n",
    "    out_dir_coupling = project_folder / Path(bau.FOLDER_NAMES['coupling'])\n",
    "\n",
    "if not os.path.exists(out_dir_coupling):\n",
    "    os.mkdir(out_dir_coupling)\n",
    "else:\n",
    "    print('out_dir_coupling exists')\n",
    "    files = glob.glob(str(out_dir_coupling) + '/*.html')\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "    \n",
    "# Building the coupling graph of the corpus\n",
    "print('Building the coupling graph of the corpus, please wait...')\n",
    "G_coupl = bau.build_coupling_graph(in_dir_coupling)\n",
    "\n",
    "# Building the partition of the corpus\n",
    "print('Building the partition of the corpus, please wait...')\n",
    "G_coupl,partition = bau.build_louvain_partition(G_coupl)\n",
    "print()\n",
    "\n",
    "# Adding attributes to the coupling graph nodes\n",
    "attr_dic = {}\n",
    "add_attrib = input(\"Add attributes to the coupling graph nodes (y/n)? \")\n",
    "if add_attrib == 'y':\n",
    "    while True:\n",
    "        print('\\n\\nChoose the item for the attributes to add in the tk window')\n",
    "        item, m_max_attrs = bau.coupling_attr_selection(fact=2, win_widthmm=80, win_heightmm=60, font_size=16)\n",
    "        attr_dic[item] = m_max_attrs\n",
    "        print(\"Item selected:\",item,\" with \",m_max_attrs, \" attributes\" )\n",
    "        G_coupl = bau.add_item_attribute(G_coupl, item, m_max_attrs, in_dir_freq, in_dir_coupling)\n",
    "        renew_attrib = input(\"\\nAdd attributes for a new item (y/n)?\") \n",
    "        if renew_attrib == 'n' : break      \n",
    "\n",
    "# Plot control of the coupling graph before using Gephy\n",
    "NODES_NUMBER_MAX = 1\n",
    "bau.plot_coupling_graph(G_coupl,partition,nodes_number_max=NODES_NUMBER_MAX)\n",
    "\n",
    "# Creating a Gephy file of the coupling graph  \n",
    "bau.save_graph_gexf(G_coupl,out_dir_coupling)\n",
    "print(\"\\nCoupling analysis of the corpus saved as Gephy file in folder:\\n\", str(out_dir_coupling))\n",
    "\n",
    "# Creating an EXCEL file of the coupling analysis results\n",
    "bau.save_communities_xls(partition,in_dir_coupling,out_dir_coupling)\n",
    "print(\"\\nCoupling analysis of the corpus saved as EXCEL file in folder:\\n\", str(out_dir_coupling))\n",
    "\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### &emsp;&emsp;II-2.1.3  HTML graph of coupling analysis \n",
    "##### after Data parsing / Corpus Description / Coupling analysis  \n",
    "You may execute the HTML graph construction script several times successively on the available coupling graph of the corpus. The result files are saved in the corresponding coupling floder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Creating html file of graph G using pyviz\n",
    "   This script uses the results of the Biblioanalysis coupling analysis:\n",
    "   - out_dir_coupling (Path): path for saving the coupling analysis results;\n",
    "   - G (networkx object): coupling graph with added attributes;\n",
    "   - partition (dict):  partition of graph G;\n",
    "   - attr_dic (dict): dict of added attributes with number of added values. \n",
    "   \n",
    "'''\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "# Checking the availability of the corpus coupling graph G with all attributes and its partition\n",
    "assert(G_coupl is not None),'''Please run first the \"Bibliographic coupling analysis\" \n",
    "                                script to build the coupling graph'''\n",
    "\n",
    "# Setting the item label among the added attribute to be colored\n",
    "colored_attr = input('Please enter the item label among the added attributes to be colored (default: S)')\n",
    "if colored_attr == '':colored_attr = 'S'\n",
    "print('Attribute to be colored:',colored_attr)\n",
    "if colored_attr == 'S': \n",
    "    heading3 = 'Colored by main discipline (grey: without filtering subjects as main discipline).'\n",
    "else:\n",
    "    heading3 = 'Colored by main attribute values (grey: without filtering attribute values as main discipline).'\n",
    "assert(colored_attr in attr_dic.keys()),\\\n",
    "    f'''Selected colored attribute should be among the added attributes: {list(attr_dic.keys())}.\n",
    "Please run this script again to select an effectivelly added attribute to the coupling graph node \n",
    "or run again the \"Bibliographic coupling analysis\" script to add the targetted attribute to the coupling graph.'''\n",
    "\n",
    "# Setting the colors for the values of the attribute to be colored\n",
    "# default: values of 'S' item from a particular corpus\n",
    "# TO DO: define the list of the attribute values through a GUI\n",
    "colored_attr_values = {'Neurosciences & Neurology':'0',\n",
    "                  'Psychology':'1',\n",
    "                  'Computer Science':'2',\n",
    "                  'Robotics,Automation & Control Systems':'3',\n",
    "                  'Life Sciences & Biomedicine - Other Topics':'4',\n",
    "                  'Biochemistry & Molecular Biology':'4',\n",
    "                  'Cell Biology':'4',\n",
    "                  'Evolutionary Biology':'4',\n",
    "                  'Biomedical Social Sciences':'4',\n",
    "                  'Biotechnology & Applied Microbiology':'4',\n",
    "                  'Developmental Biology':'4',\n",
    "                  'Microbiology':'4',\n",
    "                  'Marine & Freshwater Biology':'4',\n",
    "                  'Reproductive Biology':'4',\n",
    "                  'Genetics & Heredity':'4',\n",
    "                  'Philosophy':'5',\n",
    "                  'History & Philosophy of Science':'5',\n",
    "                  'Social Sciences - Other Topics':'6',\n",
    "                  'Mathematical Methods In Social Sciences':'6',\n",
    "                  'Linguistics':'7',\n",
    "                  'Anthropology':'8',\n",
    "                 }\n",
    "\n",
    "# Setting the attribute value to be specifically shaped\n",
    "shaped_attr = input('Please enter the added attribute value to be specifically shaped (default: Psychology)')\n",
    "if shaped_attr == '':shaped_attr = 'Psychology'\n",
    "print('Attribute value to be specifically shaped (triangle):',shaped_attr)\n",
    "heading4 = 'Triangles for \"' + shaped_attr + '\" in disciplines.'\n",
    "\n",
    "# Computing the number of communities\n",
    "community_number = len(set(partition.values()))\n",
    "print('Number of communities:',community_number)\n",
    "\n",
    "# Computing the size of the communities\n",
    "communities_size = {}\n",
    "for value in set(partition.values()):\n",
    "    communities_size[value]=0\n",
    "    for key in set(partition.keys()):\n",
    "        if partition[key] == value:\n",
    "            communities_size[value]+=1\n",
    "            \n",
    "# Building the html graphs per community\n",
    "for community_id in range(community_number):\n",
    "    community_size = communities_size[community_id] \n",
    "    heading2 = 'Coupling graph for community ID: ' + str(community_id) + ' Size: ' + str(community_size)\n",
    "    heading = '<h1>' + main_heading + '</h1>' + '<h2>' + heading2 + '</h2>' \\\n",
    "                  + '<h3 align=left nowrap>' + heading3 + '<br>'  + heading4 + '</h3>'\n",
    "    html_file= str(out_dir_coupling /Path('coupling_' + 'com' + str(community_id) \\\n",
    "                                          + '_size' + str(community_size) + '.html'))\n",
    "    #bau.coupling_graph_html_plot(G_coupl,html_file,community_id,attr_dic,colored_attr,\n",
    "    #                             colored_attr_values,shaped_attr,nodes_colors,edges_color,\n",
    "    #                             background_color,font_color,heading)\n",
    "    bau.coupling_graph_html_nwplt(G_coupl,html_file,community_id,attr_dic,colored_attr,\n",
    "                                  colored_attr_values,shaped_attr,heading)\n",
    "# Building the html graph for the full corpus\n",
    "heading2  = ' All ' + str(community_number) + ' communities'\n",
    "heading = '<h1>' + main_heading + '</h1>' + '<h2>' + heading2 + '</h2>' \\\n",
    "          + '<h3 align=left nowrap>' + heading3 + '<br>'  + heading4 + '</h3>'\n",
    "html_file= str(out_dir_coupling /Path('coupling_' + 'all.html'))\n",
    "#bau.coupling_graph_html_plot(G_coupl,html_file,'all',attr_dic,colored_attr,\n",
    "#                         colored_attr_values,shaped_attr,nodes_colors,edges_color,\n",
    "#                         background_color,font_color,heading)\n",
    "bau.coupling_graph_html_nwplt(G_coupl,html_file,'all',attr_dic,colored_attr,\n",
    "                              colored_attr_values,shaped_attr,heading)\n",
    "\n",
    "print(\"\\nCreated html files of graph G_coupl using pyviz for the corpus in folder:\\n\", str(out_dir_coupling))\n",
    "\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;&emsp;II-2.2 Data parsing / Co-occurrence Maps\n",
    "You may execute the co-occurence script several times successively on unfiltered corpus and on available filtering steps of the corpus.\n",
    "The result files are saved in independant folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "# Building the names of the useful folders and creating the output folder if not find \n",
    "filtering = input(\n",
    "                  \"Corpus filtered (y/n)? \"\n",
    "                 )   \n",
    "if filtering == \"y\":\n",
    "    filtering_step = input(\n",
    "                            \"Enter filtering step : \"\n",
    "                          ) \n",
    "    in_dir_cooc = project_folder / Path(bau.FOLDER_NAMES['filtering'] + '_' + str(filtering_step))\n",
    "    out_dir_cooc = project_folder / Path(bau.FOLDER_NAMES['cooccurrence'] + '_' + str(filtering_step))\n",
    "else:\n",
    "    in_dir_cooc = out_dir_parsing   \n",
    "    out_dir_cooc = project_folder / Path(bau.FOLDER_NAMES['cooccurrence']) \n",
    "\n",
    "if not os.path.exists(out_dir_cooc):\n",
    "    os.mkdir(out_dir_cooc)\n",
    "else:\n",
    "    print('out_dir_cooc available')\n",
    "\n",
    "## Building the co-ocurrence graph\n",
    "size_min = 1\n",
    "node_size_ref=300\n",
    "while True :\n",
    "    print('\\n\\nChoose the item for co-occurrence analysis in the tk window')\n",
    "    cooc_item, size_min = bau.cooc_selection(fact=3, win_widthmm=80, win_heightmm=100, font_size=16) \n",
    "    print(\"Item selected:\",cooc_item,\" at minimum size \",size_min)\n",
    "    out_dir_cooc_item = out_dir_cooc / Path('cooc_' + cooc_item + \\\n",
    "                                            '_thr' + str(size_min))\n",
    "    if not os.path.exists(out_dir_cooc_item):\n",
    "        os.mkdir(out_dir_cooc_item)\n",
    "    else:\n",
    "        print('out_dir_cooc_item available')\n",
    "    G_cooc = bau.build_item_cooc(cooc_item,in_dir_cooc, out_dir_cooc_item, size_min = size_min)\n",
    "    if G_cooc is None:\n",
    "        print(f'The minimum node size ({size_min}) is two large. Relax this constraint.')\n",
    "    else:\n",
    "        print(\"Co-occurrence analysis of the corpus for item \" + cooc_item + \\\n",
    "          \" saved in folder:\", str(out_dir_cooc_item))\n",
    "        heading2 = 'Co_occurence graph for item ' + cooc_item + ' with minimum node size ' + str(size_min)\n",
    "        heading3 = 'Bold node title: Node attributes[number of item value occurrences-item value (total number of edges)]'\n",
    "        heading4 = 'Light node titles: Neighbors attributes[number of item value occurrences-item value (number of edges with node)]'\n",
    "        heading = '<h1>' + main_heading + '</h1>' + '<h2>' + heading2 + '</h2>' \\\n",
    "                  + '<h3 align=left nowrap>' + heading3 + '<br>'  + heading4 + '</h3>'\n",
    "    \n",
    "        bau.plot_cooc_graph(G_cooc,cooc_item,size_min=size_min,node_size_ref=node_size_ref)\n",
    "        # Creating html file of graph G_cooc using pyviz\n",
    "        html_file= str(out_dir_cooc_item /Path('cooc_' + cooc_item + '_thr' + str(size_min) + '.html'))\n",
    "        bau.cooc_graph_html_plot(G_cooc,html_file,heading)\n",
    "        print(\"Created html file of\",cooc_item,\"co-occurrence graph using pyviz in folder:\\n\",\\\n",
    "              str(out_dir_cooc_item))\n",
    "        \n",
    "    renew_cooc = input(\"\\n\\nCo-occurrence analysis for a new item (y/n)?\") \n",
    "    if renew_cooc == 'n' : break\n",
    "\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III- Temporal development of item values weight\n",
    "To run this cell a set of annual corpuses with their description should be available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "# Initialize the search configuration dict \n",
    "keyword_filters = {\n",
    "    'is_in':[],    \n",
    "    'is_equal':[]}\n",
    "\n",
    "## Get the folder for the configuration file for the temporal development analysis \n",
    "# To Do : use the new gui\n",
    "temporaldev_config_folder = bau.select_folder_gui(user_root,'Select the folder for config_temporal.json file')\n",
    "print('Item_values selection folder:', temporaldev_config_folder )\n",
    "\n",
    "## Building the search configuration:\n",
    "#### - either by reading of the 'config_temporal.json' without modification\n",
    "#### - or by an interactive modification of the configuration and storing it in this file for a futher use\n",
    "TemporalDev_file = temporaldev_config_folder / Path('config_temporal.json')\n",
    "\n",
    "keywords_modif = input('Modification of the keywords list (y/n)?')\n",
    "if keywords_modif == 'y':\n",
    "    \n",
    "        # Selection of items\n",
    "    items_full_list = ['IK','AK','TK','S','S2']\n",
    "    print('\\nPlease select the items to be analyzed via the tk window')\n",
    "    items = bau.Select_multi_items(items_full_list,'multiple')\n",
    "\n",
    "        # Selection of the folder of item-values full-list file\n",
    "        # To Do : use the new gui\n",
    "    select_folder = bau.select_folder_gui(user_root,'Select the folder of the item-values list files')\n",
    "\n",
    "        # Setting the file of the item-values full list  \n",
    "    keywords_full_list_file = select_folder / Path('TempDevK_full.txt')\n",
    "    \n",
    "        # Setting the list of item-values full list\n",
    "    keywords_full_list = bau.item_values_list(keywords_full_list_file)\n",
    "    \n",
    "        # Selection of the item-values list to be put in the temporal development configuration file \n",
    "    search_modes = ['is_in','is_equal']\n",
    "    for search_mode in search_modes:\n",
    "        print('\\nPlease select the keywords for ',search_mode, ' via the tk window')\n",
    "        keyword_filters[search_mode] = bau.Select_multi_items(keywords_full_list,mode = 'multiple')\n",
    "        \n",
    "    # Saving the new configuration in the 'config_temporal.json' file   \n",
    "    bau.write_config_temporaldev(TemporalDev_file,items,keyword_filters)\n",
    "    print('\\n New temporal development configuration saved in: \\n', TemporalDev_file)    \n",
    "else:\n",
    "    # Reading the search configuration from the 'config_temporal.json' file  \n",
    "    items,keywords_param = bau.read_config_temporaldev(TemporalDev_file)\n",
    "    print('Selection of items:\\n',items)    \n",
    "    keyword_filters['is_in'] = keywords_param['is_in']\n",
    "    keyword_filters['is_equal'] = keywords_param['is_equal']\n",
    "\n",
    "## Selection of annual corpus files\n",
    "corpusfiles_list = os.listdir(corpuses_folder)\n",
    "corpusfiles_list.sort()\n",
    "print('\\nPlease select the corpuses to be analyzed via the tk window')\n",
    "years = bau.Select_multi_items(corpusfiles_list,'multiple')\n",
    "\n",
    "# Print configuration\n",
    "print('Search items:', items)\n",
    "print('\\nSearch Words:\\n' + json.dumps(keyword_filters, indent=2))\n",
    "print('\\n Selection of annual corpus files:\\n',years, '\\n')\n",
    "\n",
    "# Performing the search using the keyword_filters dict\n",
    "keyword_filter_list = bau.temporaldev_itemvalues_freq(keyword_filters ,items, years, corpuses_folder)\n",
    "\n",
    "# Saving the search results in an EXCEL file\n",
    "store_file = corpuses_folder / Path('Results_Files/TempDev_synthesis.xlsx')\n",
    "bau.temporaldev_result_toxlsx(keyword_filter_list,store_file)\n",
    "print('\\nTemporal development results saved in:\\n', store_file) \n",
    "\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annexe 1- Databases merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "database, filename, in_dir, out_dir = bau.merge_database_gui()\n",
    "bau.merge_database(database,filename,in_dir,out_dir)\n",
    "\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annexe 2- Item values selection to list for filters configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from pathlib import Path\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "# Get the folder for the filter configuration file \n",
    "# To Do : use the new gui\n",
    "filter_config_folder = bau.select_folder_gui(user_root,'Select the folder for the config_filters.json file')\n",
    "print('Filter configuration folder:', filter_config_folder) \n",
    "\n",
    "file_config_filters = filter_config_folder/ Path('config_filters.json')    \n",
    "bau.filters_modification(filter_config_folder,file_config_filters)\n",
    "\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annexe 3- Upgrade of parsing files with column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "# Get the folder for the filter configuration file\n",
    "# To Do : use the new gui\n",
    "corpus_folder_to_upgrade = bau.select_folder_gui(user_root,'Select the corpus folder to upgrade')\n",
    "print('Corpus folder to upgrade:', corpus_folder_to_upgrade) \n",
    "bau.upgrade_col_names(corpus_folder_to_upgrade)\n",
    "\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data parsing\n",
    "- articles.dat is the central file, listing all the publications within the corpus. It contains informations such as the document type (article, letter, review, conf proceeding, etc), title, year of publication, publication source, doi, number of citations (given by WOS or Scopus at the time of the extraction) AND a unique identifier used in all the other files to identify a precise publication.\n",
    "- database.dat keeps track of the origin of the data, some part of the analysis being specific to WOS or Scopus data.\n",
    "- authors.dat lists all authors names associated to all publications ID.\n",
    "- addresses.dat lists all adresses associated to all publications ID, along with a specific ID for each adresse line. These adresses are reported as they appear in the raw data, without any further processing.\n",
    "- countries.dat lists all countries associated to all publications ID and adresses lines ID. The countries are extracted from the adresses fields of the raw data, with some cleaning (changing mentions of US states and UK countries to respectively the USA and UK).\n",
    "- institutions.dat lists all the comma-separated entities appearing in the adresses field associated to all publications ID and adresses lines ID, except those refering to a physical adresses. These entities correspond to various name variants of universities, organisms, hospitals, labs, services, departments, etc as they appear in the raw data. No treatment is made to e.g. filtering out the entities corresponding a given hierarchy level.\n",
    "- keywords.dat lists various types of keywords associated to all publications ID. \"AK\" keywords correspond to Author's keywords. \"IK\" keywords correspond to either WOS or Scopus keywords, which are built based on the authors' keywords, the title and abstract. \"TK\" correspond to title words (from which we simply remove common words and stop words - no stemming is performed). TK are especially useful when studying pre-90's publications, when the use of keywords was not yet standard.\n",
    "- references.dat lists all the references associated to all publications ID. The rawdata is parsed to store the first author name, title, source, volume and page of each reference of the raw \"references\" field.\n",
    "- subjects.dat lists all subject categories associated to all publications ID (a journal may be associated to many subject category). WOS classifies the sources it indexes into ∼ 250 categories, that are reported in the extracted data. Scopus classifies its sources into 27 major categories and ∼ 300 sub-categories, none of which are reported in the extracted data. We use Elsevier Source Title List (october 2017 version) to retrieve that information. The \"subject.dat\" contains the info relative to the major categories.\n",
    "- subjects2.dat lists Scopus's sub-categories, if the use database is Scopus.\n",
    "- AA_log.txt keeps track of the date/time the script was executed and of all the messages displayed on the terminal (number of publications extracted, % of references rejected, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Corpus description\n",
    "Before doing anything else, you should get a general idea of the content of your database.  This script performs several basic tasks:\n",
    "- it performs a series of frequency analysis, computing the number of occurrences of each item (authors, keywords, references, etc) within the publications of the corpus. These frequencies are automatically stored into several \"freq_xxx.dat\" files within a newly created \"freq\" folder.\n",
    "- it performs a series of generic statistical analysis, storing the numbers of distinct items of each type (e.g. there are x distinct keyword in the corpus ), the distributions of number of occurrences of each item (e.g. there are x keywords appearing in at least y publications) and the distribution of number of items per publication (e.g.there are x% of publications with y keywords). All these statistics are stored in the \"DISTRIBS_itemuse.json\" file.\n",
    "- it also performs a co-occurrence analysis, computing the number of co-occurrence of pairs of items among the top 100 most frequent items of each type (e.g. computing how often the two most used keywords appear together in the same publications). The results of this analysis are stored in the \"coocnetworks.json\" file. More systematic co-occurrence analysis can also be performed with another script, cf the Co-occurrence Maps section below.\n",
    "All the generated files can be opened and read with a simple text editor. The freq_xxx.dat, listing items by order of frequency, can also be read in a spreadsheet software such as excel. All the files are however primarily made to be read in the BiblioMaps interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the data\n",
    "#### To be run after corpus description to allow using the following functions : describe_corpus() , treemap_item()\n",
    "\n",
    "If, upon exploring the nature of the data you realize that before going further you'd prefer to filter your corpus based on some characteristic (keeping only the publications from certain years, using some keywords or references, written by some authors from some countries, etc), you can filter the initial corpus thanks to the script:\n",
    "\n",
    "- python BiblioTools3.2/filter.py -i myprojectname/ -o myprojectname_filtered -v <br>\n",
    "\n",
    "Edit the 'filter.py' file to specify your filters. You'll also need to create a new \"myprojectname_filtered\" main folder before running the script.\n",
    "- create the files articles.dat, addresses.dat, authors.dat, countries.dat, institutions.dat, keywords.dat, references.dat, subjects.dat, subjects2.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurrence Maps\n",
    "You may execute the co-occurrence script several times successively on unfiltered corpus and on available filtering steps of the corpus.\n",
    "The result files are saved in independant folders.\n",
    "\n",
    "The script create multiple co-occurrence networks, all stored in gdf and gexf files that can be opened in Gephi, among which:\n",
    "\n",
    "Example of heterogeneous network generated with BiblioAnlysis and visualized in Gephi.\n",
    "\n",
    "- a co-cocitation network, linking references that are cited in the same publications.\n",
    "- a co-refsources network, linking references's sources that are cited in the same publications.\n",
    "- a co-author network, linking authors that collaborated in some publications.\n",
    "- a co-country network, linking countries with researchers that collaborated in some publications.\n",
    "- a co-institution network, linking institutions with researchers that collaborated in some publications. For this network to be fully useful, you may want to spend some time cleaning the \"institutions.dat\", e.g. by keeping only the big institutions (university level) or by replacing minor name variant by the dominant name variant (\"Ecole Normale Supérieure de Lyon\" → \"ENS Lyon\")\n",
    "- a co-keyword network, linking keywords being co-used in some publications. Be careful about the interpretation: keywords can be polysemic, their meaning differing from field to another (eg \"model\", \"energy\", \"evolution\", etc)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
