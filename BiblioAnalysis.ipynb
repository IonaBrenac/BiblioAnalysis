{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiblioAnalysis\n",
    "\n",
    "### Version: 5.1.2 : tested to take into account changes in scopus on 07/2023 and combining issn and e-issn for wos\n",
    "\n",
    "#### Validating the modules of the package `BiblioAnalysis_Utils` version 5.3.1 (not yet on GitHub)\n",
    "#### Git Hub version of the package: 5.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### Aims\n",
    "- This jupyter notebook results from the use analysis of BiblioTools2jupyter notebook and a new implementation of the following parts:\n",
    "    - Parsing: replaced and tested \n",
    "    - Corpus description: replaced and tested\n",
    "    - Filtering: replaced and tested, integrating the \"EXCLUSION\" mode and the recursive filtering\n",
    "    - Cooccurrence analysis : replaced and tested, integrating graph plot and countries GPS coordinates\n",
    "    - Coupling analyis : replaced and tested\n",
    "    \n",
    "### Created modules in the package BiblioAnalysis_Utils\n",
    "    - BiblioCooc.py\n",
    "    - BiblioCoupling.py\n",
    "    - BiblioDescription.py\n",
    "    - BiblioFilter.py    \n",
    "    - BiblioGeneralGlobals.py\n",
    "    - BiblioGraphPlot.py\n",
    "    - BiblioGui.py\n",
    "    - BiblioNltk.py\n",
    "    - BiblioParsingConcat.py\n",
    "    - BiblioParsingInstitutions.py\n",
    "    - BiblioParsingScopus.py\n",
    "    - BiblioParsingUtils.py\n",
    "    - BiblioParsingWos.py  \n",
    "    - BibloRefs.py\n",
    "    - BiblioSpecificGlobals.py\n",
    "    - BiblioSys.py\n",
    "    - BiblioTempDev.py\n",
    "\n",
    "### BiblioTool3.2 source\n",
    "http://www.sebastian-grauwin.com/bibliomaps/download.html \n",
    "\n",
    "### List of initial Python packages extracted from  BiblioTool3.2\n",
    "- biblio_parser.py\t⇒ pre-processes WOS / Scopus data files,\n",
    "- corpus_description.py\t⇒ performs a frequency analysis of the items in corpus,\n",
    "- filter.py\t⇒ filters the corpus according to a range of potential queries but still too specific\n",
    "- biblio_coupling.py\t⇒ performs a BC anaysis of the corpus,\n",
    "- cooc_graphs.py\t⇒ produces various co-occurrence graphs based on the corpus (call parameters changed)\n",
    "\n",
    "### Specifically required list of pip install \n",
    "(to be integrated in the setup.py of BiblioAnalysis_Utils)\n",
    "- !pip3 install fuzzywuzzy\n",
    "- !pip3 install squarify \n",
    "- !pip3 install python-louvain\n",
    "- !pip3 install python-Levenshtein\n",
    "- !pip3 install pyvis\n",
    "- !pip3 install screeninfo\n",
    "\n",
    "### Specifically required nltk downloads \n",
    "(integrated in BiblioNltk.py of BiblioAnalysis_Utils)\n",
    "- import nltk\n",
    "    - nltk.download('punkt')\n",
    "    - nltk.download('averaged_perceptron_tagger')\n",
    "    - nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preliminary instructions\n",
    "#### These actions will be interactively performed in the next version of the Jupyter notebook\n",
    "- Create the 'BiblioAnalysis_Files/' folder in your 'Users/' folder\n",
    "<br>\n",
    "<br>\n",
    "- Create in this 'BiblioAnalysis_Files/' folder, the 'Configuration_Files/' folder\n",
    "<br>\n",
    "- Store the configuration files (config_filter.json) a the 'Configuration_Files/' folder that are:\n",
    "    - 'config_filter.json' used for the filtering of a corpus\n",
    "    - 'congig_temporal.json'used for the temporal development of item values in a set of annual coupuses \n",
    "<br>\n",
    "<br>\n",
    "- Create, in the 'Configuration_Files/' folder, your additional_files folder to be named 'Selection_Files/' \n",
    "<br>\n",
    "- Store your files (free names) of selected item values in this additional_files folder together with:\n",
    "    - 'TempDevK_full.txt' used to select the words to search in the description files of the corpuses for the temporal development of item values in the set of annual coupuses\n",
    "<br>\n",
    "<br>\n",
    "- Create, in the 'BiblioAnalysis_Files/' folder, your project folder\n",
    "<br>\n",
    "- Create the 'rawdata/' folder in your project folder\n",
    "<br>\n",
    "- Store your corpus file (either wos or scopus extraction) in the 'rawdata/' folder of your project folder\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User environment setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import platform\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "clear_output(wait=True)\n",
    "bold_text = bau.BOLD_TEXT\n",
    "light_text = bau.LIGHT_TEXT\n",
    "\n",
    "# Set the venv use status\n",
    "venv = False\n",
    "print('Virtual environment: ', venv)\n",
    "\n",
    "# Get the information of current operating system\n",
    "os_name = platform.uname().system\n",
    "print('Operating system:    ', os_name)\n",
    "if os_name=='Darwin':\n",
    "    bau.add_site_packages_path(venv)\n",
    "    \n",
    "\n",
    "# User identification\n",
    "user_root = Path.home()\n",
    "user_id =  str(user_root)[str(user_root).rfind('/')+1:]\n",
    "print('User:                ', user_id)\n",
    "expert =  False\n",
    "\n",
    "# Getting the corpuses folder\n",
    " # Setting the GUI titles\n",
    "gui_titles = {'main':   'Corpuses folder selection window',\n",
    "              'result': 'Selected folder'}\n",
    "gui_buttons = ['SELECTION','HELP']\n",
    "\n",
    "corpuses_folder = bau.select_folder_gui_new(user_root, gui_titles, gui_buttons, bau.GUI_DISP)\n",
    "print('\\nCorpuses folder:', corpuses_folder)\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# I- Merging corpuses from different databases for single year corpus analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &emsp;&emsp;I-1 Building useful paths aliases for corpuses parsing and merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "# Setting global aliases\n",
    "concat_folder_alias = bau.FOLDER_NAMES['concat']\n",
    "corpus_folder_alias = bau.FOLDER_NAMES['corpus']\n",
    "rational_folder_alias = bau.FOLDER_NAMES['dedup']\n",
    "parsing_folder_alias = bau.FOLDER_NAMES['parsing']\n",
    "rawdata_folder_alias = bau.FOLDER_NAMES['rawdata']\n",
    "scopus_folder_alias = bau.FOLDER_NAMES['scopus']\n",
    "wos_folder_alias = bau.FOLDER_NAMES['wos']\n",
    "\n",
    "# Setting global regular expressions\n",
    "re_year = bau.RE_YEAR\n",
    "\n",
    "# Setting default values of inputs\n",
    "ref_files_check ='y' \n",
    "\n",
    "# Selecting corpus folder (year)\n",
    "corpusfiles_list =[file for file in os.listdir(corpuses_folder) if re_year.findall(file)]\n",
    "corpusfiles_list.sort()\n",
    "print('Please select the corpus via the tk window')\n",
    "myprojectname = bau.Select_multi_items(corpusfiles_list,'single',2)[0]+'/'\n",
    "project_folder = corpuses_folder / Path(myprojectname) / Path(corpus_folder_alias) \n",
    "print(bold_text + f'\\nThe corpus folder selected is: {project_folder}' + light_text)\n",
    "\n",
    "# Setting the useful paths\n",
    "path_scopus_parsing = project_folder / Path(scopus_folder_alias) / Path(parsing_folder_alias)\n",
    "path_scopus_rawdata = project_folder / Path(scopus_folder_alias) / Path(rawdata_folder_alias) \n",
    "path_wos_parsing = project_folder / Path(wos_folder_alias) / Path(parsing_folder_alias)\n",
    "path_wos_rawdata = project_folder / Path(wos_folder_alias) / Path(rawdata_folder_alias) \n",
    "path_concat = project_folder / Path(concat_folder_alias)\n",
    "path_concat_parsing = path_concat / Path(parsing_folder_alias)\n",
    "if not os.path.exists(path_concat_parsing):\n",
    "    if not os.path.exists(path_concat): os.mkdir(path_concat)\n",
    "    os.mkdir(path_concat_parsing)\n",
    "path_rational = project_folder / Path(rational_folder_alias)\n",
    "path_rational_parsing = path_rational / Path(parsing_folder_alias)\n",
    "if not os.path.exists(path_rational_parsing):\n",
    "    if not os.path.exists(path_rational): os.mkdir(path_rational)\n",
    "    os.mkdir(path_rational_parsing)\n",
    "\n",
    "# Setting the useful-paths lists     \n",
    "database_list = [(bau.WOS, path_wos_parsing, path_wos_rawdata), (bau.SCOPUS, path_scopus_parsing, path_scopus_rawdata)]\n",
    "useful_path_list = [path_scopus_parsing,path_wos_parsing,path_concat_parsing,path_rational_parsing] \n",
    "\n",
    "print('\\nUseful paths list built')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &emsp;&emsp;I-2 Corpuses parsing and merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "# Checking availibility of parsing for each of the corpuses to be merged\n",
    "parsing_files_check = input('\\n Parsings available for each corpus (y/n, default: n) ?')\n",
    "if parsing_files_check == 'n' or parsing_files_check == '':\n",
    "    # Get the folder for the general files\n",
    "    # and specific files for scopus type database in this folder\n",
    "    rep_package = Path('BiblioAnalysis_Utils')\n",
    "    rep_utils = Path(bau.REP_UTILS) \n",
    "    actual_folder = Path.cwd()\n",
    "    print('\\nWorking directory: ', actual_folder)\n",
    "    print('Default folder for the reference files: ', actual_folder / rep_package / rep_utils)  \n",
    "    \n",
    "    ref_files_check = input(' Are all reference files in this folder (y/n, default: y) ?')\n",
    "    if ref_files_check == 'y' or ref_files_check == '':\n",
    "        for database_tup in database_list:\n",
    "            database_type = database_tup[0]\n",
    "            database_parsing_path = database_tup[1]\n",
    "            database_rawdata_path = database_tup[2]\n",
    "                # Folder containing the wos or scopus file to process\n",
    "            in_dir_parsing = database_rawdata_path\n",
    "\n",
    "                # Folder containing the output files of the data parsing \n",
    "            out_dir_parsing = database_parsing_path \n",
    "            if not os.path.exists(out_dir_parsing):\n",
    "                os.mkdir(out_dir_parsing)\n",
    "\n",
    "            ## Running function biblio_parser\n",
    "            inst_filter_list_init = None\n",
    "            bau.biblio_parser(in_dir_parsing, out_dir_parsing, database_type, expert, rep_utils, inst_filter_list_init)\n",
    "\n",
    "            # Useful printings\n",
    "            with open(Path(out_dir_parsing) / Path(bau.PARSING_PERF), 'r') as failed_json:\n",
    "                    data_failed=failed_json.read()\n",
    "            dic_failed = json.loads(data_failed)\n",
    "            articles_number = dic_failed[\"number of article\"]\n",
    "            print('\\n' + bold_text + f'Parsing processed on full {database_type} corpus' + light_text)\n",
    "            print(\"\\n Success rates\")\n",
    "            del dic_failed['number of article']\n",
    "            for item, value in dic_failed.items():\n",
    "                print(f'    {item}: {value[\"success (%)\"]:.2f}%')\n",
    "    \n",
    "    else:\n",
    "        print('\\n' + bold_text + 'Please put all reference files in the specified folder and run again cell' + light_text)\n",
    "        \n",
    "if ref_files_check == 'y': \n",
    "    second_inst = input(\"\\n Secondary institutions to be parsed (y/n, default = y)? \")\n",
    "    if second_inst == '': second_inst = 'y' \n",
    "        \n",
    "    # Setting the specific affiliations filter     \n",
    "    if second_inst == 'y':   \n",
    "        inst_filter_list = bau.INST_FILTER_LIST\n",
    "        print(f' Default secondary institutions filter is: {inst_filter_list}' )\n",
    "        change_inst_filter = input(\" Do you want to change it (y/n, default = n)? \")\n",
    "        if change_inst_filter == '': change_inst_filter = 'n'\n",
    "        if change_inst_filter == 'y':\n",
    "            # Setting the specific affiliations filter \n",
    "            inst_filter_list = bau.setting_secondary_inst_filter(path_concat_parsing)\n",
    "    else:\n",
    "        inst_filter_list = None\n",
    "    \n",
    "    # Concatenating and deduplicating parsing saved in 'project_folder' folder\n",
    "    bau.parsing_concatenate_deduplicate(useful_path_list, inst_filter_list )\n",
    "    print(bold_text + '\\nCell-run completed' + light_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &emsp;&emsp;I-3 Building 'addressesinst.dat' file with institutions normalisation by address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "norm_raw_aff_dict =  bau.build_norm_raw_affiliations_dict(country_affiliations_file_path = None, verbose = False)\n",
    "print('\\nDict \"norm_raw_aff_dict\" is built') \n",
    "\n",
    "aff_type_dict = bau.read_inst_types(inst_types_file_path = None, inst_types_usecols = None)\n",
    "print('\\nDict \"aff_type_dict\" is built')\n",
    "\n",
    "# Setting the useful path\n",
    "path_parsing = useful_path_list[3] \n",
    "\n",
    "# Checking availibility of parsing from merged corpuses\n",
    "parsing_files_check = input('\\n Parsings available from merged corpuses (y/n, default: n) ?')\n",
    "if parsing_files_check == 'y':\n",
    "    print('\\nProcess in progress, please be patient!')\n",
    "    message = bau.build_addresses_institutions(path_parsing, norm_raw_aff_dict, aff_type_dict)\n",
    "    \n",
    "else:\n",
    "    message = 'WARNING :  need to run the cell \"II- Merging corpuses from different databases for single year corpus analysis\"'\n",
    "print('\\n'+ message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II- Single year corpus analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &emsp;&emsp;II-1 Selection of the corpus file for BiblioAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "## Selection of corpus file\n",
    "corpusfiles_list = os.listdir(corpuses_folder)\n",
    "corpusfiles_list.sort()\n",
    "print('Please select the corpus via the tk window')\n",
    "myprojectname = bau.Select_multi_items(corpusfiles_list,'single',2)[0]+'/'\n",
    "project_folder = corpuses_folder /Path(myprojectname)\n",
    "database_type = input('Corpus file type (scopus, wos - default: \"wos\")? ')\n",
    "if not database_type: database_type = 'wos' \n",
    "project_folder = corpuses_folder / Path(myprojectname) / Path(database_type)\n",
    "\n",
    " # Get the folder for the general files\n",
    " # and specific files for scopus type database in this folder\n",
    "rep_package = Path('BiblioAnalysis_Utils')\n",
    "rep_utils = Path(bau.REP_UTILS) \n",
    "actual_folder = Path.cwd()\n",
    "print('\\nWorking directory: ', actual_folder)\n",
    "print('Default folder for the reference files: ', actual_folder / rep_package / rep_utils)   \n",
    "ref_files_check = 'y'\n",
    "ref_files_check = input(' Are all reference files in this folder (y/n, default: y) ?')\n",
    "if ref_files_check == 'y':\n",
    "    ## Setting the  graph main heading\n",
    "    digits_list = list(filter(str.isdigit, myprojectname))\n",
    "    corpus_year = ''\n",
    "    for i in range(len(digits_list)):corpus_year = corpus_year + digits_list[i]\n",
    "    init = str(corpuses_folder).rfind(\"_\")+1\n",
    "    corpus_state = str(corpuses_folder)[init:]\n",
    "    main_heading = corpus_year + ' Corpus:' + corpus_state\n",
    "\n",
    "    ## Printing useful information\n",
    "    dict_print = {'Specific-paths set for user:': user_id,\n",
    "                  'Project folder:': project_folder,\n",
    "                  'Corpus year:': corpus_year,\n",
    "                  'Corpus status:': corpus_state,\n",
    "                  'Project name:': myprojectname,\n",
    "                  'Corpus file type:':database_type}\n",
    "\n",
    "    pad = 3\n",
    "    max_len_str = max( [len(str(x)) for x in dict_print.values()]) + pad\n",
    "    print('\\n')\n",
    "    for key,val in dict_print.items():\n",
    "        print(key.ljust(max_len_str),val)\n",
    "    print('\\n' + bold_text + 'Cell-run completed' + light_text)\n",
    "    \n",
    "else:\n",
    "    print('\\n' + bold_text +'Please put all reference files in the specified folder' + light_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &emsp;&emsp;II-2 Data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "## Building the names of the useful folders\n",
    "\n",
    "    # Folder containing the wos or scopus file to process\n",
    "in_dir_parsing = project_folder / Path(bau.FOLDER_NAMES['rawdata'])\n",
    "\n",
    "    # Folder containing the output files of the data parsing \n",
    "out_dir_parsing = project_folder / Path(bau.FOLDER_NAMES['parsing'])\n",
    "if not os.path.exists(out_dir_parsing):\n",
    "    os.mkdir(out_dir_parsing)\n",
    "\n",
    "## Running function biblio_parser\n",
    "parser_done = input(\"Parsing available (y/n)? \")\n",
    "if parser_done == \"n\":\n",
    "    inst_filter_list_init = None\n",
    "    bau.biblio_parser(in_dir_parsing, out_dir_parsing, database_type, expert, rep_utils, inst_filter_list_init)\n",
    "         \n",
    "    second_inst = input(\"Secondary institutions to be parsed (y/n)? \")\n",
    "    if second_inst=='y': \n",
    "        inst_filter_list = bau.INST_FILTER_LIST\n",
    "        print(f' Default secondary institutions filter is: {inst_filter_list}' )\n",
    "        change_inst_filter = input(\" Do you want to change it (y/n, default = n)? \")\n",
    "        if change_inst_filter == '': change_inst_filter = 'n'\n",
    "        if change_inst_filter == 'y':\n",
    "            # Setting the specific affiliations filter \n",
    "            inst_filter_list = bau.setting_secondary_inst_filter(out_dir_parsing)\n",
    "        # Extending the author with institutions parsing file\n",
    "        bau.extend_author_institutions(out_dir_parsing, inst_filter_list)\n",
    "    \n",
    "    # Useful printings\n",
    "    PARSING_PERF = 'failed.json'\n",
    "    with open(Path(out_dir_parsing) / Path(bau.PARSING_PERF), 'r') as failed_json:\n",
    "            data_failed=failed_json.read()\n",
    "    dic_failed = json.loads(data_failed)\n",
    "    articles_number = dic_failed[\"number of article\"]\n",
    "    print(\"Parsing processed on full corpus\")\n",
    "    print(\"\\n\\nSuccess rates\")\n",
    "    del dic_failed['number of article']\n",
    "    for item, value in dic_failed.items():\n",
    "        print(f'    {item}: {value[\"success (%)\"]:.2f}%')\n",
    "else:\n",
    "    second_inst = input(\"Secondary institutions to be parsed (y/n)? \")\n",
    "    if second_inst=='y' : \n",
    "        # Setting the specific affiliations filter \n",
    "        inst_filter_list = bau.setting_secondary_inst_filter(out_dir_parsing)\n",
    "        # Extending the author with institutions parsing file\n",
    "        bau.extend_author_institutions(out_dir_parsing, inst_filter_list)\n",
    "        \n",
    "    parser_filt = input(\"Parsing available without rawdata -from filtering- (y/n)? \")\n",
    "    if parser_filt == \"n\": \n",
    "        # Reading json file of parsing performances\n",
    "        with open(Path(out_dir_parsing) / Path(bau.PARSING_PERF), 'r') as failed_json:\n",
    "            data_failed=failed_json.read()\n",
    "        dic_failed = json.loads(data_failed)\n",
    "        articles_number = dic_failed[\"number of article\"]\n",
    "        # Usefull printings\n",
    "        print(\"Parsing available from full corpus\")\n",
    "        print(\"\\n\\nSuccess rates\")\n",
    "        del dic_failed['number of article']\n",
    "        for item, value in dic_failed.items():\n",
    "            print(f'    {item}: {value[\"success (%)\"]:.2f}%')\n",
    "    else:\n",
    "        #clear_output(wait=True)\n",
    "        print(\"Parsing available from filtered corpus without rawdata\")\n",
    "        file = project_folder /Path('parsing/' + 'articles.dat')\n",
    "        with open(file) as f:\n",
    "            lines = f.readlines()\n",
    "        articles_number = len(lines)\n",
    "\n",
    "print(\"\\n\\nCorpus parsing saved in folder:\\n\", str(out_dir_parsing))\n",
    "print('\\nNumber of articles in the corpus : ', articles_number)\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  &emsp;&emsp;II-2.1 Data parsing / Corpus description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "## Building the names of the useful folders\n",
    "\n",
    "    # Folder containing the wos or scopus parsed files\n",
    "in_dir_corpus = out_dir_parsing\n",
    "\n",
    "    # Folder containing the wos or scopus parsed and analysed files\n",
    "out_dir_corpus = project_folder / Path(bau.FOLDER_NAMES['description'])\n",
    "if not os.path.exists(out_dir_corpus):\n",
    "    os.mkdir(out_dir_corpus)    \n",
    "\n",
    "## Running describe_corpus\n",
    "description_done = input(\"Description available (y/n)? \")\n",
    "#clear_output(wait=True)\n",
    "if description_done == \"n\":\n",
    "    verbose = False\n",
    "    bau.describe_corpus(in_dir_corpus, out_dir_corpus, database_type, verbose)\n",
    "    print(\"Corpus description saved in folder:\", str(out_dir_corpus))\n",
    "else:\n",
    "    print(\"Corpus description available in folder:\", str(out_dir_corpus))\n",
    "\n",
    "# Building the name of file for histogram plot of an item\n",
    "fullpath_distrib_item = out_dir_corpus / Path(bau.DISTRIBS_ITEM_FILE)\n",
    "\n",
    "## Running plot of treemap, scatter plot and histogram for a selected item_treemap\n",
    "do_treemap = input(\"Treemap for an item of the corpus description (y/n)? \")\n",
    "if do_treemap == 'y':\n",
    "    renew_treemap = 'y'\n",
    "    while renew_treemap == 'y' :\n",
    "        print(\"Choose the item for treemap in the tk window\")\n",
    "        item_treemap = bau.treemap_item_selection()\n",
    "        fullpath_file_treemap = out_dir_corpus / Path('freq_'+ item_treemap +'.dat')\n",
    "        print(\"Item selected:\",item_treemap)\n",
    "        bau.treemap_item(item_treemap, fullpath_file_treemap)\n",
    "        do_scatter = input(\"Scatter plot for the item (y/n)? \")\n",
    "        if do_scatter == 'y':\n",
    "            bau.plot_counts(item_treemap, fullpath_file_treemap)\n",
    "        do_histo = input(\"Histogram plot for the item (y/n)? \")\n",
    "        if do_histo == 'y':\n",
    "            bau.plot_histo(item_treemap, fullpath_distrib_item)\n",
    "        renew_treemap = input(\"\\n\\nTreemap for a new item (y/n)? \")\n",
    "\n",
    "# Initialize the variable G_coupl that will receive the biblioanalysis coupling graphs\n",
    "try: G_coupl\n",
    "except NameError: G_coupl = None\n",
    "\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &emsp;&emsp;II-2.1.1 Data parsing / Corpus description / Filtering the data and filtered corpus description\n",
    "To be run after corpus description to allow using the following functions : describe_corpus() , treemap_item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil                      \n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "## Recursive filtering\n",
    "\n",
    "# Allows prints in filter_corpus_new function\n",
    "verbose = False\n",
    "\n",
    "# Initialization of parameters for recursive filtering\n",
    "filtering_step = 1\n",
    "while True:\n",
    "\n",
    "    ## Building the names of the useful folders and creating the output folder if not find \n",
    "    if filtering_step == 1:\n",
    "        in_dir_filter = out_dir_parsing\n",
    "        ### Get the folder for the filter configuration file         \n",
    "        gui_titles = {'main':   'Folder selection GUI for config_filters.json file ',\n",
    "                      'result': 'Selected folder'}\n",
    "        gui_buttons = ['SELECTION','HELP']\n",
    "        filter_config_folder = bau.select_folder_gui_new(user_root, gui_titles, gui_buttons, bau.GUI_DISP,\n",
    "                                                         widget_ratio=1, button_ratio=1, \n",
    "                                                         max_lines_nb=3)\n",
    "        \n",
    "        print('Filter configuration folder:', filter_config_folder)\n",
    "        file_config_filters = filter_config_folder / Path('config_filters.json')\n",
    "        print('Filter configuration file:',file_config_filters)\n",
    "        modif_filtering = input(\"Modification of item-values list from a predefined file (y/n)? \")\n",
    "        if modif_filtering == \"y\":\n",
    "            bau.filters_modification(filter_config_folder,file_config_filters)    \n",
    "    else:\n",
    "        renew_filtering = input(\"Apply a new filtering process (y/n)? \") \n",
    "        if renew_filtering == \"n\": break\n",
    "        in_dir_filter = project_folder / Path(bau.FOLDER_NAMES['filtering'] + '_' + str(filtering_step-1))\n",
    "        file_config_filters = in_dir_filter / Path('save_config_filters.json')\n",
    "        print('Filter configuration file:',file_config_filters) \n",
    "        \n",
    "    out_dir_filter = project_folder / Path(bau.FOLDER_NAMES['filtering'] + '_' + str(filtering_step))\n",
    "            \n",
    "    if not os.path.exists(out_dir_filter):\n",
    "        os.mkdir(out_dir_filter)\n",
    "    else:\n",
    "        print('out_dir_filter exists')\n",
    "        files = glob.glob(str(out_dir_filter) + '/*.*')\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "\n",
    "    # Building the absolute file name of filter configuration file to save for the filtering step\n",
    "    save_config_filters = out_dir_filter / Path(bau.SAVE_CONFIG_FILTERS)\n",
    "    print('\\nSaving filter configuration file:',save_config_filters)\n",
    "    \n",
    "    # Configurating the filtering through a dedicated GUI or getting it from the existing file\n",
    "    bau.filters_selection(file_config_filters,save_config_filters,in_dir_filter,\n",
    "                         fact=3, win_widthmm=85, win_heightmm=115, font_size=16)\n",
    "    shutil.copyfile(save_config_filters, file_config_filters)\n",
    "\n",
    "    # Read the filtering status\n",
    "    combine,exclusion,filter_param = bau.read_config_filters(file_config_filters) \n",
    "    print(\"\\nFiltering status:\")\n",
    "    print(\"   Combine   :\",combine)\n",
    "    print(\"   Exclusion :\",exclusion)\n",
    "    for key,value in filter_param.items():\n",
    "        print(f\"   Item      : {key}\\n   Values    : {value}\\n\")\n",
    "\n",
    "    # Running function filter_corpus_new\n",
    "    bau.filter_corpus_new(in_dir_filter, out_dir_filter, verbose, file_config_filters) # <---???\n",
    "    file = out_dir_filter /Path('articles.dat')\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        articles_number = len(lines)\n",
    "    if articles_number == 0:\n",
    "        print('Filtered corpus empty !')\n",
    "        break\n",
    "    print(\"Filtered-corpus parsing saved in folder \", \n",
    "            str(out_dir_filter),\n",
    "            \" with the corresponding filters configuration\")\n",
    "\n",
    "        # Folder containing the wos or scopus parsed and filtered files\n",
    "    in_dir_freq_filt = out_dir_filter\n",
    "\n",
    "        # Folder containing the wos or scopus parsed, filtered and analysed files\n",
    "    out_dir_freq_filt = project_folder / Path(bau.FOLDER_NAMES['description'] + '_' + str(filtering_step))\n",
    "    if not os.path.exists(out_dir_freq_filt): os.mkdir(out_dir_freq_filt)\n",
    "\n",
    "        # Running describe_corpus \n",
    "    verbose = False\n",
    "    bau.describe_corpus(in_dir_freq_filt, out_dir_freq_filt, database_type, verbose)\n",
    "    print(\"Filtered corpus description saved in folder:\", str(out_dir_freq_filt))\n",
    "\n",
    "    # Treemap plot by a corpus item after filtering\n",
    "    make_treemap = 'n'\n",
    "    make_treemap = input(\"\\n\\nDraw treemap (y/n)?\")\n",
    "    if make_treemap == 'y' :\n",
    "\n",
    "            # Running plot of treemap for selected item_treemap\n",
    "        renew_treemap = 'y'    \n",
    "        while renew_treemap == 'y' :\n",
    "            print('\\n\\nChoose the item for treemap of the filtered corpus description in the tk window')\n",
    "            item_treemap = bau.treemap_item_selection()\n",
    "            file_name_treemap = project_folder / Path(bau.FOLDER_NAMES['description'] + '_'\\\n",
    "                                                      + str(filtering_step) + '/' + 'freq_'+ item_treemap +'.dat')\n",
    "            print(\"Item selected:\",item_treemap)\n",
    "            bau.treemap_item(item_treemap, file_name_treemap)\n",
    "            renew_treemap = input(\"\\n\\nTreemap for a new item (y/n)? \") \n",
    "\n",
    "    filtering_step += 1\n",
    "\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &emsp;&emsp;II-2.1.2 Data parsing / Corpus Description / Bibliographic Coupling analysis\n",
    "To be run after corpus description to use the frequency analysis. You may execute the bibliographic coupling script several times successively on unfiltered corpus and on available filtering steps of the corpus.\n",
    "The result files are saved in independant folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "# Building the names of the useful folders and creating the output folder if not find  \n",
    "filtering = input(\n",
    "                  \"Corpus filtered (y/n)? \"\n",
    "                 )   \n",
    "if filtering == \"y\":\n",
    "    filtering_step = input(\n",
    "                            \"Enter filtering step : \"\n",
    "                          ) \n",
    "    in_dir_coupling = project_folder / Path(bau.FOLDER_NAMES['filtering'] + '_' + str(filtering_step))\n",
    "    in_dir_freq= project_folder / Path(bau.FOLDER_NAMES['description'] + '_' + str(filtering_step))\n",
    "    out_dir_coupling = project_folder / Path(bau.FOLDER_NAMES['coupling'] + '_' + str(filtering_step))\n",
    "else:\n",
    "    in_dir_coupling = out_dir_parsing\n",
    "    in_dir_freq= out_dir_corpus    \n",
    "    out_dir_coupling = project_folder / Path(bau.FOLDER_NAMES['coupling'])\n",
    "\n",
    "if not os.path.exists(out_dir_coupling):\n",
    "    os.mkdir(out_dir_coupling)\n",
    "else:\n",
    "    print('out_dir_coupling exists')\n",
    "    files = glob.glob(str(out_dir_coupling) + '/*.html')\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "    \n",
    "# Building the coupling graph of the corpus\n",
    "print('Building the coupling graph of the corpus, please wait...')\n",
    "G_coupl = bau.build_coupling_graph(in_dir_coupling)\n",
    "\n",
    "# Building the partition of the corpus\n",
    "print('Building the partition of the corpus, please wait...')\n",
    "G_coupl,partition = bau.build_louvain_partition(G_coupl)\n",
    "print()\n",
    "\n",
    "# Adding attributes to the coupling graph nodes\n",
    "attr_dic = {}\n",
    "add_attrib = input(\"Add attributes to the coupling graph nodes (y/n)? \")\n",
    "if add_attrib == 'y':\n",
    "    while True:\n",
    "        print('\\n\\nChoose the item for the attributes to add in the tk window')\n",
    "        item, m_max_attrs = bau.coupling_attr_selection(fact=2, win_widthmm=80, win_heightmm=60, font_size=16)\n",
    "        attr_dic[item] = m_max_attrs\n",
    "        print(\"Item selected:\",item,\" with \",m_max_attrs, \" attributes\" )\n",
    "        G_coupl = bau.add_item_attribute(G_coupl, item, m_max_attrs, in_dir_freq, in_dir_coupling)\n",
    "        renew_attrib = input(\"\\nAdd attributes for a new item (y/n)?\") \n",
    "        if renew_attrib == 'n' : break      \n",
    "\n",
    "# Plot control of the coupling graph before using Gephy\n",
    "NODES_NUMBER_MAX = 1\n",
    "bau.plot_coupling_graph(G_coupl,partition,nodes_number_max=NODES_NUMBER_MAX)\n",
    "\n",
    "# Creating a Gephy file of the coupling graph  \n",
    "bau.save_graph_gexf(G_coupl,out_dir_coupling)\n",
    "print(\"\\nCoupling analysis of the corpus saved as Gephy file in folder:\\n\", str(out_dir_coupling))\n",
    "\n",
    "# Creating an EXCEL file of the coupling analysis results\n",
    "bau.save_communities_xls(partition,in_dir_coupling,out_dir_coupling)\n",
    "print(\"\\nCoupling analysis of the corpus saved as EXCEL file in folder:\\n\", str(out_dir_coupling))\n",
    "\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### &emsp;&emsp;II-2.1.3  HTML graph of coupling analysis \n",
    "##### after Data parsing / Corpus Description / Coupling analysis  \n",
    "You may execute the HTML graph construction script several times successively on the available coupling graph of the corpus. The result files are saved in the corresponding coupling floder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Creating html file of graph G using pyviz\n",
    "   This script uses the results of the Biblioanalysis coupling analysis:\n",
    "   - out_dir_coupling (Path): path for saving the coupling analysis results;\n",
    "   - G (networkx object): coupling graph with added attributes;\n",
    "   - partition (dict):  partition of graph G;\n",
    "   - attr_dic (dict): dict of added attributes with number of added values. \n",
    "   \n",
    "'''\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "# Checking the availability of the corpus coupling graph G with all attributes and its partition\n",
    "assert(G_coupl is not None),'''Please run first the \"Bibliographic coupling analysis\" \n",
    "                                script to build the coupling graph'''\n",
    "\n",
    "# Setting the item label among the added attribute to be colored\n",
    "colored_attr = input('Please enter the item label among the added attributes to be colored (default: S)')\n",
    "if colored_attr == '':colored_attr = 'S'\n",
    "print('Attribute to be colored:',colored_attr)\n",
    "if colored_attr == 'S': \n",
    "    heading3 = 'Colored by main discipline (grey: without filtering subjects as main discipline).'\n",
    "else:\n",
    "    heading3 = 'Colored by main attribute values (grey: without filtering attribute values as main discipline).'\n",
    "assert(colored_attr in attr_dic.keys()),\\\n",
    "    f'''Selected colored attribute should be among the added attributes: {list(attr_dic.keys())}.\n",
    "Please run this script again to select an effectivelly added attribute to the coupling graph node \n",
    "or run again the \"Bibliographic coupling analysis\" script to add the targetted attribute to the coupling graph.'''\n",
    "\n",
    "# Setting the colors for the values of the attribute to be colored\n",
    "# default: values of 'S' item from a particular corpus\n",
    "# TO DO: define the list of the attribute values through a GUI\n",
    "colored_attr_values = {'Neurosciences & Neurology':'0',\n",
    "                  'Psychology':'1',\n",
    "                  'Computer Science':'2',\n",
    "                  'Robotics,Automation & Control Systems':'3',\n",
    "                  'Life Sciences & Biomedicine - Other Topics':'4',\n",
    "                  'Biochemistry & Molecular Biology':'4',\n",
    "                  'Cell Biology':'4',\n",
    "                  'Evolutionary Biology':'4',\n",
    "                  'Biomedical Social Sciences':'4',\n",
    "                  'Biotechnology & Applied Microbiology':'4',\n",
    "                  'Developmental Biology':'4',\n",
    "                  'Microbiology':'4',\n",
    "                  'Marine & Freshwater Biology':'4',\n",
    "                  'Reproductive Biology':'4',\n",
    "                  'Genetics & Heredity':'4',\n",
    "                  'Philosophy':'5',\n",
    "                  'History & Philosophy of Science':'5',\n",
    "                  'Social Sciences - Other Topics':'6',\n",
    "                  'Mathematical Methods In Social Sciences':'6',\n",
    "                  'Linguistics':'7',\n",
    "                  'Anthropology':'8',\n",
    "                 }\n",
    "\n",
    "# Setting the attribute value to be specifically shaped\n",
    "shaped_attr = input('Please enter the added attribute value to be specifically shaped (default: Psychology)')\n",
    "if shaped_attr == '':shaped_attr = 'Psychology'\n",
    "print('Attribute value to be specifically shaped (triangle):',shaped_attr)\n",
    "heading4 = 'Triangles for \"' + shaped_attr + '\" in disciplines.'\n",
    "\n",
    "# Computing the number of communities\n",
    "community_number = len(set(partition.values()))\n",
    "print('Number of communities:',community_number)\n",
    "\n",
    "# Computing the size of the communities\n",
    "communities_size = {}\n",
    "for value in set(partition.values()):\n",
    "    communities_size[value]=0\n",
    "    for key in set(partition.keys()):\n",
    "        if partition[key] == value:\n",
    "            communities_size[value]+=1\n",
    "            \n",
    "# Building the html graphs per community\n",
    "for community_id in range(community_number):\n",
    "    community_size = communities_size[community_id] \n",
    "    heading2 = 'Coupling graph for community ID: ' + str(community_id) + ' Size: ' + str(community_size)\n",
    "    heading = '<h1>' + main_heading + '</h1>' + '<h2>' + heading2 + '</h2>' \\\n",
    "                  + '<h3 align=left nowrap>' + heading3 + '<br>'  + heading4 + '</h3>'\n",
    "    html_file= str(out_dir_coupling /Path('coupling_' + 'com' + str(community_id) \\\n",
    "                                          + '_size' + str(community_size) + '.html'))\n",
    "    #bau.coupling_graph_html_plot(G_coupl,html_file,community_id,attr_dic,colored_attr,\n",
    "    #                             colored_attr_values,shaped_attr,nodes_colors,edges_color,\n",
    "    #                             background_color,font_color,heading)\n",
    "    bau.coupling_graph_html_nwplt(G_coupl,html_file,community_id,attr_dic,colored_attr,\n",
    "                                  colored_attr_values,shaped_attr,heading)\n",
    "# Building the html graph for the full corpus\n",
    "heading2  = ' All ' + str(community_number) + ' communities'\n",
    "heading = '<h1>' + main_heading + '</h1>' + '<h2>' + heading2 + '</h2>' \\\n",
    "          + '<h3 align=left nowrap>' + heading3 + '<br>'  + heading4 + '</h3>'\n",
    "html_file= str(out_dir_coupling /Path('coupling_' + 'all.html'))\n",
    "#bau.coupling_graph_html_plot(G_coupl,html_file,'all',attr_dic,colored_attr,\n",
    "#                         colored_attr_values,shaped_attr,nodes_colors,edges_color,\n",
    "#                         background_color,font_color,heading)\n",
    "bau.coupling_graph_html_nwplt(G_coupl,html_file,'all',attr_dic,colored_attr,\n",
    "                              colored_attr_values,shaped_attr,heading)\n",
    "\n",
    "print(\"\\nCreated html files of graph G_coupl using pyviz for the corpus in folder:\\n\", str(out_dir_coupling))\n",
    "\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;&emsp;II-2.2 Data parsing / Co-occurrence Maps\n",
    "You may execute the co-occurence script several times successively on unfiltered corpus and on available filtering steps of the corpus.\n",
    "The result files are saved in independant folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "# Building the names of the useful folders and creating the output folder if not find \n",
    "filtering = input(\n",
    "                  \"Corpus filtered (y/n)? \"\n",
    "                 )   \n",
    "if filtering == \"y\":\n",
    "    filtering_step = input(\n",
    "                            \"Enter filtering step : \"\n",
    "                          ) \n",
    "    in_dir_cooc = project_folder / Path(bau.FOLDER_NAMES['filtering'] + '_' + str(filtering_step))\n",
    "    out_dir_cooc = project_folder / Path(bau.FOLDER_NAMES['cooccurrence'] + '_' + str(filtering_step))\n",
    "else:\n",
    "    in_dir_cooc = out_dir_parsing   \n",
    "    out_dir_cooc = project_folder / Path(bau.FOLDER_NAMES['cooccurrence']) \n",
    "\n",
    "if not os.path.exists(out_dir_cooc):\n",
    "    os.mkdir(out_dir_cooc)\n",
    "else:\n",
    "    print('out_dir_cooc available')\n",
    "\n",
    "## Building the co-ocurrence graph\n",
    "size_min = 1\n",
    "node_size_ref=300\n",
    "while True :\n",
    "    print('\\n\\nChoose the item for co-occurrence analysis in the tk window')\n",
    "    cooc_item, size_min = bau.cooc_selection(fact=3, win_widthmm=80, win_heightmm=100, font_size=16) \n",
    "    print(\"Item selected:\",cooc_item,\" at minimum size \",size_min)\n",
    "    out_dir_cooc_item = out_dir_cooc / Path('cooc_' + cooc_item + \\\n",
    "                                            '_thr' + str(size_min))\n",
    "    if not os.path.exists(out_dir_cooc_item):\n",
    "        os.mkdir(out_dir_cooc_item)\n",
    "    else:\n",
    "        print('out_dir_cooc_item available')\n",
    "    G_cooc = bau.build_item_cooc(cooc_item,in_dir_cooc, out_dir_cooc_item, size_min = size_min)\n",
    "    if G_cooc is None:\n",
    "        print(f'The minimum node size ({size_min}) is two large. Relax this constraint.')\n",
    "    else:\n",
    "        print(\"Co-occurrence analysis of the corpus for item \" + cooc_item + \\\n",
    "          \" saved in folder:\", str(out_dir_cooc_item))\n",
    "        heading2 = 'Co_occurence graph for item ' + cooc_item + ' with minimum node size ' + str(size_min)\n",
    "        heading3 = 'Bold node title: Node attributes[number of item value occurrences-item value (total number of edges)]'\n",
    "        heading4 = 'Light node titles: Neighbors attributes[number of item value occurrences-item value (number of edges with node)]'\n",
    "        heading = '<h1>' + main_heading + '</h1>' + '<h2>' + heading2 + '</h2>' \\\n",
    "                  + '<h3 align=left nowrap>' + heading3 + '<br>'  + heading4 + '</h3>'\n",
    "    \n",
    "        bau.plot_cooc_graph(G_cooc,cooc_item,size_min=size_min,node_size_ref=node_size_ref)\n",
    "        # Creating html file of graph G_cooc using pyviz\n",
    "        html_file= str(out_dir_cooc_item /Path('cooc_' + cooc_item + '_thr' + str(size_min) + '.html'))\n",
    "        bau.cooc_graph_html_plot(G_cooc,html_file,heading)\n",
    "        print(\"Created html file of\",cooc_item,\"co-occurrence graph using pyviz in folder:\\n\",\\\n",
    "              str(out_dir_cooc_item))\n",
    "        \n",
    "    renew_cooc = input(\"\\n\\nCo-occurrence analysis for a new item (y/n)?\") \n",
    "    if renew_cooc == 'n' : break\n",
    "\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III- Temporal development of item values weight\n",
    "To run this cell a set of annual corpuses with their description should be available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "# Initialize the search configuration dict \n",
    "keyword_filters = {\n",
    "    'is_in':[],    \n",
    "    'is_equal':[]}\n",
    "\n",
    "## Get the folder for the configuration file for the temporal development analysis \n",
    "# To Do : use the new gui\n",
    "temporaldev_config_folder = bau.select_folder_gui(user_root,'Select the folder for config_temporal.json file')\n",
    "print('Item_values selection folder:', temporaldev_config_folder )\n",
    "\n",
    "## Building the search configuration:\n",
    "#### - either by reading of the 'config_temporal.json' without modification\n",
    "#### - or by an interactive modification of the configuration and storing it in this file for a futher use\n",
    "TemporalDev_file = temporaldev_config_folder / Path('config_temporal.json')\n",
    "\n",
    "keywords_modif = input('Modification of the keywords list (y/n)?')\n",
    "if keywords_modif == 'y':\n",
    "    \n",
    "        # Selection of items\n",
    "    items_full_list = ['IK','AK','TK','S','S2']\n",
    "    print('\\nPlease select the items to be analyzed via the tk window')\n",
    "    items = bau.Select_multi_items(items_full_list,'multiple')\n",
    "\n",
    "        # Selection of the folder of item-values full-list file\n",
    "        # To Do : use the new gui\n",
    "    select_folder = bau.select_folder_gui(user_root,'Select the folder of the item-values list files')\n",
    "\n",
    "        # Setting the file of the item-values full list  \n",
    "    keywords_full_list_file = select_folder / Path('TempDevK_full.txt')\n",
    "    \n",
    "        # Setting the list of item-values full list\n",
    "    keywords_full_list = bau.item_values_list(keywords_full_list_file)\n",
    "    \n",
    "        # Selection of the item-values list to be put in the temporal development configuration file \n",
    "    search_modes = ['is_in','is_equal']\n",
    "    for search_mode in search_modes:\n",
    "        print('\\nPlease select the keywords for ',search_mode, ' via the tk window')\n",
    "        keyword_filters[search_mode] = bau.Select_multi_items(keywords_full_list,mode = 'multiple')\n",
    "        \n",
    "    # Saving the new configuration in the 'config_temporal.json' file   \n",
    "    bau.write_config_temporaldev(TemporalDev_file,items,keyword_filters)\n",
    "    print('\\n New temporal development configuration saved in: \\n', TemporalDev_file)    \n",
    "else:\n",
    "    # Reading the search configuration from the 'config_temporal.json' file  \n",
    "    items,keywords_param = bau.read_config_temporaldev(TemporalDev_file)\n",
    "    print('Selection of items:\\n',items)    \n",
    "    keyword_filters['is_in'] = keywords_param['is_in']\n",
    "    keyword_filters['is_equal'] = keywords_param['is_equal']\n",
    "\n",
    "## Selection of annual corpus files\n",
    "corpusfiles_list = os.listdir(corpuses_folder)\n",
    "corpusfiles_list.sort()\n",
    "print('\\nPlease select the corpuses to be analyzed via the tk window')\n",
    "years = bau.Select_multi_items(corpusfiles_list,'multiple')\n",
    "\n",
    "# Print configuration\n",
    "print('Search items:', items)\n",
    "print('\\nSearch Words:\\n' + json.dumps(keyword_filters, indent=2))\n",
    "print('\\n Selection of annual corpus files:\\n',years, '\\n')\n",
    "\n",
    "# Performing the search using the keyword_filters dict\n",
    "keyword_filter_list = bau.temporaldev_itemvalues_freq(keyword_filters ,items, years, corpuses_folder)\n",
    "\n",
    "# Saving the search results in an EXCEL file\n",
    "store_file = corpuses_folder / Path('Results_Files/TempDev_synthesis.xlsx')\n",
    "bau.temporaldev_result_toxlsx(keyword_filter_list,store_file)\n",
    "print('\\nTemporal development results saved in:\\n', store_file) \n",
    "\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annexe 1- Databases merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "database, filename, in_dir, out_dir = bau.merge_database_gui()\n",
    "bau.merge_database(database,filename,in_dir,out_dir)\n",
    "\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annexe 2- Item values selection to list for filters configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from pathlib import Path\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "# Get the folder for the filter configuration file \n",
    "# To Do : use the new gui\n",
    "filter_config_folder = bau.select_folder_gui(user_root,'Select the folder for the config_filters.json file')\n",
    "print('Filter configuration folder:', filter_config_folder) \n",
    "\n",
    "file_config_filters = filter_config_folder/ Path('config_filters.json')    \n",
    "bau.filters_modification(filter_config_folder,file_config_filters)\n",
    "\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annexe 3- Upgrade of parsing files with column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "# Get the folder for the filter configuration file\n",
    "# To Do : use the new gui\n",
    "corpus_folder_to_upgrade = bau.select_folder_gui(user_root,'Select the corpus folder to upgrade')\n",
    "print('Corpus folder to upgrade:', corpus_folder_to_upgrade) \n",
    "bau.upgrade_col_names(corpus_folder_to_upgrade)\n",
    "\n",
    "print('\\n' + bold_text + 'Cell-run completed' + light_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data parsing\n",
    "- articles.dat is the central file, listing all the publications within the corpus. It contains informations such as the document type (article, letter, review, conf proceeding, etc), title, year of publication, publication source, doi, number of citations (given by WOS or Scopus at the time of the extraction) AND a unique identifier used in all the other files to identify a precise publication.\n",
    "- database.dat keeps track of the origin of the data, some part of the analysis being specific to WOS or Scopus data.\n",
    "- authors.dat lists all authors names associated to all publications ID.\n",
    "- addresses.dat lists all adresses associated to all publications ID, along with a specific ID for each adresse line. These adresses are reported as they appear in the raw data, without any further processing.\n",
    "- countries.dat lists all countries associated to all publications ID and adresses lines ID. The countries are extracted from the adresses fields of the raw data, with some cleaning (changing mentions of US states and UK countries to respectively the USA and UK).\n",
    "- institutions.dat lists all the comma-separated entities appearing in the adresses field associated to all publications ID and adresses lines ID, except those refering to a physical adresses. These entities correspond to various name variants of universities, organisms, hospitals, labs, services, departments, etc as they appear in the raw data. No treatment is made to e.g. filtering out the entities corresponding a given hierarchy level.\n",
    "- keywords.dat lists various types of keywords associated to all publications ID. \"AK\" keywords correspond to Author's keywords. \"IK\" keywords correspond to either WOS or Scopus keywords, which are built based on the authors' keywords, the title and abstract. \"TK\" correspond to title words (from which we simply remove common words and stop words - no stemming is performed). TK are especially useful when studying pre-90's publications, when the use of keywords was not yet standard.\n",
    "- references.dat lists all the references associated to all publications ID. The rawdata is parsed to store the first author name, title, source, volume and page of each reference of the raw \"references\" field.\n",
    "- subjects.dat lists all subject categories associated to all publications ID (a journal may be associated to many subject category). WOS classifies the sources it indexes into ∼ 250 categories, that are reported in the extracted data. Scopus classifies its sources into 27 major categories and ∼ 300 sub-categories, none of which are reported in the extracted data. We use Elsevier Source Title List (october 2017 version) to retrieve that information. The \"subject.dat\" contains the info relative to the major categories.\n",
    "- subjects2.dat lists Scopus's sub-categories, if the use database is Scopus.\n",
    "- AA_log.txt keeps track of the date/time the script was executed and of all the messages displayed on the terminal (number of publications extracted, % of references rejected, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Corpus description\n",
    "Before doing anything else, you should get a general idea of the content of your database.  This script performs several basic tasks:\n",
    "- it performs a series of frequency analysis, computing the number of occurrences of each item (authors, keywords, references, etc) within the publications of the corpus. These frequencies are automatically stored into several \"freq_xxx.dat\" files within a newly created \"freq\" folder.\n",
    "- it performs a series of generic statistical analysis, storing the numbers of distinct items of each type (e.g. there are x distinct keyword in the corpus ), the distributions of number of occurrences of each item (e.g. there are x keywords appearing in at least y publications) and the distribution of number of items per publication (e.g.there are x% of publications with y keywords). All these statistics are stored in the \"DISTRIBS_itemuse.json\" file.\n",
    "- it also performs a co-occurrence analysis, computing the number of co-occurrence of pairs of items among the top 100 most frequent items of each type (e.g. computing how often the two most used keywords appear together in the same publications). The results of this analysis are stored in the \"coocnetworks.json\" file. More systematic co-occurrence analysis can also be performed with another script, cf the Co-occurrence Maps section below.\n",
    "All the generated files can be opened and read with a simple text editor. The freq_xxx.dat, listing items by order of frequency, can also be read in a spreadsheet software such as excel. All the files are however primarily made to be read in the BiblioMaps interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the data\n",
    "#### To be run after corpus description to allow using the following functions : describe_corpus() , treemap_item()\n",
    "\n",
    "If, upon exploring the nature of the data you realize that before going further you'd prefer to filter your corpus based on some characteristic (keeping only the publications from certain years, using some keywords or references, written by some authors from some countries, etc), you can filter the initial corpus thanks to the script:\n",
    "\n",
    "- python BiblioTools3.2/filter.py -i myprojectname/ -o myprojectname_filtered -v <br>\n",
    "\n",
    "Edit the 'filter.py' file to specify your filters. You'll also need to create a new \"myprojectname_filtered\" main folder before running the script.\n",
    "- create the files articles.dat, addresses.dat, authors.dat, countries.dat, institutions.dat, keywords.dat, references.dat, subjects.dat, subjects2.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurrence Maps\n",
    "You may execute the co-occurrence script several times successively on unfiltered corpus and on available filtering steps of the corpus.\n",
    "The result files are saved in independant folders.\n",
    "\n",
    "The script create multiple co-occurrence networks, all stored in gdf and gexf files that can be opened in Gephi, among which:\n",
    "\n",
    "Example of heterogeneous network generated with BiblioAnlysis and visualized in Gephi.\n",
    "\n",
    "- a co-cocitation network, linking references that are cited in the same publications.\n",
    "- a co-refsources network, linking references's sources that are cited in the same publications.\n",
    "- a co-author network, linking authors that collaborated in some publications.\n",
    "- a co-country network, linking countries with researchers that collaborated in some publications.\n",
    "- a co-institution network, linking institutions with researchers that collaborated in some publications. For this network to be fully useful, you may want to spend some time cleaning the \"institutions.dat\", e.g. by keeping only the big institutions (university level) or by replacing minor name variant by the dominant name variant (\"Ecole Normale Supérieure de Lyon\" → \"ENS Lyon\")\n",
    "- a co-keyword network, linking keywords being co-used in some publications. Be careful about the interpretation: keywords can be polysemic, their meaning differing from field to another (eg \"model\", \"energy\", \"evolution\", etc)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BiblioAnalysis_kernel",
   "language": "python",
   "name": "biblioanalysis_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
