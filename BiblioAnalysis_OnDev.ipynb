{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiblioAnalysis_OnDev\n",
    "\n",
    "### Version: 0.0.0\n",
    "\n",
    "### Aims\n",
    "- This jupyter notebook results from the use analysis of BiblioTools2jupyter notebook and a new implementation of the following parts:\n",
    "    - Parsing: replaced and tested \n",
    "    - Corpus description: replaced and tested\n",
    "    - Filtering: replaced and tested, integrating the \"EXCLUSION\" mode and the recursive filtering\n",
    "    - Cooccurrence analysis : replaced and tested, integrating graph plot and countries GPS coordinates\n",
    "    - Coupling analyis : replaced and tested\n",
    "    \n",
    "### Created modules in the package BiblioAnalysis_Utils\n",
    "    - BiblioParser.py\n",
    "    - BiblioDescription.py\n",
    "    - BiblioFilter.py\n",
    "    - BiblioCooc.py\n",
    "    - BiblioCoupling.py\n",
    "    - GUI_utils.py\n",
    "\n",
    "### BiblioTool3.2 source\n",
    "http://www.sebastian-grauwin.com/bibliomaps/download.html \n",
    "\n",
    "### List of initial Python packages extracted from  BiblioTool3.2\n",
    "- biblio_parser.py\t⇒ pre-processes WOS / Scopus data files,\n",
    "- corpus_description.py\t⇒ performs a frequency analysis of the items in corpus,\n",
    "- filter.py\t⇒ filters the corpus according to a range of potential queries but still too specific\n",
    "- biblio_coupling.py\t⇒ performs a BC anaysis of the corpus,\n",
    "- cooc_graphs.py\t⇒ produces various co-occurrence graphs based on the corpus (call parameters changed)\n",
    "\n",
    "### Specifically required list of pip install \n",
    "- !pip3 install squarify \n",
    "- !pip3 install inquirer\n",
    "- !pip3 install python-louvain\n",
    "- !pip3 install pyvis\n",
    "\n",
    "### Specifically required nltk downloads\n",
    "- import nltk\n",
    "    - nltk.download('punkt')\n",
    "    - nltk.download('averaged_perceptron_tagger')\n",
    "    - nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary instructions\n",
    "#### These actions will be interactively performed in the next version of the Jupyter notebook\n",
    "- Create the 'BiblioAnalysis_Files/' folder in your 'Users/' folder\n",
    "<br>\n",
    "<br>\n",
    "- Create in this 'BiblioAnalysis_Files/' folder, the 'Configuration_Files/' folder\n",
    "<br>\n",
    "- Store the configuration files (config_filter.json) a the 'Configuration_Files/' folder that are:\n",
    "    - 'config_filter.json' used for the filtering of a corpus\n",
    "    - 'congig_temporal.json'used for the temporal development of item values in a set of annual coupuses \n",
    "<br>\n",
    "<br>\n",
    "- Create, in the 'Configuration_Files/' folder, your additional_files folder to be named 'Selection_Files/' \n",
    "<br>\n",
    "- Store your files (free names) of selected item values in this additional_files folder together with:\n",
    "    - 'TempDevK_full.txt' used to select the words to search in the description files of the corpuses for the temporal development of item values in the set of annual coupuses\n",
    "<br>\n",
    "<br>\n",
    "- Create, in the 'BiblioAnalysis_Files/' folder, your project folder\n",
    "<br>\n",
    "- Create the 'rawdata/' folder in your project folder\n",
    "<br>\n",
    "- Store your corpus file (either wos or scopus extraction) in the 'rawdata/' folder of your project folder\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I- User environment setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "## User identification\n",
    "root = Path.home()\n",
    "\n",
    "## Building dict of paths for potential users (to be completed with the specific paths of new users) \n",
    "user = {}\n",
    "if root == Path('/Users/amal'):\n",
    "    user = {\n",
    "        'user_id' : 'Amal',\n",
    "        'mac_packages' : '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages',\n",
    "        'path1' : 'My_Jupyter/',\n",
    "        'path2' : 'BiblioAnalysis_Files/',\n",
    "        'path3' : 'Configuration_Files/',\n",
    "        'path4' : 'Selection_Files/',\n",
    "        'path5' : '1_Initial/'\n",
    "        }\n",
    "\n",
    "elif root == Path('C:/Users/franc'):\n",
    "    user = {\n",
    "        'user_id' : 'François',\n",
    "        'mac_packages' : '',\n",
    "        'path1' : '',\n",
    "        'path2' : 'BiblioAnalysis_Files/',\n",
    "        'path3' : 'Configuration_Files/',\n",
    "        'path4' : 'Selection_Files/',\n",
    "        'path5' : ''\n",
    "        }\n",
    "else:\n",
    "    user = {\n",
    "        'user_id' : 'Iona',\n",
    "        'mac_packages' : '/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages',\n",
    "        'path1' : 'GitHubClone/',\n",
    "        'path2' : 'BiblioAnalysis_Files/',\n",
    "        'path3' : 'Configuration_Files/',\n",
    "        'path4' : 'Selection_Files/',\n",
    "        'path5' : '1_Initial/'\n",
    "        }    \n",
    "\n",
    "## Add path of 'site-packages' where useful packages are stored on MAC-OS; no impact for Windows\n",
    "sys.path.append(user['mac_packages'])\n",
    "\n",
    "## Folder containing the general useful files\n",
    "rep_utils = root / Path(user['path1'] + 'BiblioAnalysis/' + 'BiblioAnalysis_RefFiles')\n",
    " # Specific files for scopus type database in this folder\n",
    "scopus_cat_codes = 'scopus_cat_codes.txt'\n",
    "scopus_journals_issn_cat = 'scopus_journals_issn_cat.txt'\n",
    "\n",
    "## Getting complementary information from user  \n",
    "user_id =  user['user_id']\n",
    "expert =  False\n",
    "corpuses_folder = root / Path(user['path2'] + user['path5'])\n",
    "config_folder = root / Path(user['path2'] + user['path3'])\n",
    "select_folder = config_folder / Path(user['path4'])\n",
    "\n",
    "## Printing useful information\n",
    "print('Corpuses folder:', corpuses_folder)\n",
    "print('Configuration folder:', config_folder)\n",
    "print('Selection folder:', select_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II- Single year corpus analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &emsp;&emsp;II-1 Selection of the corpus file for BiblioAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "## Selection of corpus file\n",
    "corpusfiles_list = os.listdir(corpuses_folder)\n",
    "corpusfiles_list.sort()\n",
    "print('Please select the corpus via the tk window')\n",
    "myprojectname = bau.Select_multi_items(corpusfiles_list,'single')[0]+'/'\n",
    "#clear_output(wait=False)\n",
    "project_folder = corpuses_folder /Path(myprojectname)\n",
    "database_type = input('Corpus file type (scopus, wos - default: \"wos\")? ')\n",
    "if database_type =='': database_type = 'wos' \n",
    "#clear_output(wait=True)\n",
    "\n",
    "## Setting the  graph main heading\n",
    "digits_list = list(filter(str.isdigit, myprojectname))\n",
    "corpus_year = ''\n",
    "for i in range(len(digits_list)):corpus_year = corpus_year + digits_list[i]\n",
    "init, end = str(user['path5']).find(\"_\")+1,-1\n",
    "corpus_state = str(user['path5'])[init:end]\n",
    "main_heading = corpus_year + ' Corpus: ' + corpus_state\n",
    "\n",
    "## Printing useful information\n",
    "print('Specific-paths set for user: ', user_id)\n",
    "print('Corpus year:                 ', corpus_year)\n",
    "print('Corpus status:               ', corpus_state)\n",
    "print('Project name:                ', myprojectname)\n",
    "print('Corpus file type:            ', database_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &emsp;&emsp;II-2 Data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## Building the names of the useful folders\n",
    "\n",
    "    # Folder containing the wos or scopus file to process\n",
    "in_dir_parsing = project_folder / Path('rawdata')\n",
    "\n",
    "\n",
    "    # Folder containing the output files of the data parsing \n",
    "out_dir_parsing = project_folder / Path('parsing')\n",
    "if not os.path.exists(out_dir_parsing):\n",
    "    os.mkdir(out_dir_parsing)\n",
    "\n",
    "## Running function biblio_parser\n",
    "parser_done = input(\"Parsing available (y/n)? \")\n",
    "#clear_output(wait=True)\n",
    "if parser_done == \"n\":\n",
    "    bau.biblio_parser(in_dir_parsing, out_dir_parsing, database_type, expert, rep_utils) \n",
    "    with open(Path(out_dir_parsing) / Path('failed.json'), 'r') as failed_json:\n",
    "            data_failed=failed_json.read()\n",
    "    dic_failed = json.loads(data_failed)\n",
    "    articles_number = dic_failed[\"number of article\"]\n",
    "    print(\"Parsing processed on full corpus\")\n",
    "    print(\"\\n\\nSuccess rates\")\n",
    "    del dic_failed['number of article']\n",
    "    for item, value in dic_failed.items():\n",
    "        print(f'    {item}: {value[\"success (%)\"]:.2f}%')\n",
    "else:\n",
    "    parser_filt = input(\"Parsing available without rawdata -from filtering- (y/n)? \")\n",
    "    if parser_filt == \"n\":        \n",
    "        with open(Path(out_dir_parsing) / Path('failed.json'), 'r') as failed_json:\n",
    "            data_failed=failed_json.read()\n",
    "        dic_failed = json.loads(data_failed)\n",
    "        articles_number = dic_failed[\"number of article\"]\n",
    "        #clear_output(wait=True)\n",
    "        print(\"Parsing available from full corpus\")\n",
    "        print(\"\\n\\nSuccess rates\")\n",
    "        del dic_failed['number of article']\n",
    "        for item, value in dic_failed.items():\n",
    "            print(f'    {item}: {value[\"success (%)\"]:.2f}%')\n",
    "    else:\n",
    "        #clear_output(wait=True)\n",
    "        print(\"Parsing available from filtered corpus without rawdata\")\n",
    "        file = project_folder /Path('parsing/' + 'articles.dat')\n",
    "        with open(file) as f:\n",
    "            lines = f.readlines()\n",
    "        articles_number = len(lines)\n",
    "\n",
    "print(\"\\n\\nCorpus parsing saved in folder:\\n\", str(out_dir_parsing))\n",
    "print('\\nNumber of articles in the corpus : ', articles_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  &emsp;&emsp;II-2.1 Data parsing / Corpus description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## Building the names of the useful folders\n",
    "\n",
    "    # Folder containing the wos or scopus parsed files\n",
    "in_dir_corpus = out_dir_parsing\n",
    "\n",
    "    # Folder containing the wos or scopus parsed and analysed files\n",
    "out_dir_corpus = project_folder / Path('freq')\n",
    "if not os.path.exists(out_dir_corpus):\n",
    "    os.mkdir(out_dir_corpus)\n",
    "\n",
    "## Running describe_corpus\n",
    "description_done = input(\"Description available (y/n)? \")\n",
    "#clear_output(wait=True)\n",
    "if description_done == \"n\":\n",
    "    verbose = False\n",
    "    bau.describe_corpus(in_dir_corpus, out_dir_corpus, database_type, verbose)\n",
    "    print(\"Corpus description saved in folder:\", str(out_dir_corpus))\n",
    "else:\n",
    "    print(\"Corpus description available in folder:\", str(out_dir_corpus))\n",
    "\n",
    "# Building the name of file for histogram plot of an item\n",
    "file_distrib_item = out_dir_corpus / Path('DISTRIBS_itemuse.json')\n",
    "\n",
    "## Running plot of treemap, scatter plot and histogram for a selected item_treemap\n",
    "do_treemap = input(\"Treemap for an item of the corpus description (y/n)? \")\n",
    "if do_treemap == 'y':\n",
    "    renew_treemap = 'y'\n",
    "    while renew_treemap == 'y' :\n",
    "        print(\"Choose the item for treemap in the tk window\")\n",
    "        item_treemap = bau.item_selection()\n",
    "        file_name_treemap = out_dir_corpus / Path('freq_'+ item_treemap +'.dat')\n",
    "        print(\"Item selected:\",item_treemap)\n",
    "        bau.treemap_item(item_treemap, file_name_treemap)\n",
    "        do_scatter = input(\"Scatter plot for the item (y/n)? \")\n",
    "        if do_scatter == 'y':\n",
    "            bau.plot_counts(item_treemap, file_name_treemap)\n",
    "        do_histo = input(\"Histogram plot for the item (y/n)? \")\n",
    "        if do_histo == 'y':\n",
    "            bau.plot_histo(item_treemap, file_distrib_item)\n",
    "        renew_treemap = input(\"\\n\\nTreemap for a new item (y/n)? \")\n",
    "\n",
    "# Initialize the variable G_coupl that will receive the biblioanalysis coupling graphs\n",
    "try: G_coupl\n",
    "except NameError: G_coupl = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &emsp;&emsp;II-2.1.1 Data parsing / Corpus description / Filtering the data and filtered corpus description\n",
    "To be run after corpus description to allow using the following functions : describe_corpus() , treemap_item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil                      \n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "## Recursive filtering\n",
    "\n",
    "# Allows prints in filter_corpus_new function\n",
    "verbose = False\n",
    "\n",
    "# Initialization of parameters for recursive filtering\n",
    "filtering_step = 1\n",
    "while True:\n",
    "\n",
    "    ## Building the names of the useful folders and creating the output folder if not find \n",
    "    if filtering_step == 1:\n",
    "        in_dir_filter = out_dir_parsing\n",
    "        file_config_filters = config_folder / Path('config_filters.json')\n",
    "        print('Filter configuration file:',file_config_filters)\n",
    "        modif_filtering = input(\"Modification of item-values list from a predefined file (y/n)? \")\n",
    "        if modif_filtering == \"y\":\n",
    "            bau.filters_modification(config_folder,file_config_filters)    \n",
    "    else:\n",
    "        renew_filtering = input(\"Apply a new filtering process (y/n)? \") \n",
    "        if renew_filtering == \"n\": break\n",
    "        in_dir_filter = project_folder / Path('filter' + '_' + str(filtering_step-1))\n",
    "        file_config_filters = in_dir_filter / Path('save_config_filters.json')\n",
    "        print('Filter configuration file:',file_config_filters) \n",
    "        \n",
    "    out_dir_filter = project_folder / Path( 'filter' + '_' + str(filtering_step))\n",
    "    if not os.path.exists(out_dir_filter): os.mkdir(out_dir_filter)\n",
    "\n",
    "    # Building the absolute file name of filter configuration file to save for the filtering step\n",
    "    save_config_filters = out_dir_filter / Path(bau.SAVE_CONFIG_FILTERS)\n",
    "    print('\\nSaving filter configuration file:',save_config_filters)\n",
    "    \n",
    "    # Configurating the filtering through a dedicated GUI or getting it from the existing file\n",
    "    bau.filters_selection(file_config_filters,save_config_filters,in_dir_filter) \n",
    "\n",
    "    # Read the filtering status\n",
    "    combine,exclusion,filter_param = bau.read_config_filters(file_config_filters)\n",
    "    print(\"\\nFiltering status:\")\n",
    "    print(\"   Combine   :\",combine)\n",
    "    print(\"   Exclusion :\",exclusion)\n",
    "    for key,value in filter_param.items():\n",
    "        print(f\"   Item      : {key}\\n   Values    : {value}\\n\")\n",
    "\n",
    "    # Running function filter_corpus_new\n",
    "    bau.filter_corpus_new(in_dir_filter, out_dir_filter, verbose, file_config_filters)\n",
    "    file = out_dir_filter /Path('articles.dat')\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        articles_number = len(lines)\n",
    "    if articles_number == 0:\n",
    "        print('Filtered corpus empty !')\n",
    "        break\n",
    "    print(\"Filtered-corpus parsing saved in folder \", \n",
    "            str(out_dir_filter),\n",
    "            \" with the corresponding filters configuration\")\n",
    "\n",
    "        # Folder containing the wos or scopus parsed and filtered files\n",
    "    in_dir_freq_filt = out_dir_filter\n",
    "\n",
    "        # Folder containing the wos or scopus parsed, filtered and analysed files\n",
    "    out_dir_freq_filt = project_folder / Path('freq' + '_' + str(filtering_step))\n",
    "    if not os.path.exists(out_dir_freq_filt): os.mkdir(out_dir_freq_filt)\n",
    "\n",
    "        # Copying 'database.dat' file in the freq folder for the use in describe_corpus function \n",
    "    original = project_folder / Path('parsing/database.dat')\n",
    "    target = project_folder / Path('filter' + '_' + str(filtering_step) + '/database.dat')\n",
    "    shutil.copyfile(original, target)\n",
    "\n",
    "        # Running describe_corpus \n",
    "    verbose = False\n",
    "    bau.describe_corpus(in_dir_freq_filt, out_dir_freq_filt, database_type, verbose)\n",
    "    print(\"Filtered corpus description saved in folder:\", str(out_dir_freq_filt))\n",
    "\n",
    "    # Treemap plot by a corpus item after filtering\n",
    "    make_treemap = 'n'\n",
    "    make_treemap = input(\"\\n\\nDraw treemap (y/n)?\")\n",
    "    if make_treemap == 'y' :\n",
    "\n",
    "            # Running plot of treemap for selected item_treemap\n",
    "        renew_treemap = 'y'    \n",
    "        while renew_treemap == 'y' :\n",
    "            print('\\n\\nChoose the item for treemap of the filtered corpus description in the tk window')\n",
    "            item_treemap = bau.item_selection()\n",
    "            file_name_treemap = project_folder / Path('freq_' + str(filtering_step) + '/' + 'freq_'+ item_treemap +'.dat')\n",
    "            print(\"Item selected:\",item_treemap)\n",
    "            bau.treemap_item(item_treemap, file_name_treemap)\n",
    "            renew_treemap = input(\"\\n\\nTreemap for a new item (y/n)? \") \n",
    "\n",
    "    filtering_step=filtering_step + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &emsp;&emsp;II-2.1.2 Data parsing / Corpus Description / Bibliographic Coupling analysis\n",
    "To be run after corpus description to use the frequency analysis. You may execute the bibliographic coupling script several times successively on unfiltered corpus and on available filtering steps of the corpus.\n",
    "The result files are saved in independant folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "# Building the names of the useful folders and creating the output folder if not find\n",
    "path_coupling = user['path2'] + myprojectname + 'coupling'   \n",
    "filtering = input(\n",
    "                  \"Corpus filtered (y/n)? \"\n",
    "                 )   \n",
    "if filtering == \"y\":\n",
    "    path_filter = user['path2'] + myprojectname + 'filter'\n",
    "    path_freq = user['path2'] + myprojectname + 'freq'\n",
    "    filtering_step = input(\n",
    "                            \"Enter filtering step : \"\n",
    "                          ) \n",
    "    in_dir_coupling = project_folder / Path('filter' + '_' + str(filtering_step))\n",
    "    in_dir_freq= project_folder / Path('freq' + '_' + str(filtering_step))\n",
    "    out_dir_coupling = project_folder / Path('coupling' + '_' + str(filtering_step))\n",
    "else:\n",
    "    in_dir_coupling = out_dir_parsing\n",
    "    in_dir_freq= out_dir_corpus    \n",
    "    out_dir_coupling = project_folder / Path('coupling')\n",
    "\n",
    "if not os.path.exists(out_dir_coupling):\n",
    "    os.mkdir(out_dir_coupling)\n",
    "\n",
    "# Building the coupling graph of the corpus\n",
    "print('Building the coupling graph of the corpus, please wait...')\n",
    "G_coupl = bau.build_coupling_graph(in_dir_coupling)\n",
    "#clear_output(wait = True)\n",
    "\n",
    "# Building the partition of the corpus\n",
    "print('Building the partition of the corpus, please wait...')\n",
    "G_coupl,partition = bau.build_louvain_partition(G_coupl)\n",
    "#clear_output(wait = True)\n",
    "print()\n",
    "\n",
    "# Adding attributes to the coupling graph nodes\n",
    "attr_dic = {}\n",
    "add_attrib = input(\"Add attributes to the coupling graph nodes (y/n)? \")\n",
    "if add_attrib == 'y':\n",
    "    while True:\n",
    "        print('\\n\\nChoose the item for the attributes to add in the tk window')\n",
    "        item, m_max_attrs = bau.coupling_attr_selection()\n",
    "        attr_dic[item] = m_max_attrs\n",
    "        print(\"Item selected:\",item,\" with \",m_max_attrs, \" attributes\" )\n",
    "        G_coupl = bau.add_item_attribute(G_coupl, item, m_max_attrs, in_dir_freq, in_dir_coupling)\n",
    "        renew_attrib = input(\"\\nAdd attributes for a new item (y/n)?\") \n",
    "        if renew_attrib == 'n' : break      \n",
    "\n",
    "# Plot control of the coupling graph before using Gephy\n",
    "NODES_NUMBER_MAX = 1\n",
    "bau.plot_coupling_graph(G_coupl,partition,nodes_number_max=NODES_NUMBER_MAX)\n",
    "\n",
    "# Creating a Gephy file of the coupling graph  \n",
    "bau.save_graph_gexf(G_coupl,out_dir_coupling)\n",
    "print(\"\\nCoupling analysis of the corpus saved as Gephy file in folder:\\n\", str(out_dir_coupling))\n",
    "\n",
    "# Creating an EXCEL file of the coupling analysis results\n",
    "bau.save_communities_xls(partition,in_dir_coupling,out_dir_coupling)\n",
    "print(\"\\nCoupling analysis of the corpus saved as EXCEL file in folder:\\n\", str(out_dir_coupling))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &emsp;&emsp;II-2.1.3  HTML graph of coupling analysis \n",
    "##### after Data parsing / Corpus Description / Coupling analysis  \n",
    "You may execute the HTML graph construction script several times successively on the available coupling graph of the corpus. The result files are saved in the corresponding coupling floder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Creating html file of graph G using pyviz\n",
    "   This script uses the results of the Biblioanalysis coupling analysis:\n",
    "   - out_dir_coupling (Path): path for saving the coupling analysis results;\n",
    "   - G (networkx object): coupling graph with added attributes;\n",
    "   - partition (dict):  partition of graph G;\n",
    "   - attr_dic (dict): dict of added attributes with number of added values. \n",
    "   \n",
    "'''\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "# Checking the availability of the corpus coupling graph G with all attributes and its partition\n",
    "assert(G_coupl is not None),'''Please run first the \"Bibliographic coupling analysis\" \n",
    "                                script to build the coupling graph'''\n",
    "\n",
    "\n",
    "\n",
    "color_nodes = {0: '#5677fc',  #blue\n",
    "               1: '#3f51b5',  #indigo\n",
    "               2: '#e51c23',  #red\n",
    "               3: '#00bcd4',  #cyan\n",
    "               4: '#259b24',  #green\n",
    "               5: '#ffeb3b',  #yellow\n",
    "               6: '#ff9800',  #orange\n",
    "               7: '#795548',  #brown \n",
    "               8: '#cddc39',  #lime   # limited number of colored values \n",
    "              'uncolor': '#e0e0e0'  # grey 300 - for nodes out of colored values\n",
    "              } \n",
    "\n",
    "colored_values = {'Neurosciences & Neurology':'0',\n",
    "                  'Psychology':'1',\n",
    "                  'Computer Science':'2',\n",
    "                  'Robotics,Automation & Control Systems':'3',\n",
    "                  'Life Sciences & Biomedicine - Other Topics':'4',\n",
    "                  'Biochemistry & Molecular Biology':'4',\n",
    "                  'Cell Biology':'4',\n",
    "                  'Evolutionary Biology':'4',\n",
    "                  'Biomedical Social Sciences':'4',\n",
    "                  'Biotechnology & Applied Microbiology':'4',\n",
    "                  'Developmental Biology':'4',\n",
    "                  'Microbiology':'4',\n",
    "                  'Marine & Freshwater Biology':'4',\n",
    "                  'Reproductive Biology':'4',\n",
    "                  'Genetics & Heredity':'4',\n",
    "                  'Philosophy':'5',\n",
    "                  'History & Philosophy of Science':'5',\n",
    "                  'Social Sciences - Other Topics':'6',\n",
    "                  'Mathematical Methods In Social Sciences':'6',\n",
    "                  'Linguistics':'7',\n",
    "                  'Anthropology':'8',\n",
    "                 }\n",
    "\n",
    "# Setting the attribute value to be specifically shaped\n",
    "shaped_attr = input('Please enter the added attribute value to be specifically shaped (default: Psychology)')\n",
    "if shaped_attr == '':shaped_attr = 'Psychology'\n",
    "print('Attribute value to be specifically shaped (triangle):',shaped_attr)\n",
    "heading4 = 'Triangles for \"' + shaped_attr + '\" in disciplines.'\n",
    "\n",
    "# Setting the item label among the added attribute to be colored\n",
    "colored_attr = input('Please enter the item label among the added attributes to be colored (default: S)')\n",
    "if colored_attr == '':colored_attr = 'S'\n",
    "print('Attribute to be colored:',colored_attr)\n",
    "if colored_attr == 'S': \n",
    "    heading3 = 'Colored by main discipline (grey: without filtering subjects as main discipline).'\n",
    "else:\n",
    "    heading3 = 'Colored by main attribute values (grey: without filtering attribute values as main discipline).'\n",
    "assert(colored_attr in attr_dic.keys()),\\\n",
    "    f'''Selected colored attribute should be among the added attributes: {list(attr_dic.keys())}.\n",
    "Please run this script again to select an effectivelly added attribute to the coupling graph node \n",
    "or run again the \"Bibliographic coupling analysis\" script to add the targetted attribute to the coupling graph.'''\n",
    "\n",
    "# Computing the number of communities\n",
    "community_number = len(set(partition.values()))\n",
    "print('Number of communities:',community_number)\n",
    "\n",
    "# Computing the size of the communities\n",
    "communities_size = {}\n",
    "for value in set(partition.values()):\n",
    "    communities_size[value]=0\n",
    "    for key in set(partition.keys()):\n",
    "        if partition[key] == value:\n",
    "            communities_size[value]+=1\n",
    "            \n",
    "# Building the html graphs per community\n",
    "for community_id in range(community_number):\n",
    "    community_size = communities_size[community_id] \n",
    "    heading2 = 'Coupling graph for community ID: ' + str(community_id) + ' Size: ' + str(community_size)\n",
    "    heading = '<h1>' + main_heading + '</h1>' + '<h2>' + heading2 + '</h2>' \\\n",
    "                  + '<h3 align=left nowrap>' + heading3 + '<br>'  + heading4 + '</h3>'\n",
    "    html_file= str(out_dir_coupling /Path('coupling_' + 'com' + str(community_id) \\\n",
    "                                          + '_size' + str(community_size) + '.html'))\n",
    "    bau.coupling_graph_html_plot(G_coupl,html_file,community_id,attr_dic,colored_attr,\\\n",
    "                                 colored_values,shaped_attr,color_nodes,heading)\n",
    "\n",
    "# Building the html graph for the full corpus\n",
    "heading2  = ' All ' + str(community_number) + ' communities'\n",
    "heading = '<h1>' + main_heading + '</h1>' + '<h2>' + heading2 + '</h2>' \\\n",
    "          + '<h3 align=left nowrap>' + heading3 + '<br>'  + heading4 + '</h3>'\n",
    "html_file= str(out_dir_coupling /Path('coupling_' + 'all.html'))\n",
    "bau.coupling_graph_html_plot(G_coupl,html_file,'all',attr_dic,colored_attr,\\\n",
    "                             colored_values,shaped_attr,color_nodes,heading)\n",
    "\n",
    "print(\"\\nCreated html files of graph G_coupl using pyviz for the corpus in folder:\\n\", str(out_dir_coupling))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;&emsp;II-2.2 Data parsing / Co-occurrence Maps\n",
    "You may execute the co-occurence script several times successively on unfiltered corpus and on available filtering steps of the corpus.\n",
    "The result files are saved in independant folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "# Building the names of the useful folders and creating the output folder if not find \n",
    "filtering = input(\n",
    "                  \"Corpus filtered (y/n)? \"\n",
    "                 )   \n",
    "if filtering == \"y\":\n",
    "    path_filter = user['path2'] + myprojectname + 'filter'\n",
    "    filtering_step = input(\n",
    "                            \"Enter filtering step : \"\n",
    "                          ) \n",
    "    in_dir_cooc = project_folder / Path( 'filter' + '_' + str(filtering_step))\n",
    "    out_dir_cooc = project_folder / Path('cooc' + '_' + str(filtering_step))\n",
    "else:\n",
    "    in_dir_cooc = out_dir_parsing   \n",
    "    out_dir_cooc = project_folder / Path('cooc') \n",
    "\n",
    "if not os.path.exists(out_dir_cooc):\n",
    "    os.mkdir(out_dir_cooc)\n",
    "\n",
    "## Building the coocurrence graph\n",
    "size_min = 1\n",
    "node_size_ref=300\n",
    "while True :\n",
    "    print('\\n\\nChoose the item for cooccurence analysis in the tk window')\n",
    "    cooc_item, size_min = bau.cooc_selection() \n",
    "    print(\"Item selected:\",cooc_item)\n",
    "    G_cooc = bau.build_item_cooc(cooc_item,in_dir_cooc,out_dir_cooc,size_min = size_min)\n",
    "    if G_cooc is None:\n",
    "        print(f'The minimum node size ({size_min}) is two large. Relax this constraint.')\n",
    "    else:\n",
    "        print(\"Cooccurence analysis of the corpus for item \" + cooc_item + \\\n",
    "          \" saved in folder:\", str(out_dir_cooc))\n",
    "        heading2 = 'Co_occurence graph for item ' + cooc_item + ' with minimum node size ' + str(size_min)\n",
    "        heading3 = 'Bold node title: Node attributes[number of item value occurences-item value (total number of edges)]'\n",
    "        heading4 = 'Light node titles: Neighbors attributes[number of item value occurences-item value (number of edges with node)]'\n",
    "        heading = '<h1>' + main_heading + '</h1>' + '<h2>' + heading2 + '</h2>' \\\n",
    "                  + '<h3 align=left nowrap>' + heading3 + '<br>'  + heading4 + '</h3>'\n",
    "    \n",
    "        bau.plot_cooc_graph(G_cooc,cooc_item,size_min=size_min,node_size_ref=node_size_ref)\n",
    "        # Creating html file of graph G_cooc using pyviz\n",
    "        html_file= str(out_dir_cooc /Path('cooc_'+cooc_item+'_thr'+str(size_min)+'.html'))\n",
    "        bau.cooc_graph_html_plot(G_cooc,html_file,heading)\n",
    "        print(\"Created html file of\",cooc_item,\"cooccurence graph using pyviz in folder:\\n\",\\\n",
    "              str(out_dir_cooc))\n",
    "        \n",
    "    renew_cooc = input(\"\\n\\nCooccurence analysis for a new item (y/n)?\") \n",
    "    if renew_cooc == 'n' : break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III- Temporal development of item values weight\n",
    "To run this cell a set of annual corpuses with their description should be available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "keyword_filters = {\n",
    "    'is_in':[],    \n",
    "    'is_equal':[]}\n",
    "\n",
    "## Building the search configuration:\n",
    "#### - either by reading of the 'config_temporal.json' without modification\n",
    "#### - or by an interactive modification of the configuration and storing it in this file for a futher use\n",
    "TemporalDev_file = 'config_temporal.json'\n",
    "file = config_folder / Path(TemporalDev_file)\n",
    "\n",
    "keywords_modif = input('Modification of the keywords list (y/n)?')\n",
    "if keywords_modif == 'y':\n",
    "    #clear_output(wait=True)\n",
    "    \n",
    "        # Selection of items\n",
    "    items_full_list = ['IK','AK','TK','S','S2']\n",
    "    print('\\nPlease select the items to be analyzed via the tk window')\n",
    "    items = bau.Select_multi_items(items_full_list,'multiple')\n",
    "    #clear_output(wait=True)\n",
    "\n",
    "        # Selection of the folder of keywords-full-list file\n",
    "    myfolder = select_folder\n",
    "\n",
    "        # Setting the list of keywords-full-list file \n",
    "    myfile = myfolder / Path('TempDevK_full.txt')\n",
    "    \n",
    "        # Selection of the keywords list to be put in the temporal development configuration file\n",
    "    keywords_full_list_file = myfolder / Path(myfile)\n",
    "    keywords_full_list = bau.item_values_list(keywords_full_list_file)\n",
    "    \n",
    "        # Selection of keywords  \n",
    "    search_modes = ['is_in','is_equal']\n",
    "    for search_mode in search_modes:\n",
    "        print('\\nPlease select the keywords for ',search_mode, ' via the tk window')\n",
    "        keyword_filters[search_mode] = bau.Select_multi_items(keywords_full_list,mode = 'multiple')\n",
    "        #clear_output(wait=True)\n",
    "        \n",
    "    # Saving the new configuration in the 'config_temporal.json' file   \n",
    "    bau.write_config_temporaldev(file,items,keyword_filters)\n",
    "    print('\\n New temporal development configuration saved in: \\n', file)    \n",
    "else:\n",
    "    # Reading the search configuration from the 'config_temporal.json' file  \n",
    "    items,keywords_param = bau.read_config_temporaldev(file)\n",
    "    print('Selection of items:\\n',items)    \n",
    "    keyword_filters['is_in'] = keywords_param['is_in']\n",
    "    keyword_filters['is_equal'] = keywords_param['is_equal']\n",
    "    #clear_output(wait=True)\n",
    "\n",
    "## Selection of annual corpus files\n",
    "corpusfiles_list = os.listdir(corpuses_folder)\n",
    "corpusfiles_list.sort()\n",
    "print('\\nPlease select the corpuses to be analyzed via the tk window')\n",
    "years = bau.Select_multi_items(corpusfiles_list,'multiple')\n",
    "\n",
    "# Print configuration\n",
    "#clear_output(wait=True)\n",
    "print('Search items:', items)\n",
    "print('\\nSearch Words:\\n' + json.dumps(keyword_filters, indent=2))\n",
    "print('\\n Selection of annual corpus files:\\n',years, '\\n')\n",
    "\n",
    "# Performing the search using the keyword_filters dict\n",
    "keyword_filter_list = bau.temporaldev_itemvalues_freq(keyword_filters ,items, years, corpuses_folder)\n",
    "\n",
    "# saving the search results in an EXCEL file\n",
    "store_file = corpuses_folder / Path('Results_Files/TempDev_synthesis.xlsx')\n",
    "bau.temporaldev_result_toxlsx(keyword_filter_list,store_file)\n",
    "print('\\nTemporal development results saved in:\\n', store_file) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annexe 1- Databases merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "database, filename, in_dir, out_dir = bau.merge_database_gui()\n",
    "bau.merge_database(database,filename,in_dir,out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annexe 2- Item values selection to list for filters configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from pathlib import Path\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "file_config_filters = config_folder / Path('config_filters.json')    \n",
    "bau.filters_modification(config_folder,file_config_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data parsing\n",
    "- articles.dat is the central file, listing all the publications within the corpus. It contains informations such as the document type (article, letter, review, conf proceeding, etc), title, year of publication, publication source, doi, number of citations (given by WOS or Scopus at the time of the extraction) AND a unique identifier used in all the other files to identify a precise publication.\n",
    "- database.dat keeps track of the origin of the data, some part of the analysis being specific to WOS or Scopus data.\n",
    "- authors.dat lists all authors names associated to all publications ID.\n",
    "- addresses.dat lists all adresses associated to all publications ID, along with a specific ID for each adresse line. These adresses are reported as they appear in the raw data, without any further processing.\n",
    "- countries.dat lists all countries associated to all publications ID and adresses lines ID. The countries are extracted from the adresses fields of the raw data, with some cleaning (changing mentions of US states and UK countries to respectively the USA and UK).\n",
    "- institutions.dat lists all the comma-separated entities appearing in the adresses field associated to all publications ID and adresses lines ID, except those refering to a physical adresses. These entities correspond to various name variants of universities, organisms, hospitals, labs, services, departments, etc as they appear in the raw data. No treatment is made to e.g. filtering out the entities corresponding a given hierarchy level.\n",
    "- keywords.dat lists various types of keywords associated to all publications ID. \"AK\" keywords correspond to Author's keywords. \"IK\" keywords correspond to either WOS or Scopus keywords, which are built based on the authors' keywords, the title and abstract. \"TK\" correspond to title words (from which we simply remove common words and stop words - no stemming is performed). TK are especially useful when studying pre-90's publications, when the use of keywords was not yet standard.\n",
    "- references.dat lists all the references associated to all publications ID. The rawdata is parsed to store the first author name, title, source, volume and page of each reference of the raw \"references\" field.\n",
    "- subjects.dat lists all subject categories associated to all publications ID (a journal may be associated to many subject category). WOS classifies the sources it indexes into ∼ 250 categories, that are reported in the extracted data. Scopus classifies its sources into 27 major categories and ∼ 300 sub-categories, none of which are reported in the extracted data. We use Elsevier Source Title List (october 2017 version) to retrieve that information. The \"subject.dat\" contains the info relative to the major categories.\n",
    "- subjects2.dat lists Scopus's sub-categories, if the use database is Scopus.\n",
    "- AA_log.txt keeps track of the date/time the script was executed and of all the messages displayed on the terminal (number of publications extracted, % of references rejected, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Corpus description\n",
    "Before doing anything else, you should get a general idea of the content of your database.  This script performs several basic tasks:\n",
    "- it performs a series of frequency analysis, computing the number of occurrences of each item (authors, keywords, references, etc) within the publications of the corpus. These frequencies are automatically stored into several \"freq_xxx.dat\" files within a newly created \"freq\" folder.\n",
    "- it performs a series of generic statistical analysis, storing the numbers of distinct items of each type (e.g. there are x distinct keyword in the corpus ), the distributions of number of occurrences of each item (e.g. there are x keywords appearing in at least y publications) and the distribution of number of items per publication (e.g.there are x% of publications with y keywords). All these statistics are stored in the \"DISTRIBS_itemuse.json\" file.\n",
    "- it also performs a co-occurrence analysis, computing the number of co-occurrence of pairs of items among the top 100 most frequent items of each type (e.g. computing how often the two most used keywords appear together in the same publications). The results of this analysis are stored in the \"coocnetworks.json\" file. More systematic co-occurrence analysis can also be performed with another script, cf the Co-occurrence Maps section below.\n",
    "All the generated files can be opened and read with a simple text editor. The freq_xxx.dat, listing items by order of frequency, can also be read in a spreadsheet software such as excel. All the files are however primarily made to be read in the BiblioMaps interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the data\n",
    "#### To be run after corpus description to allow using the following functions : describe_corpus() , treemap_item()\n",
    "\n",
    "If, upon exploring the nature of the data you realize that before going further you'd prefer to filter your corpus based on some characteristic (keeping only the publications from certain years, using some keywords or references, written by some authors from some countries, etc), you can filter the initial corpus thanks to the script:\n",
    "\n",
    "- python BiblioTools3.2/filter.py -i myprojectname/ -o myprojectname_filtered -v <br>\n",
    "\n",
    "Edit the 'filter.py' file to specify your filters. You'll also need to create a new \"myprojectname_filtered\" main folder before running the script.\n",
    "- create the files articles.dat, addresses.dat, authors.dat, countries.dat, institutions.dat, keywords.dat, references.dat, subjects.dat, subjects2.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurrence Maps\n",
    "You may execute the co-occurence script several times successively on unfiltered corpus and on available filtering steps of the corpus.\n",
    "The result files are saved in independant folders.\n",
    "\n",
    "The script create multiple co-occurence networks, all stored in gdf and gexf files that can be opened in Gephi, among which:\n",
    "\n",
    "Example of heterogeneous network generated with BiblioAnlysis and visualized in Gephi.\n",
    "\n",
    "- a co-cocitation network, linking references that are cited in the same publications.\n",
    "- a co-refsources network, linking references's sources that are cited in the same publications.\n",
    "- a co-author network, linking authors that collaborated in some publications.\n",
    "- a co-country network, linking countries with researchers that collaborated in some publications.\n",
    "- a co-institution network, linking institutions with researchers that collaborated in some publications. For this network to be fully useful, you may want to spend some time cleaning the \"institutions.dat\", e.g. by keeping only the big institutions (university level) or by replacing minor name variant by the dominant name variant (\"Ecole Normale Supérieure de Lyon\" → \"ENS Lyon\")\n",
    "- a co-keyword network, linking keywords being co-used in some publications. Be careful about the interpretation: keywords can be polysemic, their meaning differing from field to another (eg \"model\", \"energy\", \"evolution\", etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
