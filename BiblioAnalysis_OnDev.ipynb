{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiblioAnalysis_OnDev\n",
    "\n",
    "### Version: 0.0.0\n",
    "\n",
    "### Aims\n",
    "- This jupyter notebook results from the use analysis of BiblioTools2jupyter notebook and a new implementation of the following parts:\n",
    "    - Parsing: replaced and tested \n",
    "    - Corpus description: replaced and tested\n",
    "    - Filtering: replaced and tested, integrating the \"EXCLUSION\" mode and the recursive filtering\n",
    "    - Cooccurrence analysis : replaced and tested, integrating graph plot and countries GPS coordinates\n",
    "    - Coupling analyis : replaced and tested\n",
    "    \n",
    "### Created modules in the package BiblioAnalysis_Utils\n",
    "    - BiblioParser.py\n",
    "    - BiblioDescription.py\n",
    "    - BiblioFilter.py\n",
    "    - BiblioCooc.py\n",
    "    - BiblioCoupling.py\n",
    "    - GUI_utils.py\n",
    "\n",
    "### BiblioTool3.2 source\n",
    "http://www.sebastian-grauwin.com/bibliomaps/download.html \n",
    "\n",
    "### List of initial Python packages extracted from  BiblioTool3.2\n",
    "- biblio_parser.py\t⇒ pre-processes WOS / Scopus data files,\n",
    "- corpus_description.py\t⇒ performs a frequency analysis of the items in corpus,\n",
    "- filter.py\t⇒ filters the corpus according to a range of potential queries but still too specific\n",
    "- biblio_coupling.py\t⇒ performs a BC anaysis of the corpus,\n",
    "- cooc_graphs.py\t⇒ produces various co-occurrence graphs based on the corpus (call parameters changed)\n",
    "\n",
    "### Specifically required list of pip install \n",
    "- !pip3 install squarify \n",
    "- !pip3 install inquirer\n",
    "- !pip3 install python-louvain\n",
    "\n",
    "### Specifically required nltk downloads\n",
    "- import nltk\n",
    "    - nltk.download('punkt')\n",
    "    - nltk.download('averaged_perceptron_tagger')\n",
    "    - nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary instructions\n",
    "#### These actions will be interactively performed in the next version of the Jupyter notebook\n",
    "1- Create the 'BiblioAnalysis_Files/' folder in your 'Users/' folder\n",
    "<br>\n",
    "2- Create in this 'BiblioAnalysis_Files/' folder, the 'Configuration_Files/' folder\n",
    "<br>\n",
    "3- Store your configuration files (config_users.json and config_filter.json) in the 'Configuration_Files/' folder\n",
    "<br>\n",
    "4- Create, in the 'BiblioAnalysis_Files/' folder, your project folder with the name set in the configuration file 'config_users.json'\n",
    "<br>\n",
    "5- Create the 'rawdata/' folder in your project folder\n",
    "<br>\n",
    "6- Store your corpus file (either wos or scopus extraction) in the 'rawdata/' folder of your project folder\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "## User identification\n",
    "root = Path.home()\n",
    "\n",
    "## Building dict of paths for potential users (to be completed with the specific paths of new users) \n",
    "user = {}\n",
    "if root == Path('/Users/amal'):\n",
    "    user = {\n",
    "        'mac_packages' : '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages',\n",
    "        'path1' : 'My_Jupyter/',\n",
    "        'path2' : 'BiblioAnalysis_Files/',\n",
    "        'path3' : 'Configuration_Files/'\n",
    "        }\n",
    "\n",
    "elif root == Path('C:/Users/franc'):\n",
    "    user = {\n",
    "        'mac_packages' : '',\n",
    "        'path1' : '',\n",
    "        'path2' : 'BiblioAnalysis_Files/',\n",
    "        'path3' : 'Configuration_Files/'\n",
    "        }\n",
    "else:\n",
    "    user = {\n",
    "        'mac_packages' : '/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages',\n",
    "        'path1' : 'GitHubClone/',\n",
    "        'path2' : 'BiblioAnalysis_Files/',\n",
    "        'path3' : 'Configuration_Files/'\n",
    "        }    \n",
    "\n",
    "## Add path of 'site-packages' where useful packages are stored on MAC-OS; no impact for Windows\n",
    "sys.path.append(user['mac_packages'])\n",
    "\n",
    "## Getting complementary information from user configuration file \n",
    "file_config_users = root / Path(user['path2'] + user['path3'] + 'config_users.json') \n",
    "with open(file_config_users, \"r\") as read_file:\n",
    "    config_users = json.load(read_file)    \n",
    "user_id =  config_users['users']\n",
    "database_type =  config_users['database']\n",
    "myprojectname = config_users['myprojectname']\n",
    "expert =  config_users['expert']\n",
    "\n",
    "## Folder containing the folder Utils\n",
    "rep_utils = root / Path(user['path1'] + 'BiblioAnalysis/' + 'BiblioAnalysis_RefFiles')\n",
    "\n",
    "## Specific files for scopus type database\n",
    "scopus_cat_codes = 'scopus_cat_codes.txt'\n",
    "scopus_journals_issn_cat = 'scopus_journals_issn_cat.txt'\n",
    "\n",
    "print('Specific paths set for ' + '\"' + user_id + '\"')\n",
    "print('User file full path: ' + str(file_config_users))\n",
    "print('Database type:       ' + database_type)\n",
    "print('Project name:        ' + myprojectname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data parsing\n",
    "- articles.dat is the central file, listing all the publications within the corpus. It contains informations such as the document type (article, letter, review, conf proceeding, etc), title, year of publication, publication source, doi, number of citations (given by WOS or Scopus at the time of the extraction) AND a unique identifier used in all the other files to identify a precise publication.\n",
    "- database.dat keeps track of the origin of the data, some part of the analysis being specific to WOS or Scopus data.\n",
    "- authors.dat lists all authors names associated to all publications ID.\n",
    "- addresses.dat lists all adresses associated to all publications ID, along with a specific ID for each adresse line. These adresses are reported as they appear in the raw data, without any further processing.\n",
    "- countries.dat lists all countries associated to all publications ID and adresses lines ID. The countries are extracted from the adresses fields of the raw data, with some cleaning (changing mentions of US states and UK countries to respectively the USA and UK).\n",
    "- institutions.dat lists all the comma-separated entities appearing in the adresses field associated to all publications ID and adresses lines ID, except those refering to a physical adresses. These entities correspond to various name variants of universities, organisms, hospitals, labs, services, departments, etc as they appear in the raw data. No treatment is made to e.g. filtering out the entities corresponding a given hierarchy level.\n",
    "- keywords.dat lists various types of keywords associated to all publications ID. \"AK\" keywords correspond to Author's keywords. \"IK\" keywords correspond to either WOS or Scopus keywords, which are built based on the authors' keywords, the title and abstract. \"TK\" correspond to title words (from which we simply remove common words and stop words - no stemming is performed). TK are especially useful when studying pre-90's publications, when the use of keywords was not yet standard.\n",
    "- references.dat lists all the references associated to all publications ID. The rawdata is parsed to store the first author name, title, source, volume and page of each reference of the raw \"references\" field.\n",
    "- subjects.dat lists all subject categories associated to all publications ID (a journal may be associated to many subject category). WOS classifies the sources it indexes into ∼ 250 categories, that are reported in the extracted data. Scopus classifies its sources into 27 major categories and ∼ 300 sub-categories, none of which are reported in the extracted data. We use Elsevier Source Title List (october 2017 version) to retrieve that information. The \"subject.dat\" contains the info relative to the major categories.\n",
    "- subjects2.dat lists Scopus's sub-categories, if the use database is Scopus.\n",
    "- AA_log.txt keeps track of the date/time the script was executed and of all the messages displayed on the terminal (number of publications extracted, % of references rejected, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biblio_parser(in_dir_parsing, out_dir_parsing, database, expert, rep_utils):\n",
    "    \n",
    "    '''Chooses the appropriate parser to parse wos or scopus databases.\n",
    "    '''\n",
    "    \n",
    "    if database == \"wos\":\n",
    "        bau.biblio_parser_wos(in_dir_parsing, out_dir_parsing)\n",
    "    elif database == \"scopus\":\n",
    "        bau.biblio_parser_scopus(in_dir_parsing, out_dir_parsing, rep_utils)\n",
    "    else:\n",
    "        raise Exception(\"Sorry, unrecognized database {database} : should be wos or scopus \")\n",
    "\n",
    "\n",
    "# Standard libraries import\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## Building the names of the useful folders\n",
    "\n",
    "    # Folder containing the wos or scopus file to process\n",
    "in_dir_parsing = root / Path(user['path2'] + myprojectname +'rawdata')\n",
    "\n",
    "    # Folder containing the output files of the data parsing \n",
    "out_dir_parsing = root / Path(user['path2'] + myprojectname + 'parsing')\n",
    "if not os.path.exists(out_dir_parsing):\n",
    "    os.mkdir(out_dir_parsing)\n",
    "\n",
    "## Running function biblio_parser\n",
    "parser_done = input(\"Parsing available? (y/n): \")\n",
    "if parser_done == \"n\":\n",
    "    biblio_parser(in_dir_parsing, out_dir_parsing, database_type, expert, rep_utils) # to be dicussed\n",
    "\n",
    "print(\"Corpus parsing saved in folder:\", str(out_dir_parsing))\n",
    "\n",
    "with open(Path(out_dir_parsing) / Path('failed.json'), 'r') as failed_json:\n",
    "    data_failed=failed_json.read()\n",
    "dic_failed = json.loads(data_failed)\n",
    "print(f'number of article :{dic_failed[\"number of article\"]}')\n",
    "del dic_failed['number of article']\n",
    "for item, value in dic_failed.items():\n",
    "    print(f'Success rate of {item}: {value[\"success (%)\"]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Corpus description\n",
    "Before doing anything else, you should get a general idea of the content of your database. Execute the following command line:\n",
    "\n",
    "- python BiblioTools3.2/describe_corpus.py -i myprojectname/ -v <br>\n",
    "\n",
    "The options -i indicates the data input folder and the option -v puts the verbose mode on (detailed info about the script process will be displayed in the terminal). This script performs several basic tasks:\n",
    "\n",
    "- it performs a series of frequency analysis, computing the number of occurrences of each item (authors, keywords, references, etc) within the publications of the corpus. These frequencies are automatically stored into several \"freq_xxx.dat\" files within a newly created \"freq\" folder.\n",
    "- it performs a series of generic statistical analysis, storing the numbers of distinct items of each type (e.g. there are x distinct keyword in the corpus ), the distributions of number of occurrences of each item (e.g. there are x keywords appearing in at least y publications) and the distribution of number of items per publication (e.g.there are x% of publications with y keywords). All these statistics are stored in the \"DISTRIBS_itemuse.json\" file.\n",
    "- it also performs a co-occurrence analysis, computing the number of co-occurrence of pairs of items among the top 100 most frequent items of each type (e.g. computing how often the two most used keywords appear together in the same publications). The results of this analysis are stored in the \"coocnetworks.json\" file. More systematic co-occurrence analysis can also be performed with another script, cf the Co-occurrence Maps section below.\n",
    "All the generated files can be opened and read with a simple text editor. The freq_xxx.dat, listing items by order of frequency, can also be read in a spreadsheet software such as excel. All the files are however primarily made to be read in the BiblioMaps interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## Building the names of the useful folders\n",
    "\n",
    "    # Folder containing the wos or scopus parsed files\n",
    "in_dir_corpus = out_dir_parsing\n",
    "\n",
    "    # Folder containing the wos or scopus parsed and analysed files\n",
    "out_dir_corpus = root / Path(user['path2'] + myprojectname + 'freq')\n",
    "if not os.path.exists(out_dir_corpus):\n",
    "    os.mkdir(out_dir_corpus)\n",
    "\n",
    "## Running describe_corpus\n",
    "description_done = input(\"Description available? (y/n): \")\n",
    "if description_done == \"n\":\n",
    "    verbose = False\n",
    "    bau.describe_corpus(in_dir_corpus, out_dir_corpus, verbose)\n",
    "print(\"Corpus description saved in folder:\", str(out_dir_corpus))\n",
    "\n",
    "## Running plot of treemap for a selected item_treemap\n",
    "do_treemap = input(\"Treemap for an item of the corpus description? (y/n): \")\n",
    "if do_treemap == 'y':\n",
    "    path_treemap = user['path2'] + myprojectname + 'freq/'\n",
    "    renew_treemap = 'y'\n",
    "    while renew_treemap == 'y' :\n",
    "        print(\"Choose the item for treemap in the tk window\")\n",
    "        item_treemap = bau.item_selection()\n",
    "        file_name_treemap = root / Path(path_treemap + 'freq_'+ item_treemap +'.dat')\n",
    "        print(\"Item selected:\",item_treemap)\n",
    "        bau.treemap_item(item_treemap, file_name_treemap)\n",
    "        do_plot = input(\"Scatter plot for the item? (y/n): \")\n",
    "        if do_plot == 'y':\n",
    "            bau.plot_counts(item_treemap, file_name_treemap)\n",
    "        renew_treemap = input(\"\\n\\nTreemap for a new item ? (y/n): \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the data\n",
    "#### To be run after corpus description to allow using the following functions : describe_corpus() , treemap_item()\n",
    "\n",
    "If, upon exploring the nature of the data you realize that before going further you'd prefer to filter your corpus based on some characteristic (keeping only the publications from certain years, using some keywords or references, written by some authors from some countries, etc), you can filter the initial corpus thanks to the script:\n",
    "\n",
    "- python BiblioTools3.2/filter.py -i myprojectname/ -o myprojectname_filtered -v <br>\n",
    "\n",
    "Edit the 'filter.py' file to specify your filters. You'll also need to create a new \"myprojectname_filtered\" main folder before running the script.\n",
    "- create the files articles.dat, addresses.dat, authors.dat, countries.dat, institutions.dat, keywords.dat, references.dat, subjects.dat, subjects2.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Standard library imports \n",
    "import shutil                      \n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "## Recursive filtering\n",
    "\n",
    "# Allows prints in filter_corpus_new function\n",
    "verbose = False\n",
    "\n",
    "# String used to set the output folder name of the subsequent filterings in the recursive_filter function\n",
    "path_filter = user['path2'] + myprojectname + 'filter' \n",
    "\n",
    "## Building the absolute file name of filter configuration file for the user\n",
    "file_config_filters = root / Path(user['path2'] + user['path3'] + 'config_filters.json')\n",
    "\n",
    "# Initialization of parameters for recursive filtering\n",
    "filtering_step = 1\n",
    "while True:\n",
    "    \n",
    "    ## Building the names of the useful folders and creating the output folder if not find \n",
    "    if filtering_step == 1:\n",
    "        in_dir_filter = out_dir_parsing\n",
    "    else:\n",
    "        renew_filtering = input(\"Apply a new filtering process ? (y/n): \")   \n",
    "        in_dir_filter = root / Path(path_filter + '_' + str(filtering_step-1))\n",
    "        if renew_filtering == \"n\": break\n",
    "   \n",
    "    out_dir_filter = root / Path(path_filter + '_' + str(filtering_step))\n",
    "    if not os.path.exists(out_dir_filter): os.mkdir(out_dir_filter)\n",
    "    \n",
    "    congig_filters = \"n\"\n",
    "    while congig_filters == \"n\":\n",
    "        congig_filters = input(\"Filters configuration set and saved ? (y/n): \")\n",
    "        \n",
    "    # Running function filter_corpus_new\n",
    "    bau.filter_corpus_new(in_dir_filter, out_dir_filter, verbose, file_config_filters)\n",
    "    print(\"Filtered corpus parsing saved in folder:\", str(out_dir_filter))\n",
    "\n",
    "    # Treemap plot by a corpus item after filtering\n",
    "    \n",
    "        # Folder containing the wos or scopus parsed and filtered files\n",
    "    in_dir_corpus = out_dir_filter\n",
    "\n",
    "        # Folder containing the wos or scopus parsed, filtered and analysed files\n",
    "    out_dir_corpus = root / Path(user['path2'] + myprojectname + 'freq' + '_' + str(filtering_step))\n",
    "    if not os.path.exists(out_dir_corpus): os.mkdir(out_dir_corpus)\n",
    "    \n",
    "        # Copying 'database.dat' file in the freq folder for the use in describe_corpus function \n",
    "    original = root / Path(user['path2'] + myprojectname + 'parsing/database.dat')\n",
    "    target = root / Path(user['path2'] + myprojectname + 'filter' + '_' + str(filtering_step) + '/database.dat')\n",
    "    shutil.copyfile(original, target)\n",
    "    \n",
    "        # Running describe_corpus for treemap \n",
    "    verbose = False\n",
    "    bau.describe_corpus(in_dir_corpus, out_dir_corpus, verbose)\n",
    "    print(\"Filtered corpus description saved in folder:\", str(out_dir_corpus))\n",
    "\n",
    "        # Running plot of treemap for selected item_treemap\n",
    "    path_treemap = user['path2'] + myprojectname + 'freq_' + str(filtering_step) + \"/\"\n",
    "    renew_treemap = 'y'    \n",
    "    while renew_treemap == 'y' :\n",
    "        print('\\n\\nChoose the item for treemap of the filtered corpus description in the tk window')\n",
    "        item_treemap = bau.item_selection()\n",
    "        file_name_treemap = root / Path(path_treemap + 'freq_'+ item_treemap +'.dat')\n",
    "        print(\"Item selected:\",item_treemap)\n",
    "        bau.treemap_item(item_treemap, file_name_treemap)\n",
    "        renew_treemap = input(\"\\n\\nTreemap for a new item ? (y/n):\") \n",
    "  \n",
    "    filtering_step=filtering_step + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurrence Maps\n",
    "The command line\n",
    "\n",
    "- python BiblioTools3.2/cooc_graphs.py -i myprojectname/ -v -hg <br>\n",
    "\n",
    "will create multiple co-occurence networks, all stored in gdf files that can be opened in Gephi, among which:\n",
    "\n",
    "\n",
    "Example of heterogeneous network generated with BiblioTools and visualized in Gephi.\n",
    "\n",
    "- a co-cocitation network, linking references that are cited in the same publications.\n",
    "- a co-refsources network, linking references's sources that are cited in the same publications.\n",
    "- a co-author network, linking authors that collaborated in some publications.\n",
    "- a co-country network, linking countries with researchers that collaborated in some publications.\n",
    "- a co-institution network, linking institutions with researchers that collaborated in some publications. For this network to be fully useful, you may want to spend some time cleaning the \"institutions.dat\", e.g. by keeping only the big institutions (university level) or by replacing minor name variant by the dominant name variant (\"Ecole Normale Supérieure de Lyon\" → \"ENS Lyon\")\n",
    "- a co-keyword network, linking keywords being co-used in some publications. Be careful about the interpretation: keywords can be polysemic, their meaning differing from field to another (eg \"model\", \"energy\", \"evolution\", etc).\n",
    "- an heterogeneous co-occurrence network, gathering all the items (authors, keywords, journals, subjects, references, institutions, etc), cf example on the side figure. This network will be generated only if the option \"-hg\" is used in the command line above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "## Building the names of the useful folders\n",
    "\n",
    "    # Folder containing the wos or scopus parsed and possibly filtered files \n",
    "filtering = input(\"Corpus filtered ? (y/n): \")   \n",
    "if filtering == \"y\":\n",
    "    in_dir_cooc = out_dir_filter\n",
    "else:\n",
    "    in_dir_cooc = out_dir_parsing\n",
    "\n",
    "    # Folder containing the wos or scopus parsed, possibly filtered and analysed files\n",
    "out_dir_cooc = root / Path(user['path2'] + myprojectname + 'cooc')\n",
    "if not os.path.exists(out_dir_cooc): os.mkdir(out_dir_cooc)\n",
    "\n",
    "## Building the coocurrence graph\n",
    "size_min = 1\n",
    "node_size_ref=300\n",
    "while True :\n",
    "    print('\\n\\nChoose the item for cooccurence analysis in the tk window')\n",
    "    cooc_item, size_min = bau.cooc_selection() \n",
    "    print(\"Item selected:\",cooc_item)\n",
    "    G = bau.build_item_cooc(cooc_item,in_dir_cooc,out_dir_cooc,size_min = size_min)\n",
    "    if G is None:\n",
    "        print(f'The minimum node size ({size_min}) is two large. Relax this constraint.')\n",
    "    else:\n",
    "        print(\"Cooccurence analysis of the corpus for item \" + cooc_item + \\\n",
    "          \" saved in folder:\", str(out_dir_cooc))\n",
    "    \n",
    "        bau.plot_cooc_graph(G,cooc_item,size_min=size_min,node_size_ref=node_size_ref)\n",
    "    renew_cooc = input(\"\\n\\nCooccurence analysis for a new item ? (y/n):\") \n",
    "    if renew_cooc == 'n' : break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliographic Coupling analysis\n",
    "You may execute the bibliographic coupling script with the command line:\n",
    "\n",
    "- python BiblioTools3.2/biblio_coupling.py -i myprojectname/ -v\n",
    "\n",
    "Example of BC clusters network visualisation created in Gephi and of one cluster's ID card, part of a lengthy PDF document listing the ID cards of all clusters.\n",
    "\n",
    "The options -i indicates the data input folder and the option -v puts the verbose mode on. This script execute a number of tasks:\n",
    "\n",
    "- It first creates the BC network, computing Kessler similarities between each pair of publications\n",
    "- It detects a first set of clusters (which we will refer to as \"TOP\") using Thomas Aynaud's python implementation of the louvain algorithm. A second set of clusters (\"SUBTOP\") is then computed by applying the same algorithm to the publications in each TOP cluster, hence providing a 2-level hierarchical partition.\n",
    "- The script will then asked whether you want to create output files for the cluster. By default, the script will output information only for clusters with more than 50 publications, but the script will asked you to confirm / change this threshold. Several files will then be created by the script:\n",
    "    - Output 1: two json files, storing information about the clusters, to be used in the BiblioMaps interface (cf below).\n",
    "    - Output 2a: two .tex files (one for each hierarchical level) you'll have to compile, displaying an \"ID Card\" for each cluster, ie the list of the most frequent keywords, subject, authors, references, etc... used by the publications within this cluster.\n",
    "    - Output 2b: one .gdf file storing information relative to the BC clusters at both the TOP and SUBTOP level. You may open this file with Gephi. You may create visualisations of either the TOP or SUBTOP level by filtering it out, resie the nodes with the \"size\" parameter, run a spatialisation layout algorithm (Force Atlas 2 usually yield satisfaying layouts). You may also choose a label within the few that are available (e.g. 'most_frequent_k' correspond to the most frequent keywords of each cluster). Refer to the Id cards created with latex to know more about the content of each cluster.\n",
    "- Finally, the script proposes you to output the BC network at the publication level, in both a gdf output format that can be opened with Gephi and a json format that can be opened in the BiblioMaps interface. You may either keep the whole network or select only the publications within a given cluster. Keep in mind that both interfaces can only handle a given number of nodes (no more than a few thousands for Gephi, a few hundreds for BiblioMaps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "## ##################################################\n",
    "## Building the names of the useful folders and creating the output folder if not find\n",
    "\n",
    "    # Folder containing the wos or scopus parsed and possibly filtered files\n",
    "filtering = input(\n",
    "    \"Corpus filtered ? (y/n): \"\n",
    "            )   \n",
    "if filtering == \"y\":\n",
    "    in_dir_coupling = out_dir_filter\n",
    "else:\n",
    "    in_dir_coupling = out_dir_parsing\n",
    "\n",
    "    # Folder containing the wos or scopus parsed, possibly filtered and analysed files\n",
    "out_dir_coupling = root / Path(user['path2'] + myprojectname + 'coupling')\n",
    "if not os.path.exists(out_dir_coupling):\n",
    "    os.mkdir(out_dir_coupling)\n",
    "\n",
    "# Building the coupling graph of the corpus\n",
    "G,dic_graph_carac = bau.build_coupling_graph(in_dir_coupling) \n",
    "\n",
    "# Building the partition of the corpus\n",
    "G,partition = bau.build_louvain_partition(G)\n",
    "\n",
    "# Plot control of the coupling graph before using Gephy\n",
    "NODES_NUMBER_MAX = 1\n",
    "bau.plot_coupling_graph(G,partition,dic_graph_carac,nodes_number_max=NODES_NUMBER_MAX)\n",
    "\n",
    "# Creating a Gephy file of the coupling graph  \n",
    "bau.save_communities_gexf(G,out_dir_coupling)\n",
    "print(\"Coupling analysis of the corpus saved as Gephy file in folder:\", str(out_dir_coupling))\n",
    "\n",
    "# Creating an EXCEL file of the coupling analysis results\n",
    "bau.save_communities_xls(partition,in_dir_coupling,out_dir_coupling)\n",
    "print(\"Coupling analysis of the corpus saved as EXCEL file in folder:\", str(out_dir_coupling))\n",
    "\n",
    "#louvain_partition = graph_community(G)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliographic Title keywords analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Work in progress this is only a spinet\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "df_corpus = bau.read_database_wos(r'C:\\Users\\franc\\BiblioAnalysis Data\\Test_BT\\rawdata\\1_500.txt')\n",
    "df_title = pd.DataFrame(df_corpus['TI'].dropna())\n",
    "df_title.columns = ['Title']\n",
    "df_TK,list_of_words_occurrences = bau.build_title_keywords(df_title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODEC reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Work in progress this is only a spinet\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "df_coden = pd.read_csv(r'C:\\Temp\\coden.txt',sep='\\t',engine='python' )\n",
    "coden = df_coden['CODEN'].to_list()\n",
    "coden = [coden[i+1] if coden[i] is None else coden[i] for i in range(len(coden))]\n",
    "dict1 = dict(zip(coden[::2],df_coden['Publication Title'].to_list()[::2]))\n",
    "dict2 = dict(zip(coden[1::2],df_coden['Publication Title'].to_list()[1::2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "database, filename, in_dir, out_dir = bau.merge_database_gui()\n",
    "\n",
    "bau.merge_database(database,filename,in_dir,out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot cooccurrences graph and save them with a Gephy format (.gexf)\n",
    "Plots the graph stored in the .json file coocnetworks.json stored in the freq folder. Work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_cooc(in_dir, out_dir, item):\n",
    "    \n",
    "    '''Plots a cooccurrence graph. Saves the graph G to the Gephy .gexf standard\n",
    "    '''\n",
    "    # Standard library import\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # 3rd party import\n",
    "    import BiblioAnalysis_Utils as bau\n",
    "    \n",
    "    # Internal import\n",
    "    import networkx as nx\n",
    "\n",
    "    G = bau.plot_graph(in_dir,item)\n",
    "    nx.write_gexf(G,out_dir / Path('cooc_'+item+'.gexf'))\n",
    "    \n",
    "# \"AU\", \"S\", \"I\", \"CU\", \"S2\", \"K\", \"AK\", \"TK\", \"R\", \"RJ\"\n",
    "in_dir = Path(r'C:\\Users\\franc\\BiblioAnalysis_Files\\Test_2005\\freq')\n",
    "out_dir = Path(r'C:\\Users\\franc\\BiblioAnalysis_Files\\Test_2005\\freq')\n",
    "item = 'AK'    \n",
    "plot_cooc(in_dir, out_dir, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
