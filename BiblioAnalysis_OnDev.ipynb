{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiblioAnalysis_OnDev\n",
    "\n",
    "### Aims\n",
    "- This jupyter notebook results from the use analysis of BiblioTools2jupyter notebook and a new implementation of the following parts:\n",
    "    - Parsing: replaced and tested\n",
    "    - Corpus description: replaced and tested\n",
    "    - Filtering: replaced and tested, integrating the \"EXCLUSION\" mode and the recursive filtering\n",
    "    - Cooccurrence analysis : replaced and tested, integrating graph plot and countries GPS coordinates\n",
    "    \n",
    "### Created modules in Utils\n",
    "    - BiblioParser.py\n",
    "    - BiblioDescription.py\n",
    "    - BiblioFilter.py\n",
    "    - BiblioCooc.py\n",
    "\n",
    "### BiblioTool3.2 source\n",
    "http://www.sebastian-grauwin.com/bibliomaps/download.html \n",
    "\n",
    "### List of initial Python packages extracted from  BiblioTool3.2\n",
    "- biblio_parser.py\t⇒ pre-processes WOS / Scopus data files,\n",
    "- corpus_description.py\t⇒ performs a frequency analysis of the items in corpus,\n",
    "- filter.py\t⇒ filters the corpus according to a range of potential queries but still too specific\n",
    "- biblio_coupling.py\t⇒ performs a BC anaysis of the corpus,\n",
    "- cooc_graphs.py\t⇒ produces various co-occurrence graphs based on the corpus (call parameters changed)\n",
    "\n",
    "### Specifically required list of pip install  specifically required\n",
    "- !pip3 install squarify \n",
    "- !pip3 install inquirer\n",
    "- !pip3 install community_louvain\n",
    "\n",
    "### Required nltk downloads\n",
    "- nltk.download('punkt')\n",
    "- nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"amal\" specific paths set\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "## User identification\n",
    "root = Path.home()\n",
    "\n",
    "## Building dict of paths for potential users (to be completed with the specific paths of new users) \n",
    "user = {}\n",
    "if root == Path('/Users/amal'):\n",
    "    user = {\n",
    "        'mac_packages' : '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages',\n",
    "        'path1' : 'My_Jupyter/',\n",
    "        'path2' : 'BiblioAnalysis Data/',\n",
    "        'path3' : 'BiblioAnalysis/Config/'\n",
    "        }\n",
    "\n",
    "elif root == Path('C:/Users/franc'):\n",
    "    user = {\n",
    "        'mac_packages' : '',\n",
    "        'path1' : '',\n",
    "        'path2' : 'BiblioAnalysis Data/',\n",
    "        'path3' : 'BiblioAnalysis/Config/'\n",
    "        }\n",
    "else:\n",
    "    user = {\n",
    "        'mac_packages' : '/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages',\n",
    "        'path1' : 'GitHubClone/',\n",
    "        'path2' : 'BiblioAnalysis Data/',\n",
    "        'path3' : 'BiblioAnalysis/Config/'\n",
    "        }    \n",
    "\n",
    "## Add path of 'site-packages' where useful packages are stored on MAC-OS; no impact for Windows\n",
    "sys.path.append(user['mac_packages'])\n",
    "\n",
    "## Getting complementary information from user configuration file\n",
    "file_config_users = root / Path(user['path1'] + user['path3'] + 'config_users.json') \n",
    "with open(file_config_users, \"r\") as read_file:\n",
    "    config_users = json.load(read_file)    \n",
    "user_id =  config_users['users']\n",
    "database_type =  config_users['database']\n",
    "myprojectname = config_users['myprojectname']\n",
    "expert =  config_users['expert']\n",
    "\n",
    "## Folder containing the folder Utils\n",
    "rep_utils = root / Path(user['path1'] + 'BiblioAnalysis/' + 'BiblioAnalysis_RefFiles')\n",
    "\n",
    "## Specific files for scopus type database\n",
    "scopus_cat_codes = 'scopus_cat_codes.txt'\n",
    "scopus_journals_issn_cat = 'scopus_journals_issn_cat.txt'\n",
    "\n",
    "print('\"' + user_id + '\"'+ ' specific paths set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data parsing\n",
    "- articles.dat is the central file, listing all the publications within the corpus. It contains informations such as the document type (article, letter, review, conf proceeding, etc), title, year of publication, publication source, doi, number of citations (given by WOS or Scopus at the time of the extraction) AND a unique identifier used in all the other files to identify a precise publication.\n",
    "- database.dat keeps track of the origin of the data, some part of the analysis being specific to WOS or Scopus data.\n",
    "- authors.dat lists all authors names associated to all publications ID.\n",
    "- addresses.dat lists all adresses associated to all publications ID, along with a specific ID for each adresse line. These adresses are reported as they appear in the raw data, without any further processing.\n",
    "- countries.dat lists all countries associated to all publications ID and adresses lines ID. The countries are extracted from the adresses fields of the raw data, with some cleaning (changing mentions of US states and UK countries to respectively the USA and UK).\n",
    "- institutions.dat lists all the comma-separated entities appearing in the adresses field associated to all publications ID and adresses lines ID, except those refering to a physical adresses. These entities correspond to various name variants of universities, organisms, hospitals, labs, services, departments, etc as they appear in the raw data. No treatment is made to e.g. filtering out the entities corresponding a given hierarchy level.\n",
    "- keywords.dat lists various types of keywords associated to all publications ID. \"AK\" keywords correspond to Author's keywords. \"IK\" keywords correspond to either WOS or Scopus keywords, which are built based on the authors' keywords, the title and abstract. \"TK\" correspond to title words (from which we simply remove common words and stop words - no stemming is performed). TK are especially useful when studying pre-90's publications, when the use of keywords was not yet standard.\n",
    "- references.dat lists all the references associated to all publications ID. The rawdata is parsed to store the first author name, title, source, volume and page of each reference of the raw \"references\" field.\n",
    "- subjects.dat lists all subject categories associated to all publications ID (a journal may be associated to many subject category). WOS classifies the sources it indexes into ∼ 250 categories, that are reported in the extracted data. Scopus classifies its sources into 27 major categories and ∼ 300 sub-categories, none of which are reported in the extracted data. We use Elsevier Source Title List (october 2017 version) to retrieve that information. The \"subject.dat\" contains the info relative to the major categories.\n",
    "- subjects2.dat lists Scopus's sub-categories, if the use database is Scopus.\n",
    "- AA_log.txt keeps track of the date/time the script was executed and of all the messages displayed on the terminal (number of publications extracted, % of references rejected, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries import\n",
    "import os\n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## Building the names of the useful folders\n",
    "\n",
    "    # Folder containing the wos or scopus file to process\n",
    "in_dir_parsing = root / Path(user['path2'] + myprojectname +'rawdata')\n",
    "\n",
    "    # Folder containing the output files of the data parsing \n",
    "out_dir_parsing = root / Path(user['path2'] + myprojectname + 'parsing')\n",
    "if not os.path.exists(out_dir_parsing):\n",
    "    os.mkdir(out_dir_parsing)\n",
    "\n",
    "## Running function biblio_parser\n",
    "bau.biblio_parser(in_dir_parsing, out_dir_parsing, database_type, expert, rep_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Corpus description\n",
    "Before doing anything else, you should get a general idea of the content of your database. Execute the following command line:\n",
    "\n",
    "- python BiblioTools3.2/describe_corpus.py -i myprojectname/ -v <br>\n",
    "\n",
    "The options -i indicates the data input folder and the option -v puts the verbose mode on (detailed info about the script process will be displayed in the terminal). This script performs several basic tasks:\n",
    "\n",
    "- it performs a series of frequency analysis, computing the number of occurrences of each item (authors, keywords, references, etc) within the publications of the corpus. These frequencies are automatically stored into several \"freq_xxx.dat\" files within a newly created \"freq\" folder.\n",
    "- it performs a series of generic statistical analysis, storing the numbers of distinct items of each type (e.g. there are x distinct keyword in the corpus ), the distributions of number of occurrences of each item (e.g. there are x keywords appearing in at least y publications) and the distribution of number of items per publication (e.g.there are x% of publications with y keywords). All these statistics are stored in the \"DISTRIBS_itemuse.json\" file.\n",
    "- it also performs a co-occurrence analysis, computing the number of co-occurrence of pairs of items among the top 100 most frequent items of each type (e.g. computing how often the two most used keywords appear together in the same publications). The results of this analysis are stored in the \"coocnetworks.json\" file. More systematic co-occurrence analysis can also be performed with another script, cf the Co-occurrence Maps section below.\n",
    "All the generated files can be opened and read with a simple text editor. The freq_xxx.dat, listing items by order of frequency, can also be read in a spreadsheet software such as excel. All the files are however primarily made to be read in the BiblioMaps interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## Building the names of the useful folders\n",
    "\n",
    "    # Folder containing the wos or scopus parsed files\n",
    "in_dir_corpus = out_dir_parsing\n",
    "\n",
    "    # Folder containing the wos or scopus parsed and analysed files\n",
    "out_dir_corpus = root / Path(user['path2'] + myprojectname + 'freq')\n",
    "if not os.path.exists(out_dir_corpus):\n",
    "    os.mkdir(out_dir_corpus)\n",
    "\n",
    "## Running describe_corpus\n",
    "verbose = False\n",
    "bau.describe_corpus(in_dir_corpus, out_dir_corpus, verbose)\n",
    "\n",
    "\n",
    "## Running plot of treemap for a selected item_treemap\n",
    "path_treemap = user['path2'] + myprojectname + 'freq/'\n",
    "renew_treemap = 'y'\n",
    "while renew_treemap == 'y' :\n",
    "    print('Choose the item for treemap in the tk window')\n",
    "    item_treemap = bau.item_selection()\n",
    "    file_name_treemap = root / Path(path_treemap + 'freq_'+ item_treemap +'.dat')\n",
    "    print(\"Item selected:\",item_treemap)\n",
    "    bau.treemap_item(item_treemap, file_name_treemap)\n",
    "    print() \n",
    "    print()\n",
    "    renew_treemap = input(\"Treemap for a new item ? (y/n): \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the data\n",
    "#### To be run after corpus description to allow using the following functions : describe_corpus() , treemap_item()\n",
    "\n",
    "If, upon exploring the nature of the data you realize that before going further you'd prefer to filter your corpus based on some characteristic (keeping only the publications from certain years, using some keywords or references, written by some authors from some countries, etc), you can filter the initial corpus thanks to the script:\n",
    "\n",
    "- python BiblioTools3.2/filter.py -i myprojectname/ -o myprojectname_filtered -v <br>\n",
    "\n",
    "Edit the 'filter.py' file to specify your filters. You'll also need to create a new \"myprojectname_filtered\" main folder before running the script.\n",
    "- create the files articles.dat, addresses.dat, authors.dat, countries.dat, institutions.dat, keywords.dat, references.dat, subjects.dat, subjects2.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standard library imports \n",
    "import shutil                      \n",
    "\n",
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "## Recursive filtering\n",
    "\n",
    "# Allows prints in filter_corpus_new function\n",
    "verbose = False\n",
    "\n",
    "# String used to set the output folder name of the subsequent filterings in the recursive_filter function\n",
    "path_filter = user['path2'] + myprojectname + 'filter' \n",
    "\n",
    "## Building the absolute file name of filter configuration file for the user\n",
    "file_config_filters = root / Path(user['path1'] + user['path3'] + 'config_filters.json')\n",
    "\n",
    "# Initialization of parameters for recursive filtering\n",
    "filtering_step = 1\n",
    "while True:\n",
    "    \n",
    "    ## Building the names of the useful folders and creating the output folder if not find \n",
    "    if filtering_step == 1:\n",
    "        in_dir_filter = out_dir_parsing\n",
    "    else:\n",
    "        renew_filtering = input(\n",
    "            \"Apply a new filtering process ? (y/n): \"\n",
    "            )   \n",
    "        in_dir_filter = root / Path(path_filter + '_' + str(filtering_step-1))\n",
    "        if renew_filtering == \"n\":\n",
    "            break\n",
    "   \n",
    "    out_dir_filter = root / Path(path_filter + '_' + str(filtering_step))\n",
    "    if not os.path.exists(out_dir_filter):\n",
    "        os.mkdir(out_dir_filter)\n",
    "    \n",
    "    congig_filters = \"n\"\n",
    "    while congig_filters == \"n\":\n",
    "        congig_filters = input(\n",
    "            \"Filters configuration set and saved ? (y/n): \"\n",
    "            )\n",
    "        \n",
    "    # Running function filter_corpus_new\n",
    "    bau.filter_corpus_new(in_dir_filter, out_dir_filter, verbose, file_config_filters)\n",
    "\n",
    "    # Treemap plot by a corpus item after filtering\n",
    "        # Folder containing the wos or scopus parsed and filtered files\n",
    "    in_dir_corpus = out_dir_filter\n",
    "\n",
    "        # Folder containing the wos or scopus parsed, filtered and analysed files\n",
    "    out_dir_corpus = root / Path(user['path2'] + myprojectname + 'freq' + '_' + str(filtering_step))\n",
    "    if not os.path.exists(out_dir_corpus):\n",
    "        os.mkdir(out_dir_corpus)\n",
    "    \n",
    "        # Copying 'database.dat' file in the freq folder for the use in describe_corpus function \n",
    "    original = root / Path(user['path2'] + myprojectname + 'parsing/database.dat')\n",
    "    target = root / Path(user['path2'] + myprojectname + 'filter' + '_' + str(filtering_step) + '/database.dat')\n",
    "    shutil.copyfile(original, target)\n",
    "    \n",
    "        # Running describe_corpus\n",
    "    verbose = False\n",
    "    bau.describe_corpus(in_dir_corpus, out_dir_corpus, verbose)\n",
    "\n",
    "        # Running plot of treemap for selected item_treemap\n",
    "    path_treemap = user['path2'] + myprojectname + 'freq_' + str(filtering_step) + \"/\"\n",
    "    file_name_treemap = root / Path(path_treemap + 'freq_'+ item_treemap +'.dat')\n",
    "    renew_treemap = 'y'    \n",
    "    while renew_treemap == 'y' :\n",
    "        print()\n",
    "        print()\n",
    "        print('Choose the item for treemap in the tk window')\n",
    "        item_treemap = bau.item_selection()\n",
    "        file_name_treemap = root / Path(path_treemap + 'freq_'+ item_treemap +'.dat')\n",
    "        print(\"Item selected:\",item_treemap)\n",
    "        bau.treemap_item(item_treemap, file_name_treemap)\n",
    "        print() \n",
    "        print()\n",
    "        renew_treemap = input(\"Treemap for a new item ? (y/n):\") \n",
    "  \n",
    "    filtering_step=filtering_step + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurrence Maps\n",
    "The command line\n",
    "\n",
    "- python BiblioTools3.2/cooc_graphs.py -i myprojectname/ -v -hg <br>\n",
    "\n",
    "will create multiple co-occurence networks, all stored in gdf files that can be opened in Gephi, among which:\n",
    "\n",
    "\n",
    "Example of heterogeneous network generated with BiblioTools and visualized in Gephi.\n",
    "\n",
    "- a co-cocitation network, linking references that are cited in the same publications.\n",
    "- a co-refsources network, linking references's sources that are cited in the same publications.\n",
    "- a co-author network, linking authors that collaborated in some publications.\n",
    "- a co-country network, linking countries with researchers that collaborated in some publications.\n",
    "- a co-institution network, linking institutions with researchers that collaborated in some publications. For this network to be fully useful, you may want to spend some time cleaning the \"institutions.dat\", e.g. by keeping only the big institutions (university level) or by replacing minor name variant by the dominant name variant (\"Ecole Normale Supérieure de Lyon\" → \"ENS Lyon\")\n",
    "- a co-keyword network, linking keywords being co-used in some publications. Be careful about the interpretation: keywords can be polysemic, their meaning differing from field to another (eg \"model\", \"energy\", \"evolution\", etc).\n",
    "- an heterogeneous co-occurrence network, gathering all the items (authors, keywords, journals, subjects, references, institutions, etc), cf example on the side figure. This network will be generated only if the option \"-hg\" is used in the command line above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "import BiblioAnalysis_Utils as bau\n",
    "\n",
    "## Building the names of the useful folders\n",
    "\n",
    "    # Folder containing the wos or scopus parsed and possibly filtered files \n",
    "filtering = input(\n",
    "    \"Corpus filtered ? (y/n): \"\n",
    "            )   \n",
    "if filtering == \"y\":\n",
    "    in_dir_cooc = out_dir_filter\n",
    "else:\n",
    "    in_dir_cooc = out_dir_parsing\n",
    "\n",
    "    # Folder containing the wos or scopus parsed, possibly filtered and analysed files\n",
    "out_dir_cooc = root / Path(user['path2'] + myprojectname + 'cooc')\n",
    "if not os.path.exists(out_dir_cooc):\n",
    "    os.mkdir(out_dir_cooc)\n",
    "\n",
    "## Building the coocurrence graph    \n",
    "size_min = 2\n",
    "item = 'CU' \n",
    "G = bau.build_item_cooc(item,in_dir_cooc,out_dir_cooc,size_min = size_min)\n",
    "bau.plot_cooc_graph(G,node_size_ref=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On dev for interactive definition of the items to be treated\n",
    "COOC_THR = {\n",
    "    \"AU\": 50,\n",
    "    \"CU\": 1,\n",
    "    \"I\": 50,\n",
    "    \"S\": 1,\n",
    "    \"S2\": 1,\n",
    "    \"K\": 50,\n",
    "    \"AK\": 50,\n",
    "    \"TK\": 50,\n",
    "    \"R\": 30,\n",
    "    \"RJ\": 200,\n",
    "    \"Y\": 1,\n",
    "    \"J\": 20,\n",
    "    \"DT\": 1,\n",
    "    \"LA\": 1,\n",
    "}\n",
    "# cooc_items dic keyed by the items and their thresholds\n",
    "cooc_graph(in_dir_cooc, out_dir_cooc, cooc_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliographic Coupling analysis\n",
    "You may execute the bibliographic coupling script with the command line:\n",
    "\n",
    "- python BiblioTools3.2/biblio_coupling.py -i myprojectname/ -v\n",
    "\n",
    "Example of BC clusters network visualisation created in Gephi and of one cluster's ID card, part of a lengthy PDF document listing the ID cards of all clusters.\n",
    "\n",
    "The options -i indicates the data input folder and the option -v puts the verbose mode on. This script execute a number of tasks:\n",
    "\n",
    "- It first creates the BC network, computing Kessler similarities between each pair of publications\n",
    "- It detects a first set of clusters (which we will refer to as \"TOP\") using Thomas Aynaud's python implementation of the louvain algorithm. A second set of clusters (\"SUBTOP\") is then computed by applying the same algorithm to the publications in each TOP cluster, hence providing a 2-level hierarchical partition.\n",
    "- The script will then asked whether you want to create output files for the cluster. By default, the script will output information only for clusters with more than 50 publications, but the script will asked you to confirm / change this threshold. Several files will then be created by the script:\n",
    "    - Output 1: two json files, storing information about the clusters, to be used in the BiblioMaps interface (cf below).\n",
    "    - Output 2a: two .tex files (one for each hierarchical level) you'll have to compile, displaying an \"ID Card\" for each cluster, ie the list of the most frequent keywords, subject, authors, references, etc... used by the publications within this cluster.\n",
    "    - Output 2b: one .gdf file storing information relative to the BC clusters at both the TOP and SUBTOP level. You may open this file with Gephi. You may create visualisations of either the TOP or SUBTOP level by filtering it out, resie the nodes with the \"size\" parameter, run a spatialisation layout algorithm (Force Atlas 2 usually yield satisfaying layouts). You may also choose a label within the few that are available (e.g. 'most_frequent_k' correspond to the most frequent keywords of each cluster). Refer to the Id cards created with latex to know more about the content of each cluster.\n",
    "- Finally, the script proposes you to output the BC network at the publication level, in both a gdf output format that can be opened with Gephi and a json format that can be opened in the BiblioMaps interface. You may either keep the whole network or select only the publications within a given cluster. Keep in mind that both interfaces can only handle a given number of nodes (no more than a few thousands for Gephi, a few hundreds for BiblioMaps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "   Author : Sebastian Grauwin (http://www.sebastian-grauwin.com/)\n",
    "   Copyright (C) 2017\n",
    "\"\"\"\n",
    "\n",
    "# usage: biblio_coupling.py -i DIR [-o DIR] [-p] [-v]\n",
    "# if the output dir is not specified, the code use the input folder as an output folder\n",
    "# use option -p when the partitions have already been computed and you want to use them\n",
    "# use option -v for verbose informations\n",
    "\n",
    "import os\n",
    "import inspect\n",
    "import numpy\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import Utils.Utils as Utils\n",
    "import Utils.BCUtils as BCUtils\n",
    "import networkx as nx\n",
    "import Utils.community as community\n",
    "from subprocess import call, Popen, PIPE\n",
    "\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "def BC_network(in_dir, out_dir, ini_suffix, verbose, presaved, ask):\n",
    "    ## INI\n",
    "    t1 = time.time()\n",
    "    with open(os.path.join(in_dir, \"database.dat\"), \"r\") as ff:\n",
    "        if ff.read() == \"Scopus\" and \"S\" in stuff_tokeep:\n",
    "            stuff_tokeep.append(\"S2\")\n",
    "\n",
    "    part_suffix = ini_suffix + part_suffixBC\n",
    "    ############################################################\n",
    "    ############################################################\n",
    "    ## INPUT DATA\n",
    "    if verbose:\n",
    "        print(\"..Initialize\")\n",
    "    with open(os.path.join(in_dir, \"database.dat\"), \"r\") as ff:\n",
    "        database = ff.read()\n",
    "\n",
    "    nR = dict()  # store the number of refs of the articles\n",
    "    py_id = dict()\n",
    "    # store the publication year of the articles\n",
    "    py_ref = dict()\n",
    "    # store the sum of publication year of the articles' refs\n",
    "    tc_id = dict()\n",
    "    # store the Ncitations of the articles (according to records)\n",
    "\n",
    "    src1 = os.path.join(in_dir, \"articles.dat\")\n",
    "    pl = Utils.Article()\n",
    "    pl.read_file(src1)\n",
    "    nb_art = len(pl.articles)  # store the number of articles within database\n",
    "    for l in pl.articles:\n",
    "        # .. pub year\n",
    "        py_id[l.id] = l.year\n",
    "        py_ref[l.id] = 0\n",
    "        # ..times_cited\n",
    "        tc_id[l.id] = l.times_cited\n",
    "        # .. refs\n",
    "        nR[l.id] = 0\n",
    "    del pl\n",
    "\n",
    "    ############################################################\n",
    "    ############################################################\n",
    "    ## CREATE BC WEIGHT TABLE\n",
    "    if verbose:\n",
    "        print(\"..Create the 'Bibliographic Coupling' weight table\")\n",
    "\n",
    "    ref_table = dict()  # store the id of articles using a given ref\n",
    "    BC_table = dict()  # store the number of common refs between pairs of articles\n",
    "\n",
    "    if verbose:\n",
    "        print(\"....loading refs table\")\n",
    "    with open(os.path.join(in_dir, \"references.dat\"), \"r\", encoding=\"utf8\") as file:\n",
    "        data_lines = file.read().split(\"\\n\")[:-1]\n",
    "    for l in data_lines:\n",
    "        foo = (\n",
    "            \", \".join([l.split(\"\\t\")[k] for k in range(1, 6)])\n",
    "            .replace(\",0,0\", \"\")\n",
    "            .replace(\", 0, 0\", \"\")\n",
    "            .replace(\", 0\", \"\")\n",
    "        )\n",
    "        pubid = int(l.split(\"\\t\")[0])\n",
    "        pubyear = int(l.split(\"\\t\")[2])\n",
    "        if foo in ref_table:\n",
    "            ref_table[foo].append(pubid)\n",
    "        else:\n",
    "            ref_table[foo] = [pubid]\n",
    "        if pubyear < 2100:\n",
    "            nR[pubid] += 1\n",
    "            py_ref[pubid] += pubyear\n",
    "    del data_lines\n",
    "\n",
    "    if verbose:\n",
    "        print(\"....detecting common references\")\n",
    "    for foo in ref_table:\n",
    "        if len(ref_table[foo]) >= RTUthr:\n",
    "            for i in ref_table[foo]:\n",
    "                for j in ref_table[foo]:\n",
    "                    if i < j:\n",
    "                        if i not in BC_table:\n",
    "                            BC_table[i] = dict()\n",
    "                        if j not in BC_table[i]:\n",
    "                            BC_table[i][j] = 0\n",
    "                        BC_table[i][j] += 1\n",
    "    del ref_table\n",
    "\n",
    "    t2 = time.time()\n",
    "    if verbose:\n",
    "        print(\"..time needed until now: %ds\" % (t2 - t1))\n",
    "\n",
    "    ############################################################\n",
    "    ############################################################\n",
    "    ## PREP NETWORK\n",
    "    if verbose:\n",
    "        print(\"....define graph in networkx format\")\n",
    "    G = nx.Graph()\n",
    "    TOTW = 0\n",
    "    for i in BC_table:\n",
    "        for j in BC_table[i]:\n",
    "            w_ij = (1.0 * BC_table[i][j]) / math.sqrt(nR[i] * nR[j])\n",
    "            if (\n",
    "                (BC_table[i][j] >= bcthr)\n",
    "                and (nR[i] >= NRthr)\n",
    "                and (nR[j] >= NRthr)\n",
    "                and (abs(py_id[i] - py_id[j]) <= DYthr)\n",
    "                and (w_ij >= Wthr)\n",
    "            ):\n",
    "                TOTW += w_ij\n",
    "                G.add_edge(i, j, weight=w_ij, nc=BC_table[i][j])\n",
    "\n",
    "    del BC_table\n",
    "\n",
    "    #  compute stuff\n",
    "    if len(G.nodes()) == 0:\n",
    "        print(\"BC network is empty\")\n",
    "        return\n",
    "\n",
    "    # different ways of computing the degree depending on you having networkx 1 or 2\n",
    "    try:\n",
    "        h = dict(G.degree).values()\n",
    "    except:\n",
    "        h = nx.degree(G).values()\n",
    "    avg_degree = sum(h) * 1.0 / len(h)\n",
    "    avg_weight = 2 * TOTW * 1.0 / (len(G.nodes()) * (len(G.nodes()) - 1))\n",
    "\n",
    "    ############################################################\n",
    "    ############################################################\n",
    "    ## COMPUTE BC CLUSTERS\n",
    "    # check that option -p was not forgotten\n",
    "    fooname = \"partitions%s.txt\" % (part_suffix)\n",
    "    filename = os.path.join(out_dir, fooname)  # <------ Modified by Amal Chabli\n",
    "    if ask and not presaved and os.path.isfile(filename):\n",
    "        confirm = input(\n",
    "            '..Do you want to compute a new partition? If you answer yes, the existing one (\"%s\") will be deleted. If you answer no, the script will continue by using the existing one. (y/n): '\n",
    "            % fooname\n",
    "        )\n",
    "        if confirm == \"n\":\n",
    "            presaved = True\n",
    "\n",
    "    ############################################################\n",
    "    # aux functions c++\n",
    "    def runcpplouvain(XX, which):\n",
    "        call(\n",
    "            [\n",
    "                louvainPATH + \"convert\",\n",
    "                \"-i\",\n",
    "                \"foo.txt\",\n",
    "                \"-o\",\n",
    "                \"foo.bin\",\n",
    "                \"-w\",\n",
    "                \"foo.weights\",\n",
    "            ]\n",
    "        )\n",
    "        max_mod = -1\n",
    "        best_topl = 0\n",
    "        for run in range(Nruns):\n",
    "            if Nruns > 1:\n",
    "                print(\"......run (%d/%d)\" % (run + 1, Nruns))\n",
    "            # .. run the louvain algo\n",
    "            with open(\"foo.tree\", \"w\") as f:\n",
    "                call(\n",
    "                    [\n",
    "                        louvainPATH + \"louvain\",\n",
    "                        \"foo.bin\",\n",
    "                        \"-w\",\n",
    "                        \"foo.weights\",\n",
    "                        \"-l\",\n",
    "                        \"-1\",\n",
    "                    ],\n",
    "                    stdout=f,\n",
    "                )\n",
    "            # .. get the upper level partition and its modularity\n",
    "            output = str(\n",
    "                Popen(\n",
    "                    [louvainPATH + \"hierarchy\", \"foo.tree\"], stdout=PIPE\n",
    "                ).communicate()[0]\n",
    "            )\n",
    "            call([louvainPATH + \"hierarchy\", \"foo.tree\"])\n",
    "            topl = int(output[output.find(\"levels:\") + 8]) - 1\n",
    "            with open(\"part.txt\", \"w\") as f:\n",
    "                call([louvainPATH + \"hierarchy\", \"foo.tree\", \"-l\", str(topl)], stdout=f)\n",
    "            partfoo = dict(\n",
    "                (antimapping[x[0]], int(x[1])) for x in numpy.loadtxt(\"part.txt\")\n",
    "            )\n",
    "            mod = community.modularity(partfoo, XX)\n",
    "            # .. keep all info if this is the best one\n",
    "            if mod > max_mod:\n",
    "                max_mod = mod\n",
    "                best_topl = topl\n",
    "                if which == \"a\":\n",
    "                    out_part = dict()\n",
    "                    for lev in range(topl):\n",
    "                        with open(\"part.txt\", \"w\") as f:\n",
    "                            call(\n",
    "                                [\n",
    "                                    louvainPATH + \"hierarchy\",\n",
    "                                    \"foo.tree\",\n",
    "                                    \"-l\",\n",
    "                                    str(lev + 1),\n",
    "                                ],\n",
    "                                stdout=f,\n",
    "                            )\n",
    "                            out_part[lev] = dict(\n",
    "                                (antimapping[x[0]], int(x[1]))\n",
    "                                for x in numpy.loadtxt(\"part.txt\")\n",
    "                            )\n",
    "                elif which == \"t\":\n",
    "                    out_part = partfoo.copy()\n",
    "                else:\n",
    "                    print(\"error in runcpplouvain function\")\n",
    "        return [out_part, max_mod, best_topl]\n",
    "\n",
    "    ############################################################\n",
    "    # ... extract louvain partition with c++ code\n",
    "    if not presaved and (algo_method == \"c++\"):\n",
    "        louvainPATH = (\n",
    "            os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "            + \"/Utils/louvain2017/\"\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\"..Computing partition with c++ Louvain algo\")\n",
    "        # .. convert labels of graph to consecutive integers\n",
    "        mapping = dict(zip(G.nodes(), range(0, len(G.nodes()))))\n",
    "        antimapping = dict(zip(range(0, len(G.nodes())), G.nodes()))\n",
    "        H = nx.relabel_nodes(G, mapping, copy=True)\n",
    "        # .. output graph\n",
    "        nx.write_weighted_edgelist(H, \"foo.txt\")\n",
    "        # .. compute partition\n",
    "        if verbose:\n",
    "            print(\"......compute top partition\")\n",
    "        [louvain_partition, mod, topl] = runcpplouvain(G, \"a\")\n",
    "        part = louvain_partition[topl - 1]\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"..... splitting BC network in %d top-clusters, Q=%.4f\"\n",
    "                % (len(set(part.values())), mod)\n",
    "            )\n",
    "\n",
    "        # .. second louvain partition\n",
    "        if verbose:\n",
    "            print(\"......compute level %d sub-clusters\" % (topl))\n",
    "        part2 = part.copy()\n",
    "        toupdate = {}\n",
    "        for com in set(part.values()):\n",
    "            list_nodes = [nodes for nodes in part.keys() if part[nodes] == com]\n",
    "            # split clusters of size > SIZECUT\n",
    "            if len(list_nodes) > SIZECUT:\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        \"...==> splitting cluster %d (N=%d records)\"\n",
    "                        % (com, len(list_nodes))\n",
    "                    )\n",
    "                F = G.subgraph(list_nodes).copy()\n",
    "                mapping = dict(zip(F.nodes(), range(0, len(F.nodes()))))\n",
    "                antimapping = dict(zip(range(0, len(F.nodes())), F.nodes()))\n",
    "                H = nx.relabel_nodes(F, mapping, copy=True)\n",
    "                nx.write_weighted_edgelist(H, \"foo.txt\")\n",
    "                # .. compute partition\n",
    "                [partfoo, mod, x] = runcpplouvain(F, \"t\")\n",
    "                # add prefix code\n",
    "                for aaa in partfoo.keys():\n",
    "                    partfoo[aaa] = (com + 1) * 1000 + partfoo[aaa]\n",
    "                nb_comm = len(set(partfoo.values()))\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        \"...==> cluster %d (N=%d records) was split in %d sub-clusters, Q=%.4f\"\n",
    "                        % (com, len(list_nodes), nb_comm, mod)\n",
    "                    )\n",
    "                part2.update(partfoo)\n",
    "\n",
    "            else:  # for communities of less than SIZECUT nodes, shift the com label as well\n",
    "                for n in list_nodes:\n",
    "                    toupdate[n] = \"\"\n",
    "        for n in toupdate:\n",
    "            part2[n] += 1\n",
    "        del toupdate\n",
    "\n",
    "        # ...clean\n",
    "        os.remove(\"part.txt\")\n",
    "        os.remove(\"foo.weights\")\n",
    "        os.remove(\"foo.tree\")\n",
    "        os.remove(\"foo.bin\")\n",
    "        os.remove(\"foo.txt\")\n",
    "\n",
    "        # ... save partitions\n",
    "        # .. I want communtity labels starting from 1 instead of 0 for top level\n",
    "        for k in louvain_partition[topl - 1].keys():\n",
    "            louvain_partition[topl - 1][k] += 1\n",
    "        louvain_partition[topl] = part2\n",
    "        fooname = \"partitions%s.txt\" % (part_suffix)\n",
    "        with open(\n",
    "            os.path.join(out_dir, fooname), \"w\"\n",
    "        ) as f_out:  # <------ Modified by Amal Chabl\n",
    "            f_out.write(\"%s\" % json.dumps(louvain_partition))\n",
    "\n",
    "        if verbose:\n",
    "            print(\"..time needed until now: %ds\" % (time.time() - t1))\n",
    "\n",
    "    ############################################################\n",
    "    # aux functions python\n",
    "    def runpythonlouvain(XX):\n",
    "        max_mod = -1\n",
    "        for run in range(Nruns):\n",
    "            if Nruns > 1:\n",
    "                print(\"......run (%d/%d)\" % (run + 1, Nruns))\n",
    "            foodendogram = community.generate_dendogram(XX, part_init=None)\n",
    "            partfoo = community.partition_at_level(foodendogram, len(foodendogram) - 1)\n",
    "            mod = community.modularity(partfoo, XX)\n",
    "            if mod > max_mod:\n",
    "                max_mod = mod\n",
    "                part = partfoo.copy()\n",
    "                dendogram = foodendogram.copy()\n",
    "        return [dendogram, part, max_mod]\n",
    "\n",
    "    ############################################################\n",
    "    # ... extract louvain partition with python code\n",
    "    if not presaved and (algo_method == \"python\"):\n",
    "        if verbose:\n",
    "            print(\"..Computing partition with (python) networkx Louvain algo\")\n",
    "        if verbose:\n",
    "            print(\"......compute top partition\")\n",
    "        [dendogram, part, mod] = runpythonlouvain(G)\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"..... splitting BC network in %d top-clusters, Q=%.4f\"\n",
    "                % (len(set(part.values())), mod)\n",
    "            )\n",
    "\n",
    "        # ... second louvain partition\n",
    "        if verbose:\n",
    "            print(\"......compute level %d sub-clusters\" % (len(dendogram)))\n",
    "        part2 = part.copy()\n",
    "        toupdate = {}\n",
    "        for com in set(part.values()):\n",
    "            list_nodes = [nodes for nodes in part.keys() if part[nodes] == com]\n",
    "            # split clusters of size > SIZECUT\n",
    "            if len(list_nodes) > SIZECUT:\n",
    "                H = G.subgraph(list_nodes).copy()\n",
    "                [dendo2, partfoo, mod] = runpythonlouvain(H)\n",
    "                dendo2 = community.generate_dendogram(H, part_init=None)\n",
    "                partfoo = community.partition_at_level(dendo2, len(dendo2) - 1)\n",
    "                # add prefix code\n",
    "                for aaa in partfoo.keys():\n",
    "                    partfoo[aaa] = (com + 1) * 1000 + partfoo[aaa]\n",
    "                nb_comm = len(set(partfoo.values()))\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        \"... ==> cluster %d (N=%d records) was split in %d sub-clusters, Q=%.3f\"\n",
    "                        % (com, len(list_nodes), nb_comm, mod)\n",
    "                    )\n",
    "                part2.update(partfoo)\n",
    "            else:  # for communities of less than SIZECUT nodes, shift the com label as well\n",
    "                for n in list_nodes:\n",
    "                    toupdate[n] = \"\"\n",
    "        for n in toupdate:\n",
    "            part2[n] += 1\n",
    "        del toupdate\n",
    "\n",
    "        # ... save partitions\n",
    "        louvain_partition = dict()\n",
    "        for lev in range(len(dendogram)):\n",
    "            louvain_partition[lev] = community.partition_at_level(dendogram, lev)\n",
    "        # .. I want communtity labels starting from 1 instead of 0 for top level\n",
    "        for k in louvain_partition[len(dendogram) - 1].keys():\n",
    "            louvain_partition[len(dendogram) - 1][k] += 1\n",
    "        louvain_partition[len(dendogram)] = part2\n",
    "\n",
    "        fooname = \"partitions%s.txt\" % (part_suffix)\n",
    "        with open(\n",
    "            os.path.join(out_dir, fooname), \"w\"\n",
    "        ) as f_out:  # <------ Modified by Amal Chabli\n",
    "            f_out.write(\"%s\" % json.dumps(louvain_partition))\n",
    "\n",
    "        num_levels = len(louvain_partition) - 1\n",
    "        if verbose:\n",
    "            print(\"..time needed until now: %ds\" % (time.time() - t1))\n",
    "\n",
    "    ############################################################\n",
    "    # ... upload previously computed partition\n",
    "    if presaved:\n",
    "        if verbose:\n",
    "            print(\"....uploading previously computed clusters\")\n",
    "        # ... upload partition\n",
    "        fooname = \"partitions%s.txt\" % (part_suffix)\n",
    "        filename = os.path.join(out_dir, fooname)  # <------ Modified by Amal Chabli\n",
    "        if not os.path.isfile(filename):\n",
    "            print(\"....file %s does not exists\" % filename)\n",
    "            return\n",
    "        ffin = open(filename, \"r\")\n",
    "        foo = ffin.read()\n",
    "        lines = foo.split(\"\\n\")\n",
    "        #\n",
    "        louvain_partition = json.loads(lines[0])\n",
    "        num_levels = len(louvain_partition) - 1\n",
    "        if verbose:\n",
    "            print(\"....%d+1 levels\" % num_levels)\n",
    "        # ... convert keys back into integer (json dump put them in strings)\n",
    "        auxlist = list(louvain_partition.keys())\n",
    "        for k in auxlist:\n",
    "            louvain_partition[int(k)] = dict()\n",
    "            for kk in louvain_partition[k].keys():\n",
    "                louvain_partition[int(k)][int(kk)] = louvain_partition[k][kk]\n",
    "            del louvain_partition[k]\n",
    "\n",
    "    # compute modularities Qs\n",
    "    Qtop = community.modularity(louvain_partition[num_levels - 1], G)\n",
    "    Qsub = community.modularity(louvain_partition[num_levels], G)\n",
    "\n",
    "    ############################################################\n",
    "    # ... output infos\n",
    "    print(\"....There are %d publications in the database\" % (nb_art))\n",
    "    print(\"....There are %d publications in the BC network\" % (len(G.nodes())))\n",
    "    if len(part_suffix) > len(ini_suffix):\n",
    "        print(\n",
    "            \"\\n......BEWARE: THRESHOLDS HAVE BEEN MODIFIED FROM THEIR DEFAULT VALUES IN THE PYTHON FILE\\n......(%s)\\n\"\n",
    "            % (initexthr)\n",
    "        )\n",
    "    print(\n",
    "        \"....BC networks: Average degree: %.3f, average weight: 1/%d, Qtop: %.3f, Qsub: %.3f\"\n",
    "        % (avg_degree, round(1 / avg_weight), Qtop, Qsub)\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\"..time needed until now: %ds\" % (time.time() - t1))\n",
    "\n",
    "    ########################################################################################################################\n",
    "    ########################################################################################################################\n",
    "    ## EXTRACT CLUSTERS TABLES / INFOS\n",
    "    if ask:\n",
    "        confirm = input(\n",
    "            \"..Prep and output json, gephi and tex files at 'clusters' level? (y/n): \"\n",
    "        )\n",
    "    else:\n",
    "        confirm = \"y\"\n",
    "    if confirm == \"y\":\n",
    "\n",
    "        # prep partitions\n",
    "        if verbose:\n",
    "            print(\"..Prep partitions\")\n",
    "        num_levels = len(louvain_partition) - 1\n",
    "        part = louvain_partition[(num_levels - 1)].copy()\n",
    "        part2 = louvain_partition[num_levels].copy()\n",
    "        # .. top partition\n",
    "        list_nodes = dict()\n",
    "        size_top = dict()\n",
    "        dgcl_id = dict()\n",
    "        wdgcl_id = dict()\n",
    "        w_id = dict()\n",
    "        Qint = dict()\n",
    "        for com in set(part.values()):\n",
    "            auxlist = [nodes for nodes in part.keys() if part[nodes] == com]\n",
    "            list_nodes[com] = dict()\n",
    "            for elt in auxlist:\n",
    "                list_nodes[com][elt] = \"\"\n",
    "            size_top[com] = len(list_nodes[com])\n",
    "            H = G.subgraph(auxlist).copy()\n",
    "            partfoo = dict((k, part2[k]) for k in list_nodes[com])\n",
    "            mod = community.modularity(partfoo, H)\n",
    "            Qint[com] = mod + 0.0000001\n",
    "            # density\n",
    "            for id1 in list_nodes[com]:\n",
    "                dgcl_id[id1] = len(\n",
    "                    [\n",
    "                        id2\n",
    "                        for id2 in list_nodes[com]\n",
    "                        if (id2 in G.adj[id1] and id1 != id2)\n",
    "                    ]\n",
    "                )\n",
    "                wdgcl_id[id1] = sum(\n",
    "                    [\n",
    "                        G.adj[id1][id2][\"weight\"]\n",
    "                        for id2 in list_nodes[com]\n",
    "                        if (id2 in G.adj[id1] and id1 != id2)\n",
    "                    ]\n",
    "                )\n",
    "                w_id[id1] = sum(\n",
    "                    [G.adj[id1][id2][\"weight\"] for id2 in G.adj[id1] if (id1 != id2)]\n",
    "                )\n",
    "                # dgcl_id[id1]=len([id2 for id2 in list_nodes[com] if ((id1,id2) in G.edges() and id1!=id2)])\n",
    "                # wdgcl_id[id1]=sum([G.edges[(id1,id2)][\"weight\"] for id2 in list_nodes[com] if ((id1,id2) in G.edges and id1!=id2)])\n",
    "                # w_id[id1]=sum([G.edges[(id1,id2)][\"weight\"] for id2 in list_nodes[com] if ((id1,id2) in G.edges and id1!=id2)])\n",
    "        # .. subtop partition\n",
    "        list_nodes2 = dict()\n",
    "        size_subtop = dict()\n",
    "        dgcl_id2 = dict()\n",
    "        wdgcl_id2 = dict()\n",
    "        my_top = dict()\n",
    "        for com in set(part2.values()):\n",
    "            auxlist = [nodes for nodes in part2.keys() if part2[nodes] == com]\n",
    "            list_nodes2[com] = dict()\n",
    "            for elt in auxlist:\n",
    "                list_nodes2[com][elt] = \"\"\n",
    "            # list_nodes2[com] = [nodes for nodes in part2.keys() if part2[nodes] == com]\n",
    "            size_subtop[com] = len(list_nodes2[com])\n",
    "            my_top[com] = part[auxlist[0]]\n",
    "            # density\n",
    "            for id1 in list_nodes2[com]:\n",
    "                dgcl_id2[id1] = len(\n",
    "                    [\n",
    "                        id2\n",
    "                        for id2 in list_nodes2[com]\n",
    "                        if id2 in G.adj[id1] and id1 != id2\n",
    "                    ]\n",
    "                )\n",
    "                wdgcl_id2[id1] = sum(\n",
    "                    [\n",
    "                        G.adj[id1][id2][\"weight\"]\n",
    "                        for id2 in list_nodes2[com]\n",
    "                        if (id2 in G.adj[id1] and id1 != id2)\n",
    "                    ]\n",
    "                )\n",
    "                # dgcl_id2[id1]=len([id2 for id2 in list_nodes2[com] if ((id1,id2) in G.edges and id1!=id2)])\n",
    "                # wdgcl_id2[id1]=sum([G.edges[(id1, id2)][\"weight\"] for id2 in list_nodes2[com] if ((id1,id2) in G.edges and id1!=id2)])\n",
    "        t2 = time.time()\n",
    "        print(\"..time needed until now: %ds\" % (t2 - t1))\n",
    "\n",
    "        # .. extract\n",
    "        print(\n",
    "            \"..Choose thresholds (by default, the top clusters for which a sub-partition has been computed are those of size > %d)\"\n",
    "            % SIZECUT\n",
    "        )\n",
    "        if ask:\n",
    "            topthr = input(\"....keep top clusters of size > to:\")\n",
    "            subtopthr = input(\"....keep subtop clusters of size > to:\")\n",
    "        else:\n",
    "            topthr = min(SIZECUT, len(G.nodes()) / 100)\n",
    "            subtopthr = min(10, len(G.nodes()) / 100)\n",
    "\n",
    "        confirm = \"n\"\n",
    "        while confirm != \"y\":\n",
    "            topthr = int(topthr)\n",
    "            subtopthr = int(subtopthr)\n",
    "            # ..infos  top\n",
    "            keep = [com for com in size_top if size_top[com] > topthr]\n",
    "            top_n_sup_thr = len(keep)\n",
    "            top_size_sup_thr = sum([size_top[com] for com in keep])\n",
    "            print(\n",
    "                \"....Top clusters with size > %d: %d publications gathered in %d clusters\"\n",
    "                % (topthr, top_size_sup_thr, top_n_sup_thr)\n",
    "            )\n",
    "            # ..infos  subtop\n",
    "            keep2 = [\n",
    "                com\n",
    "                for com in size_subtop\n",
    "                if size_subtop[com] > subtopthr and my_top[com] in keep\n",
    "            ]\n",
    "            sub_n_sup_thr = len(keep2)\n",
    "            sub_size_sup_thr = sum([size_subtop[com] for com in keep2])\n",
    "            print(\n",
    "                \"....Their subtop clusters: %d publications gathered in %d clusters\"\n",
    "                % (sub_size_sup_thr, sub_n_sup_thr)\n",
    "            )\n",
    "            # ..confirm\n",
    "            if ask:\n",
    "                confirm = input(\"....do you confirm? (y/n): \")\n",
    "            else:\n",
    "                confirm = \"y\"\n",
    "            if confirm == \"n\":\n",
    "                topthr = input(\"......keep top clusters of size > to:\")\n",
    "                subtopthr = input(\"......keep subtop clusters of size > to:\")\n",
    "\n",
    "        # order by size\n",
    "        fff = [[com, size_top[com]] for com in size_top if size_top[com] > topthr]\n",
    "        fff.sort(key=lambda e: -e[1])\n",
    "        keep = [f[0] for f in fff]\n",
    "        keep2 = dict()\n",
    "        for topcom in keep:\n",
    "            fff = [\n",
    "                [com, size_subtop[com]]\n",
    "                for com in size_subtop\n",
    "                if size_subtop[com] > subtopthr and my_top[com] == topcom\n",
    "            ]\n",
    "            fff.sort(key=lambda e: -e[1])\n",
    "            keep2[topcom] = [f[0] for f in fff]\n",
    "\n",
    "        t2 = time.time()\n",
    "        if verbose:\n",
    "            print(\"....total time needed: %ds\" % (t2 - t1))\n",
    "        ############################################################\n",
    "        ## PREP LINKS\n",
    "\n",
    "        # .. compute links\n",
    "        if verbose:\n",
    "            print(\"..Compute clusters links\")\n",
    "        linkW = dict()\n",
    "\n",
    "        # ...top-top\n",
    "        if verbose:\n",
    "            print(\".... top-top links\")\n",
    "        myL = mylinks(list_nodes, list_nodes, keep, keep, G, 0)\n",
    "        linkW.update(myL)\n",
    "        maxtopL = max([0.000001] + [elt[0] for elt in myL.values()])\n",
    "        del myL\n",
    "        # ...subtop-subtop\n",
    "        if verbose:\n",
    "            print(\".... subtop-subtop links\")\n",
    "        fkeep = []\n",
    "        for topcom in keep:\n",
    "            fkeep += [com for com in keep2[topcom]]\n",
    "        myL = mylinks(list_nodes2, list_nodes2, fkeep, fkeep, G, 0)\n",
    "        linkW.update(myL)\n",
    "        maxsubL = max([0.000001] + [elt[0] for elt in myL.values()])\n",
    "        del myL\n",
    "\n",
    "        t2 = time.time()\n",
    "        if verbose:\n",
    "            print(\"....total time needed: %ds\" % (t2 - t1))\n",
    "\n",
    "        ############################################################\n",
    "        #####################   QUANTITATIVES MEASURES\n",
    "        ############################################################\n",
    "        if verbose:\n",
    "            print(\"..Computing quantitative measures\")\n",
    "\n",
    "        # aux function to compute h index\n",
    "        def extract_h(mylist):\n",
    "            mylist.sort()\n",
    "            h = 1\n",
    "            if len(mylist) == 0:\n",
    "                h = 0\n",
    "            else:\n",
    "                while h < len(mylist) and mylist[-h] > h:\n",
    "                    h += 1\n",
    "            #\n",
    "            return h\n",
    "\n",
    "        ## global measures\n",
    "        Wtot = sum([w_id[id1] for id1 in w_id]) * 1.0 / 2\n",
    "        NN = len(G.nodes())\n",
    "        QQ = community.modularity(part, G)\n",
    "        TCTC = sum([tc_id[id1] for id1 in part]) * 1.0 / NN\n",
    "        HH = extract_h([tc_id[id1] for id1 in part])\n",
    "        PYPY = sum([py_id[id1] for id1 in part]) * 1.0 / NN\n",
    "        YMIN = min([py_id[id1] for id1 in part])\n",
    "        YMAX = max([py_id[id1] for id1 in part])\n",
    "        NRNR = sum([nR[id1] for id1 in part]) * 1.0 / NN\n",
    "        ARAR = (\n",
    "            sum([nR[id1] * py_id[id1] - py_ref[id1] for id1 in part])\n",
    "            * 1.0\n",
    "            / (NN * NRNR)\n",
    "        )\n",
    "        # different ways of computing the degree depending on you having networkx 1 or 2\n",
    "        try:\n",
    "            h = dict(G.degree).values()\n",
    "        except:\n",
    "            h = nx.degree(G).values()\n",
    "        avg_degree = sum(h) * 1.0 / len(h)\n",
    "        avg_weight = Wtot * 1.0 / (NN * (NN - 1))\n",
    "\n",
    "        ## top cluster measures\n",
    "        comm_nr = dict()\n",
    "        comm_ar = dict()\n",
    "        comm_py = dict()\n",
    "        comm_tc = dict()\n",
    "        comm_h = dict()\n",
    "        comm_dg = dict()\n",
    "        comm_win = dict()\n",
    "        comm_q = dict()\n",
    "\n",
    "        for com in keep:\n",
    "            # compute stuff\n",
    "            comm_nr[com] = (\n",
    "                sum([nR[id1] for id1 in list_nodes[com]]) * 1.0 / size_top[com]\n",
    "            )\n",
    "            comm_ar[com] = (\n",
    "                sum([nR[id1] * py_id[id1] - py_ref[id1] for id1 in list_nodes[com]])\n",
    "                * 1.0\n",
    "                / (comm_nr[com] * size_top[com])\n",
    "            )\n",
    "            comm_py[com] = (\n",
    "                sum([py_id[id1] for id1 in list_nodes[com]]) * 1.0 / size_top[com]\n",
    "            )\n",
    "            comm_tc[com] = (\n",
    "                sum([tc_id[id1] for id1 in list_nodes[com]]) * 1.0 / size_top[com]\n",
    "            )\n",
    "            comm_h[com] = extract_h([tc_id[id1] for id1 in list_nodes[com]])\n",
    "            comm_dg[com] = (\n",
    "                sum([dgcl_id[id1] for id1 in list_nodes[com]]) * 1.0 / size_top[com]\n",
    "            )\n",
    "            W = sum([wdgcl_id[id1] for id1 in list_nodes[com]]) * 1.0 / 2\n",
    "            comm_win[com] = 2.0 * W / (size_top[com] * (size_top[com] - 1))\n",
    "            # module\n",
    "            # q1 = fraction of total weight within com\n",
    "            # q2 = fraction of total weight attached to nodes in com\n",
    "            q1 = W * 1.0 / Wtot\n",
    "            q2 = sum([w_id[id1] for id1 in list_nodes[com]]) * 1.0 / (2 * Wtot)\n",
    "            comm_q[com] = q1 - q2 * q2\n",
    "\n",
    "        ## subtop cluster measures\n",
    "        Bcomm_nr = dict()\n",
    "        Bcomm_ar = dict()\n",
    "        Bcomm_py = dict()\n",
    "        Bcomm_tc = dict()\n",
    "        Bcomm_h = dict()\n",
    "        Bcomm_dg = dict()\n",
    "        Bcomm_win = dict()\n",
    "        Bcomm_q = dict()\n",
    "\n",
    "        for topcom in keep:\n",
    "            for com in keep2[topcom]:\n",
    "                # compute stuff\n",
    "                Bcomm_nr[com] = (\n",
    "                    sum([nR[id1] for id1 in list_nodes2[com]]) * 1.0 / size_subtop[com]\n",
    "                )\n",
    "                Bcomm_ar[com] = (\n",
    "                    sum(\n",
    "                        [nR[id1] * py_id[id1] - py_ref[id1] for id1 in list_nodes2[com]]\n",
    "                    )\n",
    "                    * 1.0\n",
    "                    / (Bcomm_nr[com] * size_subtop[com])\n",
    "                )\n",
    "                Bcomm_py[com] = (\n",
    "                    sum([py_id[id1] for id1 in list_nodes2[com]])\n",
    "                    * 1.0\n",
    "                    / size_subtop[com]\n",
    "                )\n",
    "                Bcomm_tc[com] = (\n",
    "                    sum([tc_id[id1] for id1 in list_nodes2[com]])\n",
    "                    * 1.0\n",
    "                    / size_subtop[com]\n",
    "                )\n",
    "                Bcomm_h[com] = extract_h([tc_id[id1] for id1 in list_nodes2[com]])\n",
    "                Bcomm_dg[com] = (\n",
    "                    sum([dgcl_id[id1] for id1 in list_nodes2[com]])\n",
    "                    * 1.0\n",
    "                    / size_subtop[com]\n",
    "                )\n",
    "                W = sum([wdgcl_id[id1] for id1 in list_nodes2[com]]) * 1.0 / 2\n",
    "                Bcomm_win[com] = 2.0 * W / (size_subtop[com] * (size_subtop[com] - 1))\n",
    "                # module\n",
    "                q1 = W * 1.0 / Wtot\n",
    "                q2 = sum([w_id[id1] for id1 in list_nodes2[com]]) * 1.0 / (2 * 2 * Wtot)\n",
    "                Bcomm_q[com] = q1 - q2 * q2\n",
    "\n",
    "        ############################################################\n",
    "        #####################   PREP CLUSTERS STUFF\n",
    "        ############################################################\n",
    "\n",
    "        ## CLUSTERS ITEMS\n",
    "        print(\".... Compute clusters' most frequent items\")\n",
    "\n",
    "        # .. extract top\n",
    "        if verbose:\n",
    "            print(\"....Computing most frequent items in top clusters\")\n",
    "        (quantR, stuff, avail) = BCUtils.comm_tables(in_dir, part, topthr, verbose)\n",
    "        comm_label = dict()\n",
    "        for com in stuff[\"K\"]:\n",
    "            comm_label[com] = extract_labels(stuff[\"K\"][com])\n",
    "\n",
    "        # .. extract subtop\n",
    "        if verbose:\n",
    "            print(\"....Computing most frequent items in subtop clusters\")\n",
    "        (BquantR, Bstuff, Bavail) = BCUtils.comm_tables(\n",
    "            in_dir, part2, subtopthr, verbose\n",
    "        )\n",
    "\n",
    "        Bcomm_label = dict()\n",
    "        for com in Bstuff[\"K\"]:\n",
    "            Bcomm_label[com] = extract_labels(Bstuff[\"K\"][com])\n",
    "\n",
    "        \"\"\"\n",
    "    ## CLUSTERS % IN ALL SUBJECTS\n",
    "    #.. extract top\n",
    "    if verbose: print (\"....Computing frequencies of aggregated items in top clusters\")\n",
    "    (listS, groupS)=BCUtils.comm_groups(in_dir,part,topthr,verbose);\n",
    "\n",
    "    #.. extract subtop\n",
    "    if verbose: print (\"....Computing frequencies of aggregated items in subtop clusters\")\n",
    "    (BlistS, BgroupS)=BCUtils.comm_groups(in_dir,part2,subtopthr,verbose);\n",
    "    \"\"\"\n",
    "\n",
    "        ## CLUSTERS AR\n",
    "        # .. extract top\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"....Computing most cited publications & authors + most representative publications in top clusters\"\n",
    "            )\n",
    "        (CR_papers, CR_authors) = BCUtils.comm_AR(\n",
    "            in_dir, part, topthr, dgcl_id, verbose\n",
    "        )\n",
    "\n",
    "        # .. extract subtop\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"....Computing most cited publications & authors + most representative publications in subtop clusters\"\n",
    "            )\n",
    "        (BCR_papers, BCR_authors) = BCUtils.comm_AR(\n",
    "            in_dir, part2, subtopthr, dgcl_id2, verbose\n",
    "        )\n",
    "\n",
    "        t2 = time.time()\n",
    "        if verbose:\n",
    "            print(\"....total time needed: %ds\" % (t2 - t1))\n",
    "\n",
    "        ############################################################\n",
    "        #####################   OUTPUT GEPHI FILES\n",
    "        ############################################################\n",
    "        ## ini\n",
    "        if verbose:\n",
    "            print(\"..output gdf file for gephi\")\n",
    "        viz_dir = os.path.join(out_dir, \"gdffiles\")\n",
    "        if not os.path.exists(viz_dir):\n",
    "            os.makedirs(viz_dir)\n",
    "        name = \"BCclusters%s.gdf\" % (part_suffix)\n",
    "        with open(os.path.join(viz_dir, name), \"w\") as f_gephi:\n",
    "            ## ... prep nodes\n",
    "            if verbose:\n",
    "                print(\"....nodes\")\n",
    "            f_gephi.write(\n",
    "                (\"nodedef>name VARCHAR,type VARCHAR,label_f VARCHAR,label_s VARCHAR,label_fs\" +\n",
    "                \" VARCHAR,label VARCHAR,topcom VARCHAR,colortop VARCHAR,size DOUBLE,Qint DOUBLE\\n\")\n",
    "            )\n",
    "            for com in keep:\n",
    "                f_gephi.write(\n",
    "                    \"%d,'top','%s','%s','%s','%s',%d,color%d,%d,%1.0f\\n\"\n",
    "                    % (\n",
    "                        com,\n",
    "                        comm_label[com][0],\n",
    "                        comm_label[com][1],\n",
    "                        comm_label[com][2],\n",
    "                        comm_label[com][3],\n",
    "                        com,\n",
    "                        com,\n",
    "                        size_top[com],\n",
    "                        Qint[com],\n",
    "                    )\n",
    "                )\n",
    "                # corresponding subtop\n",
    "                for c in keep2[com]:\n",
    "                    f_gephi.write(\n",
    "                        \"%d,'subtop','%s','%s','%s','%s',%d,color%d,%d,%1.0f\\n\"\n",
    "                        % (\n",
    "                            c,\n",
    "                            Bcomm_label[c][0],\n",
    "                            Bcomm_label[c][1],\n",
    "                            Bcomm_label[c][2],\n",
    "                            Bcomm_label[c][3],\n",
    "                            com,\n",
    "                            com,\n",
    "                            size_subtop[c],\n",
    "                            Qint[com],\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            ## ... prep links\n",
    "            if verbose:\n",
    "                print(\"....links\")\n",
    "            f_gephi.write(\n",
    "                \"edgedef>node1 VARCHAR,node2 VARCHAR,num_links DOUBLE,weight DOUBLE, logweight DOUBLE\\n\"\n",
    "            )\n",
    "            for elt in linkW:\n",
    "                if linkW[elt][0] > 0.000001:\n",
    "                    f_gephi.write(\n",
    "                        \"%d,%d,%d,%.9f,%.2f\\n\"\n",
    "                        % (\n",
    "                            elt[0],\n",
    "                            elt[1],\n",
    "                            linkW[elt][1],\n",
    "                            linkW[elt][0],\n",
    "                            6 + math.log(linkW[elt][0]) / math.log(10),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        ############################################################\n",
    "        #####################   OUTPUT JSON FILES\n",
    "        ############################################################\n",
    "\n",
    "        # .. output stuff\n",
    "        print(\"..output stuff in json files\")\n",
    "        viz_dir = os.path.join(out_dir, \"jsonfiles\")\n",
    "        if not os.path.exists(viz_dir):\n",
    "            os.makedirs(viz_dir)\n",
    "        if verbose:\n",
    "            print(\"....clusters network\")\n",
    "        name = \"BCclusters%s.json\" % (part_suffix)\n",
    "        kompt = 0\n",
    "        group = 1\n",
    "        with open(os.path.join(viz_dir, name), \"w\", encoding=\"utf8\") as outfile:\n",
    "            # nodes\n",
    "            outfile.write('{\\n\\t\"nodes\":[\\n')\n",
    "            for com in keep:\n",
    "                bigstuff = \",\".join(\n",
    "                    [\n",
    "                        '\"' + x + '\":%s' % (cvst(stuff, com, x, howmuch_tokeep[x]))\n",
    "                        for x in stuff_tokeep\n",
    "                    ]\n",
    "                )\n",
    "                available = \",\".join(\n",
    "                    ['\"' + x + '\":%.2f' % (avail[x][com]) for x in stuff_tokeep]\n",
    "                )\n",
    "                if kompt > 0:\n",
    "                    outfile.write(\",\\n\")\n",
    "                outfile.write(\n",
    "                    '\\t\\t{\"name\":\"%d\",\"level\":0,\"group\":%d,\"size\":%d,\"id_top\":%d,\"Qint\":%.3f,\"q\":%.3f,\"NR\":%.2f,\"cohesion\":[%.2f,%.3f,%.3f],\"hotness\":[%.2f,%.2f,%.2f,%d],\"label\":\"foolabel%d\",\"available\":{%s},\"stuff\":{%s,\"MCP\":%s,\"MRP\":%s,\"MCAU\":%s} }'\n",
    "                    % (\n",
    "                        com,\n",
    "                        group,\n",
    "                        size_top[com],\n",
    "                        com,\n",
    "                        Qint[com],\n",
    "                        comm_q[com],\n",
    "                        comm_nr[com],\n",
    "                        comm_dg[com],\n",
    "                        2 * comm_dg[com] / (size_top[com] - 1),\n",
    "                        1000 * comm_win[com],\n",
    "                        comm_py[com],\n",
    "                        comm_ar[com],\n",
    "                        comm_tc[com],\n",
    "                        comm_h[com],\n",
    "                        com,\n",
    "                        available,\n",
    "                        bigstuff,\n",
    "                        cvar(CR_papers[com][\"MC\"], 10),\n",
    "                        cvar(CR_papers[com][\"MR\"], 10),\n",
    "                        cvau(CR_authors[com][\"MC\"], 20),\n",
    "                    )\n",
    "                )\n",
    "                kompt += 1\n",
    "                for c in keep2[com]:\n",
    "                    bigstuff = \",\".join(\n",
    "                        [\n",
    "                            '\"' + x + '\":%s' % (cvst(Bstuff, c, x, howmuch_tokeep[x]))\n",
    "                            for x in stuff_tokeep\n",
    "                        ]\n",
    "                    )\n",
    "                    available = \",\".join(\n",
    "                        ['\"' + x + '\":%.2f' % (Bavail[x][c]) for x in stuff_tokeep]\n",
    "                    )\n",
    "                    outfile.write(\n",
    "                        ',\\n\\t\\t{\"name\":\"%d\",\"level\":1,\"group\":%d,\"size\":%d,\"id_top\":%d,\"Qint\":%.3f,\"q\":%.3f,\"NR\":%.2f,\"cohesion\":[%.2f,%.3f,%.3f],\"hotness\":[%.2f,%.2f,%.2f,%d],\"label\":\"foolabel%d\",\"available\":{%s},\"stuff\":{%s,\"MCP\":%s,\"MRP\":%s,\"MCAU\":%s} }'\n",
    "                        % (\n",
    "                            c,\n",
    "                            group,\n",
    "                            size_subtop[c],\n",
    "                            com,\n",
    "                            Qint[com],\n",
    "                            Bcomm_q[c],\n",
    "                            Bcomm_nr[c],\n",
    "                            Bcomm_dg[c],\n",
    "                            2 * Bcomm_dg[c] / (size_subtop[c] - 1),\n",
    "                            1000 * Bcomm_win[c],\n",
    "                            Bcomm_py[c],\n",
    "                            Bcomm_ar[c],\n",
    "                            Bcomm_tc[c],\n",
    "                            Bcomm_h[c],\n",
    "                            c,\n",
    "                            available,\n",
    "                            bigstuff,\n",
    "                            cvar(BCR_papers[c][\"MC\"], 10),\n",
    "                            cvar(BCR_papers[c][\"MR\"], 10),\n",
    "                            cvau(BCR_authors[c][\"MC\"], 20),\n",
    "                        )\n",
    "                    )\n",
    "                group += 1\n",
    "            outfile.write(\"\\n\\t],\\n\")\n",
    "\n",
    "            # links\n",
    "            numlink = 0\n",
    "            outfile.write('\\t\"links\":[\\n')\n",
    "            for elt in linkW:\n",
    "                if linkW[elt][0] > 0.000001:\n",
    "                    if numlink > 0:\n",
    "                        outfile.write(\",\\n\")\n",
    "                    outfile.write(\n",
    "                        '\\t\\t{\"source\":\"%d\",\"target\":\"%d\",\"weight\":%.6f}'\n",
    "                        % (elt[0], elt[1], linkW[elt][0])\n",
    "                    )\n",
    "                    numlink += 1\n",
    "            outfile.write(\"\\n\\t]\\n}\")\n",
    "\n",
    "        # .. output default var\n",
    "        if verbose:\n",
    "            print(\"....default parameters\")\n",
    "        name = \"BCdefaultVAR%s.json\" % (part_suffix)\n",
    "        with open(os.path.join(viz_dir, name), \"w\") as outfile:\n",
    "            outfile.write(\"{\\n\")\n",
    "            \"\"\"\n",
    "      outfile.write('\\t\"listS\":[[\"%s\",%.3f]' % (listS[0][0],listS[0][1]) )\n",
    "      for k in range(1,len(listS)): outfile.write(',[\"%s\",%.3f]' % (listS[k][0],listS[k][1]) )\n",
    "      outfile.write('],\\n')\n",
    "      outfile.write('\\t\"BlistS\":[[\"%s\",%.3f]' % (BlistS[0][0],BlistS[0][1]) )\n",
    "      for k in range(1,len(BlistS)): outfile.write(',[\"%s\",%.3f]' % (BlistS[k][0],BlistS[k][1]) )\n",
    "      outfile.write('],\\n')\n",
    "      \"\"\"\n",
    "            keepb = []\n",
    "            for com in keep:\n",
    "                keepb += keep2[com]\n",
    "            outfile.write('\\t\"YMIN\":%d,\\n\\t\"YMAX\":%d,\\n' % (YMIN, YMAX))\n",
    "            outfile.write('\\t\"levels\":[\"Topics\", \"Subtopics\"],\\n')\n",
    "            outfile.write('\\t\"Nbc\":%d,\\n' % (len(G.nodes())))\n",
    "            outfile.write('\\t\"Q\":[%.3f,%.3f],\\n' % (Qtop, Qsub))\n",
    "            outfile.write('\\t\"ncom\":[%d, %d],\\n' % (len(keep), len(keepb)))\n",
    "            outfile.write('\\t\"TOPmaxNodeSize\":%d,\\n' % (max(size_top.values())))\n",
    "            outfile.write('\\t\"TOPmaxLinkWeight\":%.6f,\\n' % maxtopL)\n",
    "            outfile.write(\n",
    "                '\\t\"TOPWrange\":[-6,%.2f,-6],\\n' % (math.log(maxtopL) / math.log(10))\n",
    "            )\n",
    "            outfile.write('\\t\"TOPshowthr\":1.05,\\n')\n",
    "            outfile.write('\\t\"TOPforce\":[0.2,-1000,25],\\n')\n",
    "            outfile.write(\n",
    "                '\\t\"TOPpositions\":[%s],\\n'\n",
    "                % (\n",
    "                    \",\".join(\n",
    "                        [\n",
    "                            \"[\"\n",
    "                            + str(cc)\n",
    "                            + \",\"\n",
    "                            + str(\"%.2f\" % (10 + 10 * math.cos(int(cc))))\n",
    "                            + \",\"\n",
    "                            + str(\"%.2f\" % (10 + 10 * math.sin(int(cc))))\n",
    "                            + \"]\"\n",
    "                            for cc in keep\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            outfile.write('\\t\"SUBmaxNodeSize\":%d,\\n' % (max(size_subtop.values())))\n",
    "            outfile.write('\\t\"SUBmaxLinkWeight\":%.6f,\\n' % maxsubL)\n",
    "            outfile.write(\n",
    "                '\\t\"SUBWrange\":[-6,%.2f,%.2f],\\n'\n",
    "                % (\n",
    "                    math.log(maxsubL) / math.log(10),\n",
    "                    -4 + math.log(maxsubL) / math.log(10) / 3,\n",
    "                )\n",
    "            )\n",
    "            outfile.write('\\t\"SUBshowthr\":1.1,\\n')\n",
    "            outfile.write('\\t\"SUBforce\":[0.5,-500,20],\\n')\n",
    "            outfile.write(\n",
    "                '\\t\"SUBpositions\":[%s]\\n'\n",
    "                % (\n",
    "                    \",\".join(\n",
    "                        [\n",
    "                            \"[\"\n",
    "                            + str(cc)\n",
    "                            + \",\"\n",
    "                            + str(\"%.2f\" % (10 + 10 * math.cos(int(cc))))\n",
    "                            + \",\"\n",
    "                            + str(\"%.2f\" % (10 + 10 * math.sin(int(cc))))\n",
    "                            + \"]\"\n",
    "                            for cc in keepb\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            outfile.write(\"}\")\n",
    "\n",
    "        ############################################################\n",
    "        #####################   OUTPUT TEX FILES\n",
    "        ############################################################\n",
    "        myrefL = dict()\n",
    "        for com in keep:\n",
    "            myrefL[com] = extract_labels(stuff[\"K\"][com])[1]\n",
    "        for comtop in keep2:\n",
    "            for com in keep2[comtop]:\n",
    "                myrefL[com] = extract_labels(Bstuff[\"K\"][com])[1]\n",
    "\n",
    "        # \"\"\"\n",
    "        # .. output dir\n",
    "        print(\"..output stuff in tex files\")\n",
    "        out_dirT = os.path.join(out_dir, \"texfiles\")\n",
    "        if not os.path.exists(out_dirT):\n",
    "            os.makedirs(out_dirT)\n",
    "\n",
    "        if database == \"Scopus\":\n",
    "            rem = 5\n",
    "        else:\n",
    "            rem = 0\n",
    "\n",
    "        # write a .tex for TOP clusters\n",
    "        if verbose:\n",
    "            print(\"....TOP clusters\")\n",
    "        name = \"top_clusters%s.tex\" % (part_suffix)\n",
    "        f_out = open(os.path.join(out_dirT, name), \"w\", encoding=\"utf8\")\n",
    "        # .. ini\n",
    "        f_out.write(\n",
    "            \"%s\"\n",
    "            % ini_tex(\n",
    "                initexthr,\n",
    "                len(G.nodes()),\n",
    "                nb_art,\n",
    "                top_size_sup_thr,\n",
    "                \"top\",\n",
    "                topthr,\n",
    "                top_n_sup_thr,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # .. quant\n",
    "        f_out.write(\"%s\" % quant_tex(database, \"\\\\footnotesize\"))\n",
    "        f_out.write(\n",
    "            \"All in BC & %d & %.2f & %.2f & %.3f & %.3f & %.3f & - & - & - & %.2f & %.2f & %.2f & %d\\\\\\\\\\n\\hline\\n\"\n",
    "            % (\n",
    "                NN,\n",
    "                NRNR,\n",
    "                avg_degree,\n",
    "                avg_degree * 2.0 / (NN - 1),\n",
    "                1000 * avg_weight,\n",
    "                QQ,\n",
    "                PYPY,\n",
    "                ARAR,\n",
    "                TCTC,\n",
    "                HH,\n",
    "            )\n",
    "        )\n",
    "        kompt = 0\n",
    "        for com in keep:\n",
    "            kompt += 1\n",
    "            # go to new page if table too long\n",
    "            if kompt % 48 == 38:\n",
    "                f_out.write(\n",
    "                    \"\\hline\\n\\end{tabular}}\\n\\end{center}\\n\\end{table}\\n\\clearpage\\n\\\\begin{table}\\n\\\\begin{center}\\n{\\\\footnotesize\\n\\\\begin{tabular}{|l |r r| r r r r r r r|r r r r|}\\n\\hline\\nCorpus & $N$ & $<N_{ref}>$ & $k$ & $d$ & $<\\omega_{in}>*10^3$ & $Q_i$ & $q$ & $h_{ref}$ &\\n $nr_{10}$/$nr_{5}$/$nr_{2}$ & $<PY>$ & $<A>_{refs}$ & $<N_{cit}>$ & $h$\\\\\\\\\\n\\hline\\n\"\n",
    "                )\n",
    "            # write table entries\n",
    "            f_out.write(\n",
    "                \"Cluster %d  & %d & %.2f & %.2f & %.3f & %.3f & %.3f & %.3f & %d & %d/%d/%d & %.2f & %.2f & %.2f & %d\\\\\\\\\\n\"\n",
    "                % (\n",
    "                    com,\n",
    "                    size_top[com],\n",
    "                    comm_nr[com],\n",
    "                    comm_dg[com],\n",
    "                    2 * comm_dg[com] / (size_top[com] - 1),\n",
    "                    1000 * comm_win[com],\n",
    "                    Qint[com],\n",
    "                    comm_q[com],\n",
    "                    quantR[com][0],\n",
    "                    quantR[com][1],\n",
    "                    quantR[com][2],\n",
    "                    quantR[com][3],\n",
    "                    comm_py[com],\n",
    "                    comm_ar[com],\n",
    "                    comm_tc[com],\n",
    "                    comm_h[com],\n",
    "                )\n",
    "            )\n",
    "        f_out.write(\"\\hline\\n\\end{tabular}}\\n\\\\end{center}\\n\\end{table}\")\n",
    "\n",
    "        # .. output stuff tables\n",
    "        for com in keep:\n",
    "            #\n",
    "            f_out.write(\n",
    "                (\"\\clearpage\\n\\n\\\\begin{table}[!ht]\\n\\caption*{{\\\\bf Cluster %d (``%s'').}\" % (com, myrefL[com]) + \n",
    "                 \" This cluster contains $N = %d$ publications.} \\n\\\\textcolor{white}{aa}\" % (size_top[com]) + \n",
    "                 \"\\\\\\\\\\n{\\scriptsize\\\\begin{tabular}{|p{5cm} r r|}\\n\\hline\\n\")\n",
    "            )\n",
    "            #\n",
    "            # label comm_label[com][3]\n",
    "            #\n",
    "            BCUtils.my_write(f_out, \"Keywords\", stuff[\"K\"], com, 20)\n",
    "            BCUtils.my_write(f_out, \"Title Words\", stuff[\"TK\"], com, 10)\n",
    "            BCUtils.my_write(f_out, \"Journal\", stuff[\"J\"], com, 10)\n",
    "            f_out.write(\n",
    "                \"\\end{tabular}\\n}\\n{\\scriptsize\\\\begin{tabular}{|p{5cm} r r|}\\n\\hline\\n\"\n",
    "            )\n",
    "            BCUtils.my_write(f_out, \"Institution\", stuff[\"I\"], com, 20)\n",
    "            BCUtils.my_write(f_out, \"Country\", stuff[\"C\"], com, 10)\n",
    "            BCUtils.my_write(f_out, \"Author\", stuff[\"A\"], com, 10)\n",
    "            f_out.write(\n",
    "                \"\\end{tabular}\\n}\\n{\\scriptsize\\\\begin{tabular}{|p{8cm} r r|}\\n\\hline\\n\"\n",
    "            )\n",
    "            BCUtils.my_write(f_out, \"Reference\", stuff[\"R\"], com, 20 - rem)\n",
    "            BCUtils.my_write(f_out, \"RefJournal\", stuff[\"RJ\"], com, 10)\n",
    "            BCUtils.my_write(f_out, \"Subject\", stuff[\"S\"], com, 10 - rem)\n",
    "            f_out.write(\"\\end{tabular}\\n}\\n\\end{table}\\n\\n\")\n",
    "            #\n",
    "            # MOST CITED / REPR PAPERS\n",
    "            f_out.write(\n",
    "                \"\\clearpage\\n\\n\\\\begin{table}[!ht]\\n\\caption*{{\\\\bf Cluster %d (``%s'').} Most cited publications (according to %s) and most representative publications (in term of in-degree $d_{in}$ measuring the number of publications in the cluster that are linked with it) among all publications in the cluster.}\\n\\\\textcolor{white}{aa}\\\\\\\\\\n\"\n",
    "                % (com, myrefL[com], database)\n",
    "            )\n",
    "            #\n",
    "            # label comm_label[com][3]\n",
    "            #\n",
    "            nbline = 14\n",
    "            #\n",
    "            f_out.write(\"{\\scriptsize\\\\begin{tabular}{|r r p{7cm} p{17cm}|}\\n\\\\hline\\n\")\n",
    "            f_out.write(\"\\multicolumn{4}{|c|}{{\\\\bf Most Cited publications}}\\\\\\\\\\n\")\n",
    "            f_out.write(\n",
    "                \"{\\\\bf $d_{in}$} & {\\\\bf Times Cited} & {\\\\bf Publication Ref} & {\\\\bf Publication Title} \\\\\\\\\\n\"\n",
    "            )\n",
    "            BCUtils.my_writeMCP(f_out, CR_papers[com][\"MC\"], nbline)\n",
    "            f_out.write(\"\\\\hline\\n\")\n",
    "            f_out.write(\n",
    "                \"\\multicolumn{4}{|c|}{{\\\\bf Most Representative Publications}}\\\\\\\\\\n\"\n",
    "            )\n",
    "            f_out.write(\n",
    "                \"{\\\\bf $d_{in}$} & {\\\\bf Times Cited} & {\\\\bf Publication Ref} & {\\\\bf Publication Title} \\\\\\\\\\n\"\n",
    "            )\n",
    "            BCUtils.my_writeMCP(f_out, CR_papers[com][\"MR\"], nbline)\n",
    "            # f_out.write(\"\\\\hline\\n\")\n",
    "            f_out.write(\"\\end{tabular}\\n}\\n\\end{table}\\n\\n\")\n",
    "            #\n",
    "\n",
    "            # MOST CITED / REPR AUTHORS\n",
    "            f_out.write(\n",
    "                \"\\clearpage\\n\\n\\\\begin{table}[!ht]\\n\\caption*{{\\\\bf Cluster %d (``%s'').} Most cited and representative authors. For each author, we display the number $N_a$ of publications their authored in that cluster, the sum $TC_a$ of their number of citations (according to %s), and the sum $k_a$ of their in-degree. }\\n\\\\textcolor{white}{aa}\\\\\\\\\\n\"\n",
    "                % (com, myrefL[com], database)\n",
    "            )\n",
    "            #\n",
    "            # label comm_label[com][3]\n",
    "            #\n",
    "            nbline = 20\n",
    "            #\n",
    "            f_out.write(\"{\\scriptsize\\\\begin{tabular}{|l r r r|}\\n\\\\hline\\n\")\n",
    "            f_out.write(\n",
    "                \"\\multicolumn{4}{|c|}{{\\\\bf Most Cited Authors}}\\\\\\\\\\n&&&\\\\\\\\\\n\"\n",
    "            )\n",
    "            f_out.write(\n",
    "                \"{\\\\bf Author} & {\\\\bf $N_a$} & {\\\\bf $TC_a$} & {\\\\bf $k_a$}\\\\\\\\\\n\"\n",
    "            )\n",
    "            BCUtils.my_writeMCA(f_out, CR_authors[com][\"MC\"], nbline)\n",
    "            f_out.write(\"\\n\\hline\\n\\hline\\n\")\n",
    "            f_out.write(\n",
    "                \"\\multicolumn{4}{|c|}{{\\\\bf Most Representative Authors}}\\\\\\\\\\n&&&\\\\\\\\\\n\"\n",
    "            )\n",
    "            f_out.write(\n",
    "                \"{\\\\bf Author} & {\\\\bf $N_a$} & {\\\\bf $TC_a$} & {\\\\bf $k_a$}\\\\\\\\\\n\"\n",
    "            )\n",
    "            BCUtils.my_writeMCA(f_out, CR_authors[com][\"MR\"], nbline)\n",
    "            f_out.write(\"\\end{tabular}\\n}\\n\\end{table}\\n\\n\")\n",
    "\n",
    "        # .. end\n",
    "        f_out.write(\"\\end{landscape}\\n\\n\\end{document}\\n\")\n",
    "        f_out.close()\n",
    "\n",
    "        ## ###############################\n",
    "        # write a .tex for SUBTOP clusters\n",
    "        if verbose:\n",
    "            print(\"....SUBTOP clusters\")\n",
    "        name = \"subtop_clusters%s.tex\" % (part_suffix)\n",
    "        f_out = open(os.path.join(out_dirT, name), \"w\", encoding=\"utf8\")\n",
    "        # .. ini\n",
    "        f_out.write(\n",
    "            \"%s\"\n",
    "            % ini_tex(\n",
    "                initexthr,\n",
    "                len(G.nodes()),\n",
    "                nb_art,\n",
    "                sub_size_sup_thr,\n",
    "                \"subtop\",\n",
    "                subtopthr,\n",
    "                sub_n_sup_thr,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # .. quant\n",
    "        f_out.write(\"%s\" % quant_tex(database, \"\\scriptsize\"))\n",
    "        f_out.write(\n",
    "            \"All in BC & %d & %.2f & %.2f & %.3f & %.3f & %.3f & - & - & - & %.2f & %.2f & %.2f & %d\\\\\\\\\\n\\hline\\n\"\n",
    "            % (\n",
    "                NN,\n",
    "                NRNR,\n",
    "                avg_degree,\n",
    "                avg_degree * 2.0 / (NN - 1),\n",
    "                1000 * avg_weight,\n",
    "                QQ,\n",
    "                PYPY,\n",
    "                ARAR,\n",
    "                TCTC,\n",
    "                HH,\n",
    "            )\n",
    "        )\n",
    "        kompt = 0\n",
    "        for topcom in keep:\n",
    "            for com in keep2[topcom]:\n",
    "                kompt += 1\n",
    "                # go to new page if table too long\n",
    "                if kompt % 53 == 40:\n",
    "                    f_out.write(\n",
    "                        \"\\hline\\n\\end{tabular}}\\n\\end{center}\\n\\end{table}\\n\\clearpage\\n\\\\begin{table}\\n\\\\begin{center}\\n{\\scriptsize\\n\\\\begin{tabular}{|l |r r| r r r r r r r|r r r r|}\\n\\hline\\nCorpus & $N$ & $<N_{ref}>$ & $k$ & $d$ & $<\\omega_{in}>*10^3$ & $Q_i$ & $q$ & $h_{ref}$ &\\n $nr_{10}$/$nr_{5}$/$nr_{2}$ & $<PY>$ & $<A>_{refs}$ & $<N_{cit}>$ & $h$\\\\\\\\\\n\\hline\\n\"\n",
    "                    )\n",
    "                # write table entries\n",
    "                f_out.write(\n",
    "                    \"Cluster %d  & %d & %.2f & %.2f & %.3f & %.3f & - & %.3f & %d & %d/%d/%d & %.2f & %.2f & %.2f & %d\\\\\\\\\\n\"\n",
    "                    % (\n",
    "                        com,\n",
    "                        size_subtop[com],\n",
    "                        Bcomm_nr[com],\n",
    "                        Bcomm_dg[com],\n",
    "                        2 * Bcomm_dg[com] / (size_subtop[com] - 1),\n",
    "                        1000 * Bcomm_win[com],\n",
    "                        Bcomm_q[com],\n",
    "                        BquantR[com][0],\n",
    "                        BquantR[com][1],\n",
    "                        BquantR[com][2],\n",
    "                        BquantR[com][3],\n",
    "                        Bcomm_py[com],\n",
    "                        Bcomm_ar[com],\n",
    "                        Bcomm_tc[com],\n",
    "                        Bcomm_h[com],\n",
    "                    )\n",
    "                )\n",
    "        f_out.write(\"\\hline\\n\\end{tabular}}\\n\\\\end{center}\\n\\end{table}\")\n",
    "\n",
    "        # .. output tables\n",
    "        for comtop in keep:\n",
    "            for com in keep2[comtop]:\n",
    "                #\n",
    "                f_out.write(\n",
    "                    \"\\clearpage\\n\\n\\\\begin{table}[!ht]\\n\\caption*{{\\\\bf Cluster %d (``%s'').} This cluster contains $N = %d$ publications.} \\n\\\\textcolor{white}{aa}\\\\\\\\\\n{\\scriptsize\\\\begin{tabular}{|p{5cm} r r|}\\n\\hline\\n\"\n",
    "                    % (com, myrefL[com], size_subtop[com])\n",
    "                )\n",
    "                #\n",
    "                BCUtils.my_write(f_out, \"Keywords\", Bstuff[\"K\"], com, 20)\n",
    "                BCUtils.my_write(f_out, \"Title Words\", Bstuff[\"TK\"], com, 10)\n",
    "                BCUtils.my_write(f_out, \"Journal\", Bstuff[\"J\"], com, 10)\n",
    "                f_out.write(\n",
    "                    \"\\end{tabular}\\n}\\n{\\scriptsize\\\\begin{tabular}{|p{5cm} r r|}\\n\\hline\\n\"\n",
    "                )\n",
    "                BCUtils.my_write(f_out, \"Institution\", Bstuff[\"I\"], com, 20)\n",
    "                BCUtils.my_write(f_out, \"Country\", Bstuff[\"C\"], com, 10)\n",
    "                BCUtils.my_write(f_out, \"Author\", Bstuff[\"A\"], com, 10)\n",
    "                f_out.write(\n",
    "                    \"\\end{tabular}\\n}\\n{\\scriptsize\\\\begin{tabular}{|p{8cm} r r|}\\n\\hline\\n\"\n",
    "                )\n",
    "                BCUtils.my_write(f_out, \"Reference\", Bstuff[\"R\"], com, 20 - rem)\n",
    "                BCUtils.my_write(f_out, \"RefJournal\", Bstuff[\"RJ\"], com, 10)\n",
    "                BCUtils.my_write(f_out, \"Subject\", Bstuff[\"S\"], com, 10 - rem)\n",
    "                f_out.write(\"\\end{tabular}\\n}\\n\\end{table}\\n\\n\")\n",
    "                #\n",
    "                # MOST CITED PAPERS\n",
    "                f_out.write(\n",
    "                    \"\\clearpage\\n\\n\\\\begin{table}[!ht]\\n\\caption*{{\\\\bf Cluster %d (``%s'').} Most cited publications (according to %s) and most representative publications (in term of in-degree $d_{in}$ measuring the number of publications in the cluster that are linked with it) among all publications in the cluster.}\\n\\\\textcolor{white}{aa}\\\\\\\\\\n\"\n",
    "                    % (com, myrefL[com], database)\n",
    "                )\n",
    "                #\n",
    "                # label comm_label[com][3]\n",
    "                #\n",
    "                nbline = 14\n",
    "                #\n",
    "                f_out.write(\n",
    "                    \"{\\scriptsize\\\\begin{tabular}{|r r p{7cm} p{17cm}|}\\n\\\\hline\\n\"\n",
    "                )\n",
    "                f_out.write(\n",
    "                    \"\\multicolumn{4}{|c|}{{\\\\bf Most Cited Publications}}\\\\\\\\\\n\"\n",
    "                )\n",
    "                f_out.write(\n",
    "                    \"{\\\\bf $d_{in}$} & {\\\\bf Times Cited} & {\\\\bf Publication Ref} & {\\\\bf Publication Title} \\\\\\\\\\n\"\n",
    "                )\n",
    "                BCUtils.my_writeMCP(f_out, BCR_papers[com][\"MC\"], nbline)\n",
    "                f_out.write(\"\\\\hline\\n\")\n",
    "                f_out.write(\n",
    "                    \"\\multicolumn{4}{|c|}{{\\\\bf Most Representative Publications}}\\\\\\\\\\n\"\n",
    "                )\n",
    "                f_out.write(\n",
    "                    \"{\\\\bf $d_{in}$} & {\\\\bf Times Cited} & {\\\\bf Publication Ref} & {\\\\bf Publication Title} \\\\\\\\\\n\"\n",
    "                )\n",
    "                BCUtils.my_writeMCP(f_out, BCR_papers[com][\"MR\"], nbline)\n",
    "                f_out.write(\"\\end{tabular}\\n}\\n\\end{table}\\n\\n\")\n",
    "                #\n",
    "                # MOST CITED / REPR AUTHORS\n",
    "                f_out.write(\n",
    "                    \"\\clearpage\\n\\n\\\\begin{table}[!ht]\\n\\caption*{{\\\\bf Cluster %d (``%s'').} Most cited and representative authors. For each author, we display the number $N_a$ of publications their authored in that cluster, the sum $TC_a$ of their number of citations (according to %s), and the sum $k_a$ of their in-degree. }\\n\\\\textcolor{white}{aa}\\\\\\\\\\n\"\n",
    "                    % (com, myrefL[com], database)\n",
    "                )\n",
    "                #\n",
    "                # label comm_label[com][3]\n",
    "                #\n",
    "                nbline = 20\n",
    "                #\n",
    "                f_out.write(\"{\\scriptsize\\\\begin{tabular}{|l r r r|}\\n\\\\hline\\n\")\n",
    "                f_out.write(\n",
    "                    \"\\multicolumn{4}{|c|}{{\\\\bf Most Cited Authors}}\\\\\\\\\\n&&&\\\\\\\\\\n\"\n",
    "                )\n",
    "                f_out.write(\n",
    "                    \"{\\\\bf Author} & {\\\\bf $N_a$} & {\\\\bf $TC_a$} & {\\\\bf $k_a$}\\\\\\\\\\n\"\n",
    "                )\n",
    "                BCUtils.my_writeMCA(f_out, BCR_authors[com][\"MC\"], nbline)\n",
    "                f_out.write(\"\\n\\hline\\n\\hline\\n\")\n",
    "                f_out.write(\n",
    "                    \"\\multicolumn{4}{|c|}{{\\\\bf Most Representative Authors}}\\\\\\\\\\n&&&\\\\\\\\\\n\"\n",
    "                )\n",
    "                f_out.write(\n",
    "                    \"{\\\\bf Author} & {\\\\bf $N_a$} & {\\\\bf $TC_a$} & {\\\\bf $k_a$}\\\\\\\\\\n\"\n",
    "                )\n",
    "                BCUtils.my_writeMCA(f_out, BCR_authors[com][\"MR\"], nbline)\n",
    "                f_out.write(\"\\end{tabular}\\n}\\n\\end{table}\\n\\n\")\n",
    "\n",
    "        # .. end\n",
    "        f_out.write(\"\\end{landscape}\\n\\n\\end{document}\\n\")\n",
    "        f_out.close()\n",
    "\n",
    "    ########################################################################################################################\n",
    "    ########################################################################################################################\n",
    "    #####################   OUTPUT GEPHI/JSON FILES AT THE NODE LEVEL\n",
    "    ##... aux functions\n",
    "    def myref(spl):\n",
    "        if spl[4] == \"0\":\n",
    "            ref = \", \".join(spl[1:4])\n",
    "        else:\n",
    "            ref = \", \".join(spl[1:6])\n",
    "        return ref\n",
    "\n",
    "    def extract_pubstuff(filename, nn):\n",
    "        pubstuff = {}\n",
    "        for n in nodes_to_keep:\n",
    "            pubstuff[n] = []\n",
    "        with open(\n",
    "            os.path.join(in_dir, filename + \".dat\"), \"r\", encoding=\"utf8\"\n",
    "        ) as file:\n",
    "            data_lines = file.read().split(\"\\n\")[:-1]\n",
    "        if filename == \"keywords\":\n",
    "            aux = [\n",
    "                (int(l.split(\"\\t\")[0]), l.split(\"\\t\")[2])\n",
    "                for l in data_lines\n",
    "                if (int(l.split(\"\\t\")[0]) in nodes_to_keep and l.split(\"\\t\")[1] == \"IK\")\n",
    "            ]\n",
    "        elif filename == \"references\":\n",
    "            aux = [\n",
    "                (int(l.split(\"\\t\")[0]), myref(l.split(\"\\t\")))\n",
    "                for l in data_lines\n",
    "                if (int(l.split(\"\\t\")[0]) in nodes_to_keep)\n",
    "            ]\n",
    "        else:\n",
    "            aux = [\n",
    "                (int(l.split(\"\\t\")[0]), l.split(\"\\t\")[nn])\n",
    "                for l in data_lines\n",
    "                if (int(l.split(\"\\t\")[0]) in nodes_to_keep)\n",
    "            ]\n",
    "        for elt in aux:\n",
    "            pubstuff[elt[0]].append(elt[1])\n",
    "\n",
    "        return pubstuff\n",
    "\n",
    "    ##... output the BC network?\n",
    "    num_levels = len(louvain_partition) - 1\n",
    "    if ask:\n",
    "        confirm = input(\n",
    "            \"..Prep and output json and gephi files describing the BC network at 'publications' level? (y/n): \"\n",
    "        )\n",
    "    else:\n",
    "        confirm = \"n\"\n",
    "    if confirm == \"y\":\n",
    "        while confirm == \"y\":\n",
    "            p = louvain_partition[num_levels - 1]\n",
    "            confirm2 = input(\n",
    "                \"....do you want to keep the whole network, ie %d nodes / %d edges (DON'T if network is too big)? (y/n): \"\n",
    "                % (len(G.nodes()), len(G.edges()))\n",
    "            )\n",
    "            if confirm2 == \"y\":\n",
    "                nodes_to_keep = [n for n in p]\n",
    "                name = \"network_all%s\" % (part_suffix)\n",
    "            else:\n",
    "                commtokeep = int(input(\"....keep only top cluster with id:\"))\n",
    "                nodes_to_keep = [n for n in p if p[n] == commtokeep]\n",
    "                name = \"network_topcluster%d%s\" % (commtokeep, part_suffix)\n",
    "\n",
    "            ## export gephi & json\n",
    "            if verbose:\n",
    "                print(\"....preping and exporting files\")\n",
    "            json_dir = os.path.join(\n",
    "                out_dir, \"jsonfiles\"\n",
    "            )  # <------ Modified by Amal Chabli\n",
    "            if not os.path.exists(json_dir):\n",
    "                os.makedirs(json_dir)\n",
    "            gdf_dir = os.path.join(\n",
    "                out_dir, \"gdffiles\"\n",
    "            )  # <------ Modified by Amal Chabli\n",
    "            if not os.path.exists(gdf_dir):\n",
    "                os.makedirs(gdf_dir)\n",
    "            with open(os.path.join(gdf_dir, name + \".gdf\"), \"w\") as f_gephi, open(\n",
    "                os.path.join(json_dir, name + \".json\"), \"w\", encoding=\"utf8\"\n",
    "            ) as f_json:\n",
    "                ## ... prep nodes infos\n",
    "                # for gephi: only keep simple info (firstAU, journal, year)\n",
    "                # for json: keep title, journal, year and all subjects, authors, countries, keywords, references\n",
    "                pubstuff = {}\n",
    "                pubstuff[\"K\"] = extract_pubstuff(\"keywords\", 0)\n",
    "                pubstuff[\"R\"] = extract_pubstuff(\"references\", 0)\n",
    "                pubstuff[\"S\"] = extract_pubstuff(\"subjects\", 1)\n",
    "                pubstuff[\"AU\"] = extract_pubstuff(\"authors\", 2)\n",
    "                pubstuff[\"C\"] = extract_pubstuff(\"countries\", 2)\n",
    "                #\n",
    "                f_gephi.write(\n",
    "                    '''nodedef>name VARCHAR,label VARCHAR,top_com VARCHAR,subtop_com VARCHAR,\n",
    "                    colortop VARCHAR,firstAU VARCHAR,journal VARCHAR,year VARCHAR,\n",
    "                    nb_refs DOUBLE,times_cited DOUBLE\\n'''\n",
    "                )\n",
    "                f_json.write('{\\n\\t\"nodes\":[\\n')\n",
    "                pl = Utils.Article()\n",
    "                pl.read_file(src1)\n",
    "                kompt = 0\n",
    "                ymin = 3000\n",
    "                ymax = 0\n",
    "                for l in pl.articles:\n",
    "                    if l.id in nodes_to_keep:\n",
    "                        ymin = min(ymin, l.year)\n",
    "                        ymax = max(ymax, l.year)\n",
    "                        if kompt > 0:\n",
    "                            f_json.write(\",\\n\")\n",
    "                        kompt += 1\n",
    "                        topcom = str(louvain_partition[num_levels - 1][l.id])\n",
    "                        subtopcom = str(louvain_partition[num_levels][l.id])\n",
    "                        # bottomcom = str(louvain_partition[0][l.id])\n",
    "                        customlabel = l.firstAU + \", \" + l.journal + \", \" + str(l.year)\n",
    "                        f_gephi.write(\n",
    "                            \"%d,'%s',%s,%s,color%s,'%s','%s',%d,%d,%d\\n\"\n",
    "                            % (\n",
    "                                l.id,\n",
    "                                customlabel,\n",
    "                                topcom,\n",
    "                                subtopcom,\n",
    "                                topcom,\n",
    "                                l.firstAU,\n",
    "                                l.journal,\n",
    "                                l.year,\n",
    "                                nR[l.id],\n",
    "                                l.times_cited,\n",
    "                            )\n",
    "                        )\n",
    "                        bigstuff = \",\".join(\n",
    "                            [\n",
    "                                '\"'\n",
    "                                + x\n",
    "                                + '\":[%s]'\n",
    "                                % (\n",
    "                                    \", \".join(\n",
    "                                        ['\"' + st + '\"' for st in pubstuff[x][l.id]]\n",
    "                                    )\n",
    "                                )\n",
    "                                for x in [\"K\", \"R\", \"S\", \"AU\", \"C\"]\n",
    "                            ]\n",
    "                        )\n",
    "                        f_json.write(\n",
    "                            '\\t\\t{\"name\":\"%d\",\"id_top\":%s,\"id_sub\":%s,\"title\":\"%s\",\"firstAU\":\"%s\",\"journal\":\"%s\",\"year\":%d,\"doctype\":\"%s\",\"stuff\":{%s} }'\n",
    "                            % (\n",
    "                                l.id,\n",
    "                                topcom,\n",
    "                                subtopcom,\n",
    "                                l.title.replace('\"', \"'\"),\n",
    "                                l.firstAU,\n",
    "                                l.journal,\n",
    "                                l.year,\n",
    "                                l.doctype,\n",
    "                                bigstuff,\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                f_json.write(\"\\n\\t],\\n\")\n",
    "\n",
    "                ## ... prep edges\n",
    "                f_gephi.write(\n",
    "                    \"edgedef>node1 VARCHAR,node2 VARCHAR,weight DOUBLE,nb_comm_refs DOUBLE\"\n",
    "                )\n",
    "                f_json.write('\\t\"links\":[\\n')\n",
    "                H = G.subgraph(nodes_to_keep).copy()\n",
    "                kompt = 0\n",
    "                for e in H.edges(data=True):\n",
    "                    f_gephi.write(\n",
    "                        \"\\n%d,%d,%f,%d\" % (e[0], e[1], e[2][\"weight\"], e[2][\"nc\"])\n",
    "                    )\n",
    "                    if kompt > 0:\n",
    "                        f_json.write(\",\\n\")\n",
    "                    kompt += 1\n",
    "                    f_json.write(\n",
    "                        '\\t\\t{\"source\":\"%d\",\"target\":\"%d\",\"weight\":%f,\"nc\":%d}'\n",
    "                        % (e[0], e[1], e[2][\"weight\"], e[2][\"nc\"])\n",
    "                    )\n",
    "                f_json.write(\"\\n\\t]\\n}\")\n",
    "\n",
    "            ## default json file\n",
    "            deltaY = ymax - ymin + 1\n",
    "            with open(os.path.join(json_dir, name + \"_defaultVAR.json\"), \"w\") as fout:\n",
    "                fout.write(\"{\\n\")\n",
    "                fout.write('\\t\"Wrange\":[0,1,0],\\n')\n",
    "                fout.write('\\t\"NRrange\":[1,50,1],\\n')\n",
    "                fout.write('\\t\"NCrange\":[1,50,1],\\n')\n",
    "                fout.write('\\t\"RTUrange\":[2,25,2],\\n')\n",
    "                fout.write('\\t\"DYrange\":[0,%d,%d],\\n' % (deltaY, deltaY))\n",
    "                fout.write('\\t\"forceparam\":[0.5,-150,5],\\n')\n",
    "                fout.write(\n",
    "                    '\\t\"DefinedPos\":[[%s]]\\n'\n",
    "                    % (\n",
    "                        \"], [\".join(\n",
    "                            [\n",
    "                                str(x)\n",
    "                                + \",\"\n",
    "                                + str(\"%.2f\" % (10 + 10 * math.cos(int(x))))\n",
    "                                + \",\"\n",
    "                                + str(\"%.2f\" % (10 + 10 * math.sin(int(x))))\n",
    "                                + \"]\"\n",
    "                                for x in nodes_to_keep\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                fout.write(\"}\")\n",
    "\n",
    "            ## ... end\n",
    "            confirm = input(\n",
    "                '''..Do you want to extract another set of json/gephi files describing the BC network at\n",
    "                'publications' level? (y/n): '''\n",
    "            )\n",
    "\n",
    "    ##\n",
    "    ##\n",
    "    if verbose:\n",
    "        print(\"..Done!\\n\")\n",
    "\n",
    "    ## ###################################\n",
    "    ## END\n",
    "    t2 = time.time()\n",
    "    print(\".. Total time needed: %ds\" % (t2 - t1))\n",
    "    return\n",
    "\n",
    "\n",
    "## ################################################## ##################################################\n",
    "## ################################################## AUX FUNCTIONS\n",
    "## ################################################## ##################################################\n",
    "\n",
    "\n",
    "def extract_labels(stuff):\n",
    "    comm_label = dict()\n",
    "    if len(stuff) > 0:\n",
    "        # label 0 is the most freq\n",
    "        comm_label[0] = stuff[0][0]\n",
    "        # label 1 is the most sign\n",
    "        f = 0\n",
    "        for ff in range(len(stuff)):\n",
    "            if stuff[ff][2] > stuff[f][2]:\n",
    "                f = ff\n",
    "        comm_label[1] = stuff[f][0]\n",
    "        # label 2 has the max freq * sign\n",
    "        f = 0\n",
    "        for ff in range(len(stuff)):\n",
    "            if stuff[ff][1] * stuff[ff][2] > stuff[f][1] * stuff[f][2]:\n",
    "                f = ff\n",
    "        comm_label[2] = stuff[f][0]\n",
    "        # label 3 is the most freq with sign > 1\n",
    "        fff = 0\n",
    "        while (fff < len(stuff) - 1) and (stuff[fff][2] <= 0):\n",
    "            fff += 1\n",
    "        while (fff < len(stuff) - 1) and (stuff[fff][2] <= 1):\n",
    "            fff += 1\n",
    "        comm_label[3] = stuff[fff][0]\n",
    "        #\n",
    "    else:\n",
    "        for i in range(4):\n",
    "            comm_label[i] = \"\"\n",
    "    return comm_label\n",
    "\n",
    "\n",
    "## ##################################################\n",
    "\n",
    "\n",
    "def mylinks(listA, listB, keepA, keepB, G, cond):\n",
    "    links = dict()\n",
    "\n",
    "    for com1 in keepA:\n",
    "        for com2 in keepB:\n",
    "            # links[(com1, com2)]=[1,1]\n",
    "            # \"\"\"\n",
    "            if (cond == 0 and com1 > com2) or (cond == 1):\n",
    "                W = 0\n",
    "                N = 0\n",
    "                for id1 in listA[com1]:\n",
    "                    for id2 in listB[com2]:\n",
    "                        if id2 in G.adj[id1]:\n",
    "                            N += 1\n",
    "                            W += G.adj[id1][id2][\"weight\"]\n",
    "                if W > 0:\n",
    "                    links[(com1, com2)] = [\n",
    "                        W * 1.0 / (len(listA[com1]) * len(listB[com2])),\n",
    "                        N,\n",
    "                    ]\n",
    "            # \"\"\"\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "## ##################################################\n",
    "\n",
    "\n",
    "def cvst(stuff, com, x, ll):\n",
    "    ## most frequent items\n",
    "    if com in stuff[x]:\n",
    "        stuff = list(stuff[x][com].values())\n",
    "        if len(stuff) > 0:\n",
    "            d = 0\n",
    "            foo = '[[\"%s\",%.2f,%.2f]' % (\n",
    "                stuff[d][0].replace('\"', \"'\").replace(\"\\&\", \"&\"),\n",
    "                stuff[d][1],\n",
    "                stuff[d][2],\n",
    "            )\n",
    "            for d in range(1, min(ll, len(stuff))):\n",
    "                foo += ',[\"%s\",%.2f,%.2f]' % (\n",
    "                    stuff[d][0].replace('\"', \"'\").replace(\"\\&\", \"&\"),\n",
    "                    stuff[d][1],\n",
    "                    stuff[d][2],\n",
    "                )\n",
    "            foo += \"]\"\n",
    "        else:\n",
    "            foo = '[[\"\",0,0]]'\n",
    "    else:\n",
    "        foo = '[[\"\",0,0]]'\n",
    "    return foo\n",
    "\n",
    "\n",
    "def cvau(stuff, ll):\n",
    "    ## most cited authors\n",
    "    stuff = list(stuff.values())\n",
    "    if len(stuff) > 0:\n",
    "        d = 0\n",
    "        foo = '[[\"%s\",%d,%d]' % (stuff[d][0], stuff[d][1][0], stuff[d][1][1])\n",
    "        for d in range(1, min(ll, len(stuff))):\n",
    "            foo += ',[\"%s\",%d,%d]' % (stuff[d][0], stuff[d][1][0], stuff[d][1][1])\n",
    "        foo += \"]\"\n",
    "    else:\n",
    "        foo = '[[\"\",0,0]]'\n",
    "\n",
    "    return foo\n",
    "\n",
    "\n",
    "def cvar(stuff, ll):\n",
    "    ## most cited / representative papers\n",
    "    def myfunct(xx):\n",
    "        # .. return number of authors if more than 12\n",
    "        Nb = xx.count(\" \")\n",
    "        if Nb > 12:\n",
    "            yy = \"[%d different authors]\" % Nb\n",
    "        else:\n",
    "            yy = xx\n",
    "        return yy\n",
    "\n",
    "    #\n",
    "    stuff = list(stuff.values())\n",
    "    if len(stuff) > 0:\n",
    "        d = 0\n",
    "        # \"\"\"\n",
    "        foo = '[[\"%s\",\"%s\",\"%s\",%d,%d,%d,\"%s\"]' % (\n",
    "            stuff[d][2].replace(\"\\&\", \"&\").replace(\"\\\\\", \"\").replace('\"', \"\"),\n",
    "            myfunct(stuff[d][8]),\n",
    "            stuff[d][3].replace(\"\\&\", \"&\"),\n",
    "            stuff[d][1],\n",
    "            stuff[d][6],\n",
    "            stuff[d][7],\n",
    "            stuff[d][5],\n",
    "        )\n",
    "        for d in range(1, min(ll, len(stuff))):\n",
    "            foo += ',[\"%s\",\"%s\",\"%s\",%d,%d,%d,\"%s\"]' % (\n",
    "                stuff[d][2].replace(\"\\&\", \"&\").replace(\"\\\\\", \"\").replace('\"', \"\"),\n",
    "                myfunct(stuff[d][8]),\n",
    "                stuff[d][3].replace(\"\\&\", \"&\"),\n",
    "                stuff[d][1],\n",
    "                stuff[d][6],\n",
    "                stuff[d][7],\n",
    "                stuff[d][5],\n",
    "            )\n",
    "        foo += \"]\"\n",
    "        foo.replace(\"\\\\\", \"\")\n",
    "    else:\n",
    "        foo = \"[]\"\n",
    "\n",
    "    return foo\n",
    "\n",
    "\n",
    "## ##################################################\n",
    "def ini_tex(a,b,c,d,e,f,g):\n",
    "    \n",
    "    '''\n",
    "    creates the header of the subtop_clusters tex document\n",
    "    \n",
    "    Args:\n",
    "        a (int): number of publication in the cluster\n",
    "        b (int): total number of publication\n",
    "        c (int): number of clusters\n",
    "        d (int): minimum number of publication per cluster\n",
    "    '''\n",
    "\n",
    "    mystr = (\"\\\\documentclass[a4paper,11pt]{report}\\n\\\\usepackage[english]{babel}\" +\n",
    "             \"\\n\\\\usepackage[latin1]{inputenc}\\n\\\\usepackage{amsfonts,amssymb,amsmath}\" +\n",
    "             \"\\n\\\\usepackage{pdflscape}\\n\\\\usepackage{color,colortbl}\\n\\\\\" +\n",
    "             \"usepackage{graphicx}\\n\\\\usepackage{caption}\\n\\n\\\\usepackage{array}\" +\n",
    "             \"\\n\\def\\ignore#1\\endignore{}\\n\\\\newcolumntype{h}{@{}>{\\ignore}l<{\\endignore}}\" +\n",
    "             \" %%%%  blind out some columns\\n\\n\\\\addtolength{\\evensidemargin}{-60pt}\\n\\\\\" +\n",
    "             \"addtolength{\\oddsidemargin}{-60pt}\\n\\\\addtolength{\\\\textheight}{80pt}\\n\\n\\\\\" +\n",
    "             \"title{{\\\\bf Clusters ID Cards}}\\n\\date{\\\\begin{flushleft}\")\n",
    "\n",
    "    mystr += \"This document gathers the ''ID Cards'' of the BC clusters found within the studied database.\"\n",
    "    mystr += \"\\\\\\\\\\n %s - %d out of %d publications are in the network.\" % (a,b,c)\n",
    "    mystr += \" The %d clusters presented here correspond to the ones found in the %s level\" % (g,e) \n",
    "    mystr += \" grouping at least %d publications. They gather a total of %d publications.\" % (f,d) \n",
    "    mystr += (\" \\\\\\\\\\n These ID cards displays the most frequent keywords, subject categories,\" +\n",
    "            \" journals of publication, institutions, countries, authors, references and reference\" +\n",
    "            \"journals of the publications of each cluster. The significance of an item\" +\n",
    "            \" $\\sigma = \\sqrt{N} (f - p) / \\sqrt{p(1-p)}$ - where $N$ is the number of publications\" +\n",
    "            \" within the cluster and $f$ and $p$ are the proportion of publications respectively\" +\n",
    "            \" within the cluster and within the database displaying that item - is also given.\" +\n",
    "            \"\\\\\\\\\\n\\\\vspace{1cm}\\n\\copyright Sebastian Grauwin - BIBLIOTOOLS/BiblioTools3.2\" +\n",
    "            \" (October 2017) \\end{flushleft}}\\n\\n\\\\begin{document}\\n\\\\\" +\n",
    "            \"begin{landscape}\\n\\maketitle\\n\") \n",
    "\n",
    "    mystr += (\"\\n\\n\\clearpage\\n\\n\\\\begin{figure}[h!]\\n\\\\begin{center}\\n%%\\includegraphics[width=1.3\" +\n",
    "            \"\\\\textwidth]{xxx}\\n\\caption{{\\\\bf %s clusters network.} The sizes of the nodes correspond\" +\n",
    "            \" to the number of publications in each cluster, their color to the top cluster they belong to.\"+ \n",
    "            \" The edges reflect shared references between clusters: the thicker an edge between two clusters,\"+\n",
    "            \" the more references they share. Labels correspond to the most significant used keyword. }\"+\n",
    "            \"\\n\\end{center}\\n\\end{figure}\\n\\n\") % (e.capitalize());\n",
    "\n",
    "    return mystr\n",
    " \n",
    "def quant_tex(database,mysize): \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    mystr= (\"\\n\\n\\clearpage\\n\\n\\\\begin{table}[ht]\\n\\caption*{{\\\\bf Quantitative characteristics of the clusters.}\" +\n",
    "          \" $N$ is the number of publications within the cluster, $<N_{ref}>$ the average number\" +\n",
    "          \" of references per publication. The cohesiveness of the cluster can be measured by:\" +\n",
    "          \" the average degree $k$ of its publications (i.e. average number of links within\" +\n",
    "          \" the cluster per publication), its density in terms of BC links $d=2k/(N-1)$,\" +\n",
    "          \" the weighted density $<\\omega_{in}>$, the inner modularity $Q_i$ obtained when splitting\" +\n",
    "          \" the cluster in a sub-partition, and the module $q$ of the cluster within the partition.\" +\n",
    "          \" To quantify how a cluster can concentrate on a given number of references,\")\n",
    "    mystr += (\" we also display the h-index $h_{ref}$ ($h$ references are cited by at least\" +\n",
    "           \" $h$ publications of the cluster) and the numbers $nr_{10}$, $nr_{5}$, $nr_{2}$,\"+\n",
    "           \" where $nr_{x}$ is the number of references cited by at least $x\\%$ of publications\" +\n",
    "           \" within the cluster. \")\n",
    "    mystr += (\"To estimate the `hotness' of a cluster, we display the average publication year\" +\n",
    "            \" of the publications within the cluster $<PY>$, the average age of the references\" +\n",
    "            \" used in the cluster $<A>_{refs}$, the average number of citation per publication\" +\n",
    "            \" (according to %s) $<N_{cit}>$, the h index $h$ of the cluster\" +\n",
    "            \" ($h$ publications have been cited at least $h$ times).}\\n\\\\begin{center}\\n\") % database\n",
    "\n",
    "    mystr += (\"{\\n%s\\\\begin{tabular}{|l |r r| r r r r r r r|r r r r|}\\n\\hline\\nCorpus\" +\n",
    "            \" & $N$ & $<N_{ref}>$ & $k$ & $d$ & $<\\omega_{in}>*10^3$ & $Q_i$ & $q$ &\" +\n",
    "            \" $h_{ref}$ & $nr_{10}$/$nr_{5}$/$nr_{2}$ & $<PY>$ & $<A>_{refs}$ & $<N_{cit}>$\" +\n",
    "            \" & $h$ \\\\\\\\\\n\\hline\\n\") % mysize\n",
    "\n",
    "    return mystr\n",
    "\n",
    "## ##################################################\n",
    "## ##################################################\n",
    "## Building the names of the useful folders and creating the output folder if not find\n",
    "\n",
    "    # Folder containing the wos or scopus parsed and possibly filtered files\n",
    "filtering = input(\n",
    "    \"Corpus filtered ? (y/n): \"\n",
    "            )   \n",
    "if filtering == \"y\":\n",
    "    in_dir_coupling = out_dir_filter\n",
    "else:\n",
    "    in_dir_coupling = out_dir_parsing\n",
    "\n",
    "    # Folder containing the wos or scopus parsed, possibly filtered and analysed files\n",
    "out_dir_coupling = root / Path(user['path2'] + myprojectname + 'coupling')\n",
    "if not os.path.exists(out_dir_coupling):\n",
    "    os.mkdir(out_dir_coupling)\n",
    "\n",
    "##########################################################################################################\n",
    "## BC clustering with louvain algo: either use the python version or the faster c++ version if possible\n",
    "algo_method = \"python\"\n",
    "# either 'python' or 'c++'\n",
    "Nruns = 1\n",
    "# number of time the louvain algorithm is run for a given network, the best partition being kept (default 1)\n",
    "SIZECUT = 50\n",
    "# compute partitions of clusters of min(1% of corpus_size, SIZECUT) into sub-clusters (default 50)\n",
    "##########################################################################################################\n",
    "# all thresholds used to define the BC network\n",
    "bcthr = 1\n",
    "# minimum number of shared references to keep a link (default 1)\n",
    "RTUthr = 2\n",
    "# minimum time of use in the corpus to count a reference in the shared references (default 2)\n",
    "DYthr = 10000\n",
    "# maximum difference in publication year to keep a link (default 10000)\n",
    "Wthr = 0\n",
    "# minimum weight to keep a link (default 0)\n",
    "NRthr = 1\n",
    "# minimum number of references to keep a nodes (default 1)\n",
    "# items to keep in the output json files / how many to keep (the x most frequent items in each cluster)\n",
    "stuff_tokeep = [\"Y\", \"K\", \"TK\", \"S\", \"J\", \"R\", \"RJ\", \"C\", \"I\", \"A\"]  #\n",
    "howmuch_tokeep = {\n",
    "    \"Y\": 100,\n",
    "    \"K\": 20,\n",
    "    \"TK\": 10,\n",
    "    \"A\": 10,\n",
    "    \"S\": 10,\n",
    "    \"S2\": 10,\n",
    "    \"J\": 10,\n",
    "    \"R\": 10,\n",
    "    \"RJ\": 10,\n",
    "    \"C\": 10,\n",
    "    \"I\": 10,\n",
    "}\n",
    "##########################################################################################################\n",
    "\n",
    "part_suffixBC = \"\"\n",
    "initexthr = \"The BC network was built by linking pairs of publications based on the references they share\"\n",
    "if bcthr != 1:\n",
    "    part_suffixBC += \"_bcthr%d\" % bcthr\n",
    "if RTUthr != 2:\n",
    "    part_suffixBC += \"_RTUthr%d\" % RTUthr\n",
    "if DYthr != 10000:\n",
    "    part_suffixBC += \"_DYthr%d\" % DYthr\n",
    "if Wthr != 0:\n",
    "    part_suffixBC += \"_Wthr%.3f\" % Wthr\n",
    "#\n",
    "auxs = \"\"\n",
    "if bcthr > 1:\n",
    "    auxs = \"s\"\n",
    "if part_suffixBC != \"\":\n",
    "    initexthr += (\n",
    "        \". We only kept links between publications sharing more than %d reference%s\"\n",
    "        % (bcthr, auxs)\n",
    "    )\n",
    "if RTUthr > 2:\n",
    "    initexthr += (\n",
    "        \", counting the number of shared references among those cited more than %d times in the studied corpus\"\n",
    "        % RTUthr\n",
    "    )\n",
    "if DYthr < 10000:\n",
    "    initexthr += \", published less than %d years apart\" % DYthr\n",
    "if Wthr > 0:\n",
    "    initexthr += \" and if their Kessler similarity was greater than %.3f\" % Wthr\n",
    "#\n",
    "if NRthr > 1:\n",
    "    part_suffixBC += \"_NRthr%d\" % NRcthr\n",
    "    initexthr += \". Nodes with less than %d references were also excluded\" % NRcthr\n",
    "\n",
    "## Running BC_network function\n",
    "ini_suffix = \"\"\n",
    "verbose = False\n",
    "presaved = False\n",
    "ask = True\n",
    "BC_network(in_dir_coupling, out_dir_coupling, ini_suffix, verbose, presaved, ask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
